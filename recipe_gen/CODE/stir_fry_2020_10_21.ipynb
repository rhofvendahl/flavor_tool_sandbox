{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas==1.0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "import math\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = os.path.dirname(os.getcwd())\n",
    "ingredients_data_raw = pd.read_csv(os.path.join(root_path, 'DATA/ingredients_data.csv'))\n",
    "ingredients_data = ingredients_data_raw.replace(float('nan'), '')\n",
    "stir_fry_data_impractical = ingredients_data[ingredients_data['stir_fry']=='y']\n",
    "stir_fry_data_all = stir_fry_data_impractical[(stir_fry_data_impractical['stir_fry_umbrella'] != 'y') & (stir_fry_data_impractical['redirect'] != 'y')]\n",
    "stir_fry_data = stir_fry_data_all[stir_fry_data_all['stir_fry_yes'] == 'y']\n",
    "\n",
    "stir_fry_data_basic = stir_fry_data[stir_fry_data['stir_fry_basic'] == 'y']\n",
    "# stir_fry_data_current = stir_fry_data[stir_fry_data['2020_7_5'] == 'y']\n",
    "# stir_fry_data_with_umbrella = ingredients_data[(ingredients_data['stir_fry_umbrella'] != 'y') & (ingredients_data['stir_fry'] == 'y')]\n",
    "# stir_fry_data = stir_fry_data_basic\n",
    "# stir_fry_data = stir_fry_data_current\n",
    "# stir_fry_data = stir_fry_data_with_umbrella\n",
    "stir_fry_data_is_basic = False\n",
    "\n",
    "stir_fry_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = os.path.dirname(os.getcwd())\n",
    "ingredients_data_raw = pd.read_csv(os.path.join(root_path, 'DATA/ingredients_data.csv'))\n",
    "ingredients_data = ingredients_data_raw.replace(float('nan'), '')\n",
    "\n",
    "stir_fry_data = ingredients_data[(ingredients_data['stir_fry_yes'] == 'y') & (ingredients_data['stir_fry_umbrella'] != 'y') & (ingredients_data['redirect'] != 'y')]\n",
    "stir_fry_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Matching \"pairs with\" terms to ingredient names\n",
    "### (doesn't need to be run regularly) (except the first bit?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_terms_from_pairs_with(pairs_with):\n",
    "    if str(pairs_with) == 'nan':\n",
    "        return []\n",
    "    else:\n",
    "        return [term.strip() for term in pairs_with.split('\\n\\n') if term.strip() != '']\n",
    "\n",
    "# break entries in column that has 'pairs with' strings into lists of ingredient terms\n",
    "ingredient_pairs_with_terms = stir_fry_data['pairs_with'].apply(get_terms_from_pairs_with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create list of all terms, ignoring case and excluding duplicates\n",
    "# all_terms = list(set(ingredient_pairs_with_terms.sum()))\n",
    "# all_terms_lower = list(set([term.lower() for term in ingredient_pairs_with_terms.sum()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !pip install inflect\n",
    "# import inflect\n",
    "# p = inflect.engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# salad_matches = pd.read_csv(os.path.join(root_path, 'DATA/term_name_matches_specific.csv'))\n",
    "    \n",
    "# def get_tokens(phrase):\n",
    "#     tokens = [token.strip() for token in re.split('\\(|\\)|,|e\\.g\\.|esp\\.|and|—|or|aka|see|see also|;|and\\/or|\\*', phrase)]\n",
    "# #     print(tokens)\n",
    "#     tokens = [p.singular_noun(token) or token for token in tokens if token != '']\n",
    "# #     print(tokens)\n",
    "#     return tokens\n",
    "\n",
    "# def get_mark(name, term):\n",
    "# #     print()\n",
    "# #     print('NAME', name)\n",
    "# #     print('TERM', term)\n",
    "#     try:\n",
    "#         salad_match = salad_matches[term][salad_matches['name'] == name].iloc[0]\n",
    "# #         print('SALAD MATCH', salad_match)\n",
    "#     except:\n",
    "# #         print('NO SALAD MATCH')\n",
    "#         salad_match = None \n",
    "#     if salad_match:\n",
    "#         if str(salad_match) in ['0', 'nan']:\n",
    "# #             print('BAD SALAD MATCH')\n",
    "#             return ''\n",
    "#         else:\n",
    "# #             print('GOOD SALAD MATCH', salad_match)\n",
    "# #             print('NAME', name)\n",
    "# #             print('TERM', term)\n",
    "# #             print()\n",
    "#             return salad_match\n",
    "    \n",
    "#     name_tokens = [token.lower() for token in get_tokens(name)]  \n",
    "#     term_tokens_mixed = get_tokens(term)\n",
    "#     term_tokens = [token.lower() for token in term_tokens_mixed]\n",
    "    \n",
    "#     primary_name = re.split('\\(|—|e\\.g\\.', name)[0].strip()\n",
    "#     primary_name_split = primary_name.split(', ')\n",
    "#     single_comma_primary_name = len(primary_name_split) == 2\n",
    "    \n",
    "# #     print()\n",
    "#     if 'e.g.' in term:\n",
    "#         if single_comma_primary_name:\n",
    "#             if name_tokens[0] == term_tokens[0]:\n",
    "#                 if name_tokens[1] in term_tokens: # specific name in e.g. term\n",
    "# #                     print('NAME TOKENS In TERM TOKENS')\n",
    "#                     term_i = term_tokens.index(name_tokens[1])\n",
    "#                     if term_tokens_mixed[term_i] == term_tokens_mixed[term_i].upper():\n",
    "#                         match = 'D'\n",
    "#                     else:\n",
    "#                         if term_tokens_mixed[0] == term_tokens_mixed[0].upper():\n",
    "#                             match = 'D'\n",
    "#                         else:\n",
    "#                             match = 'd'\n",
    "#                 else: # name matches only generic (pre e.g.) part of term\n",
    "#                     if term_tokens_mixed[0] == term_tokens_mixed[0].upper():\n",
    "#                         match = 'C'\n",
    "#                     else:\n",
    "#                         match = 'c'\n",
    "#             else:\n",
    "#                 match = ''\n",
    "#         else:\n",
    "#             if name_tokens[0] == term_tokens[0]: # name matches term before e.g.\n",
    "#                 if term_tokens_mixed[0] == term_tokens_mixed[0].upper():\n",
    "#                     match = 'C'\n",
    "#                 else:\n",
    "#                     match = 'c'\n",
    "#             elif name_tokens[0] in term_tokens[1:]: # name matches term after e.g.\n",
    "#                 term_i = term_tokens.index(name_tokens[0])\n",
    "#                 if term_tokens_mixed[term_i] == term_tokens_mixed[term_i].upper():\n",
    "#                     match = 'D'\n",
    "#                 else:\n",
    "#                     if term_tokens_mixed[0] == term_tokens_mixed[0].upper():\n",
    "#                         match = 'D'\n",
    "#                     else:\n",
    "#                         match = 'd'\n",
    "#             else:\n",
    "#                 match = ''\n",
    "#     else:\n",
    "#         if single_comma_primary_name:\n",
    "#             if ' '.join(primary_name_split).lower() in term.lower() or primary_name.lower() in term.lower(): # if primary_name is in term, any order\n",
    "#                 if term_tokens_mixed[0] == term_tokens_mixed[0].upper():\n",
    "#                     match = 'D'\n",
    "#                 else:\n",
    "#                     match = 'd'\n",
    "#             elif name_tokens[0] == term_tokens[0]: # if first part of primary name matches first token in term\n",
    "#                 if term_tokens_mixed[0] == term_tokens_mixed[0].upper():\n",
    "#                     match = 'C'\n",
    "#                 else:\n",
    "#                     match = 'c'\n",
    "#             else:\n",
    "#                 match = ''\n",
    "#         else:\n",
    "#             if name_tokens[0] in term_tokens: # if non-comma name anywhere in non-e.g. term_tokens\n",
    "#                 if term_tokens_mixed[0] == term_tokens_mixed[0].upper():\n",
    "#                     match = 'D'\n",
    "#                 else:\n",
    "#                     match = 'd'\n",
    "#             elif len(set(name_tokens).intersection(set(term_tokens))) > 0: # if there are any common tokens\n",
    "#                 if term_tokens_mixed[0] == term_tokens_mixed[0].upper():\n",
    "#                     match = 'C'\n",
    "#                 else:\n",
    "#                     match = 'c'\n",
    "#             else:\n",
    "#                 match = ''\n",
    "                \n",
    "#     if match == '':\n",
    "#         n_common = len(set(name_tokens).intersection(set(term_tokens)))\n",
    "#         if n_common != 0:\n",
    "#             match = n_common\n",
    "#     return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # doing it this way so I can add 'print' to monitor progress\n",
    "# mark_data = []\n",
    "# for name in stir_fry_data['name']:\n",
    "#     print(name)\n",
    "#     mark_data.append([get_mark(name, term) for term in all_terms])\n",
    "\n",
    "# term_name_marks = pd.DataFrame(mark_data, columns = all_terms)\n",
    "# term_name_marks['name'] = pd.Series(stir_fry_data['name'].values.tolist())\n",
    "\n",
    "# term_name_marks.to_csv(os.path.join(root_path, 'DATA/stir_fry_term_name_marks.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Extracting \"pairs with\" data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# term_name_matches_raw = pd.read_csv(os.path.join(root_path, 'DATA/stir_fry_term_name_matches_plus.csv'))\n",
    "# term_name_matches = term_name_matches_raw.replace(['0', '1', '2', '3', '4', '5', 0, 1, 2, 3, 4, 5, float('nan')], '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Native and North) American cuisines\n",
      "BAKED GOODS, e.g., breads, cakes, cookies, pastries, pies, scones\n",
      "butter, e.g., brown\n",
      "cereals, breakfast, e.g., granola\n",
      "cheese, e.g., blue, goat, ricotta\n",
      "cherries, esp. dried\n",
      "ice cream, e.g., butter pecan\n",
      "PIES, e.g., pecan, sweet potato\n",
      "purees, vegetable\n",
      "waffles\n",
      "wheat germ\n",
      "bibimbap\n",
      "cereals, hot breakfast, e.g., with fruit and nuts\n",
      "“meatballs” (e.g., brown rice + onion + parsley + walnuts)\n",
      "soups, e.g., tomato\n",
      "sprouts, e.g., bean, pea\n",
      "sweeteners, e.g., mirin, brown sugar\n",
      "milk, e.g., nondairy (almond, coconut, rice)\n",
      "cheese, e.g., blue, mozzarella\n",
      "citrus, e.g., grapefruit, lemon, lime, orange, tangerine; juice, zest\n",
      "eggs, e.g., frittatas, hard-boiled, tortillas\n",
      "OILS, e.g., nut, olive, walnut\n",
      "salad dressings, e.g., sherry vinaigrette\n",
      "SALADS, e.g., fruit, vegetable\n",
      "sauces, e.g., butter\n",
      "SPANISH CUISINE\n",
      "vinegar, other, e.g., balsamic, red wine, white wine\n",
      "CPU times: user 27.2 s, sys: 7.79 ms, total: 27.2 s\n",
      "Wall time: 27.2 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "# # CREATE PAIRING DATA MATRIX (names x names)\n",
    "# # takes a few minutes\n",
    "\n",
    "# pairing_data = pd.DataFrame({\n",
    "#     'name': stir_fry_data['name'],\n",
    "#     'pairs_with_terms': ingredient_pairs_with_terms\n",
    "# })\n",
    "\n",
    "# for name in stir_fry_data['name']:\n",
    "#     pairing_data[name] = pd.Series(['']*len(stir_fry_data['name']))\n",
    "\n",
    "# def get_pairs_with_names(row):\n",
    "#     lower_category_names = []\n",
    "#     lower_direct_names = []\n",
    "#     upper_category_names = []\n",
    "#     upper_direct_names = []\n",
    "#     for term in row['pairs_with_terms']:\n",
    "#         if term in term_name_matches.columns.values.tolist():\n",
    "#             lower_category_names += term_name_matches[term_name_matches[term] == 'c']['name'].values.tolist()\n",
    "#             lower_direct_names += term_name_matches[term_name_matches[term] == 'd']['name'].values.tolist()\n",
    "#             upper_category_names += term_name_matches[term_name_matches[term] == 'C']['name'].values.tolist()\n",
    "#             upper_direct_names += term_name_matches[term_name_matches[term] == 'D']['name'].values.tolist()\n",
    "#         else:\n",
    "#             pass\n",
    "#             print(term)\n",
    "    \n",
    "#     for lower_category_name in lower_category_names:\n",
    "#         row[lower_category_name] = 'c'\n",
    "#     for lower_direct_name in lower_direct_names:\n",
    "#         row[lower_direct_name] = 'd'\n",
    "#     for upper_category_name in upper_category_names:\n",
    "#         row[upper_category_name] = 'C'\n",
    "#     for upper_direct_name in upper_direct_names:\n",
    "#         row[upper_direct_name] = 'D'\n",
    "\n",
    "#     return row\n",
    "\n",
    "# pairing_data = pairing_data.apply(get_pairs_with_names, axis=1)\n",
    "# pairing_data.replace(float('nan'), '', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.25 s, sys: 152 ms, total: 6.41 s\n",
      "Wall time: 6.27 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "# # SYNC PAIRING DATA MATRIX (make sure [a][b] agrees with [b][a])\n",
    "\n",
    "# for index_1, name_1 in enumerate(pairing_data['name'].values.tolist()):\n",
    "#     for index_2, name_2 in enumerate(pairing_data['name'].values.tolist()):\n",
    "#         value_1 = pairing_data[name_1][index_2]\n",
    "#         value_2 = pairing_data[name_2][index_1]\n",
    "        \n",
    "#         if name_1 == name_2:\n",
    "#             proper_value = ''\n",
    "#         elif value_1 == 'D' or value_2 == 'D':\n",
    "#             proper_value = 'D'\n",
    "#         elif value_1 == 'C' or value_2 == 'C':\n",
    "#             proper_value = 'C'\n",
    "#         elif value_1 == 'd' or value_2 == 'd':\n",
    "#             proper_value = 'd'\n",
    "#         elif value_1 == 'c' or value_2 == 'c':\n",
    "#             proper_value = 'c'\n",
    "#         else:\n",
    "#             proper_value = ''\n",
    "        \n",
    "#         pairing_data[name_1][index_2] = proper_value\n",
    "\n",
    "        \n",
    "#         pairing_data[name_2][index_1] = proper_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.6 s, sys: 7.94 ms, total: 4.6 s\n",
      "Wall time: 4.61 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "# # REPRESENT PAIRING DATA AS LISTS\n",
    "\n",
    "# def get_pairs_with_names(row):\n",
    "#     lower_category_name = pairing_data[pairing_data[row['name']] == 'c']['name'].values.tolist()\n",
    "#     lower_direct_name = pairing_data[pairing_data[row['name']] == 'd']['name'].values.tolist()\n",
    "#     upper_category_name = pairing_data[pairing_data[row['name']] == 'C']['name'].values.tolist()\n",
    "#     upper_direct_name = pairing_data[pairing_data[row['name']] == 'D']['name'].values.tolist()\n",
    "    \n",
    "#     row['lower_category_names'] = pairing_data[pairing_data[row['name']] == 'c']['name'].values.tolist()\n",
    "#     row['lower_direct_names'] = pairing_data[pairing_data[row['name']] == 'd']['name'].values.tolist()\n",
    "#     row['upper_category_names'] = pairing_data[pairing_data[row['name']] == 'C']['name'].values.tolist()\n",
    "#     row['upper_direct_names'] = pairing_data[pairing_data[row['name']] == 'D']['name'].values.tolist()\n",
    "#     row['lower_names'] = row['lower_category_names'] + row['lower_direct_names']\n",
    "#     row['upper_names'] = row['upper_category_names'] + row['upper_direct_names']\n",
    "#     row['all_names'] = row['lower_names'] + row['upper_names']\n",
    "    \n",
    "# #     row['lower_category_pairs_with_names'] = list(set([lower_category_name for lower_category_name in row['lower_category_names'] if lower_category_name != row['name']]))\n",
    "# #     row['lower_direct_pairs_with_names'] = list(set([lower_direct_name for lower_direct_name in row['lower_direct_names'] if lower_direct_name != row['name']]))\n",
    "# #     row['upper_category_pairs_with_names'] = list(set([upper_category_name for upper_category_name in row['upper_category_names'] if upper_category_name != row['name']]))\n",
    "# #     row['upper_direct_pairs_with_names'] = list(set([upper_direct_name for upper_direct_name in row['upper_direct_names'] if upper_direct_name != row['name']]))\n",
    "# #     row['lower_pairs_with_names'] = list(set(row['lower_category_pairs_with_names'] + row['lower_direct_pairs_with_names']))\n",
    "# #     row['upper_pairs_with_names'] = list(set(row['upper_category_pairs_with_names'] + row['upper_direct_pairs_with_names']))\n",
    "# #     row['all_pairs_with_names'] = list(set(row['lower_pairs_with_names'] + row['upper_pairs_with_names']))\n",
    "    \n",
    "#     row['lc_sorted_pairs'] = [tuple(sorted((row['name'], other_name,))) for other_name in row['lower_category_names']]\n",
    "#     row['ld_sorted_pairs'] = [tuple(sorted((row['name'], other_name,))) for other_name in row['lower_direct_names']]\n",
    "#     row['uc_sorted_pairs'] = [tuple(sorted((row['name'], other_name,))) for other_name in row['upper_category_names']]\n",
    "#     row['ud_sorted_pairs'] = [tuple(sorted((row['name'], other_name,))) for other_name in row['upper_direct_names']]\n",
    "#     row['l_sorted_pairs'] = [tuple(sorted((row['name'], other_name,))) for other_name in row['lower_names']]\n",
    "#     row['u_sorted_pairs'] = [tuple(sorted((row['name'], other_name,))) for other_name in row['upper_names']]\n",
    "#     row['a_sorted_pairs'] = [tuple(sorted((row['name'], other_name,))) for other_name in row['all_names']]\n",
    "    \n",
    "#     return row\n",
    "\n",
    "# pairing_data = pairing_data.apply(get_pairs_with_names, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small enough to git commit\n",
    "# pairing_data.to_pickle(os.path.join(root_path, 'DATA/stir_fry_pairing_data.pickle'))\n",
    "pairing_data = pd.read_pickle(os.path.join(root_path, 'DATA/stir_fry_pairing_data.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Creating \"clashes with\" data \\[gonna start out without this one, for stir fry\\]\n",
    "### (but still run through, to add it to data just in case?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_name_clashes_blank = stir_fry_data[['name', 'protein_cheese_sub', 'stir_fry_allium', 'fruit', 'veg']].copy()\n",
    "# names = stir_fry_data['name'].values.tolist()\n",
    "\n",
    "# for i, col_name in enumerate(names):\n",
    "#     name_name_clashes_blank[col_name] = pd.Series(['x']*(i+1) + ['']*(len(names)-(i+1)))\n",
    "\n",
    "# name_name_clashes_blank.to_csv(os.path.join(root_path, 'DATA/name_name_clashes_blank.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_name_clashes_input = pd.read_csv(os.path.join(root_path, 'DATA/name_name_clashes_input.csv'))\n",
    "# name_name_clashes_input = name_name_clashes_input.replace([float('nan'), 'x'], '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/russell/flavor_tool/venv/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# columns = ['name'] + name_name_clashes_input['name'].values.tolist()\n",
    "# name_name_clashes_input = name_name_clashes_input[columns]\n",
    "\n",
    "# def get_clashes_with_data(row):\n",
    "#     data = pd.Series([])\n",
    "#     name = row['name']\n",
    "#     data['name'] = name\n",
    "    \n",
    "#     lower_names = name_name_clashes_input['name'][name_name_clashes_input[name] == 'y'].values.tolist()\n",
    "#     upper_names = name_name_clashes_input['name'][name_name_clashes_input[name] == 'Y'].values.tolist()\n",
    "#     if 'y' in lower_names:\n",
    "#         print(lower_names)\n",
    "    \n",
    "#     for name in name_name_clashes_input['name']:\n",
    "#         if row[name] == 'y':\n",
    "#             lower_names.append(name)\n",
    "#         elif row[name] == 'Y':\n",
    "#             upper_names.append(name)\n",
    "    \n",
    "#     lower_names = list(set(lower_names))\n",
    "#     upper_names = list(set(upper_names))\n",
    "    \n",
    "#     data['lower_clashes_with_names'] = lower_names\n",
    "#     data['upper_clashes_with_names'] = upper_names\n",
    "#     data['all_clashes_with_names'] = list(set(lower_names + upper_names)) # shouldn't be overlap here, but hey\n",
    "    \n",
    "#     data['lower_clashes_with_pairs'] = [tuple(sorted([name, lower_name])) for lower_name in data['lower_clashes_with_names']]\n",
    "#     data['upper_clashes_with_pairs'] = [tuple(sorted([name, lower_name])) for lower_name in data['upper_clashes_with_names']]\n",
    "#     data['all_clashes_with_pairs'] = [tuple(sorted([name, lower_name])) for lower_name in data['all_clashes_with_names']]\n",
    "    \n",
    "#     return data\n",
    "\n",
    "# clashes_with_data = name_name_clashes_input.apply(get_clashes_with_data, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SYNC PAIRING DATA MATRIX (make sure [a][b] agrees with [b][a])\n",
    "\n",
    "# name_name_clashes_synced = name_name_clashes_input.copy()\n",
    "\n",
    "# for index_1, name_1 in enumerate(name_name_clashes_input['name'].values.tolist()):\n",
    "#     for index_2, name_2 in enumerate(name_name_clashes_input['name'].values.tolist()):\n",
    "#         value_1 = name_name_clashes_input[name_1][index_2]\n",
    "#         value_2 = name_name_clashes_input[name_2][index_1]\n",
    "        \n",
    "#         if name_1 == name_2:\n",
    "#             proper_value = ''\n",
    "#         elif value_1 == 'Y' or value_2 == 'Y':\n",
    "#             proper_value = 'Y'\n",
    "#         elif value_1 == 'y' or value_2 == 'y':\n",
    "#             proper_value = 'y'\n",
    "#         else:\n",
    "#             proper_value = ''\n",
    "        \n",
    "#         name_name_clashes_synced[name_1][index_2] = proper_value\n",
    "#         name_name_clashes_synced[name_2][index_1] = proper_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add name_name_clashes_synced to clashes_with_data\n",
    "# clashes_with_data[name_name_clashes_synced['name'].tolist()] = name_name_clashes_synced[name_name_clashes_synced['name'].tolist()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clashes_with_data.to_pickle(os.path.join(root_path, 'DATA/stir_fry_clashes_with_data.pickle'))\n",
    "clashes_with_data = pd.read_pickle(os.path.join(root_path, 'DATA/stir_fry_clashes_with_data.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Adding umbrella data\n",
    "- Because if 'onion' umbrella pairs with eg. olive oil, all onions should be paired with olive oil as at least c/C.\n",
    "- So what I'm doing here is\n",
    "    - matching umbrella-names' terms to regular ol names\n",
    "    - then marking all ingredients that fall under a specific umbrella\n",
    "    - then for each reg-term for each umbrella-name, I'll add a c/C connection value if existing connection is lower\n",
    "- and let the records show I'm deciding that\n",
    "    - if onions D pair with olive oil, but white onions c pair with olive oil, then imma overwrite c with D\n",
    "    - and if onions c pair, white onions D pair I'd write D for white onions\n",
    "    - (so I'm going with the bigger mark, just like when syncing)\n",
    "### (also, the first part of this doesn't need to be run regularly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# umbrella_data = ingredients_data[(ingredients_data['stir_fry_yes'] == 'y') & (ingredients_data['stir_fry_useful_umbrella'] == 'y') & (ingredients_data['redirect'] != 'y')]\n",
    "# umbrella_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# stir_fry_data_with_umbrella = ingredients_data[(ingredients_data['stir_fry_yes'] == 'y') & (ingredients_data['redirect'] != 'y')]\n",
    "# stir_fry_data_with_umbrella.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # break entries in column that has 'pairs with' strings into lists of ingredient terms\n",
    "# umbrella_ingredient_pairs_with_terms = umbrella_data['pairs_with'].apply(get_terms_from_pairs_with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create list of all terms, ignoring case and excluding duplicates\n",
    "# umbrella_all_terms = list(set(umbrella_ingredient_pairs_with_terms.sum()))\n",
    "# umbrella_all_terms_lower = list(set([term.lower() for term in umbrella_ingredient_pairs_with_terms.sum()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !pip install inflect\n",
    "# import inflect\n",
    "# p = inflect.engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salad_matches = pd.read_csv(os.path.join(root_path, 'DATA/term_name_matches_specific.csv'))\n",
    "# stir_fry_matches = pd.read_csv(os.path.join(root_path, 'DATA/stir_fry_term_name_matches_plus.csv'))\n",
    "    \n",
    "# def get_tokens(phrase):\n",
    "#     tokens = [token.strip() for token in re.split('\\(|\\)|,|e\\.g\\.|esp\\.|and|—|or|aka|see|see also|;|and\\/or|\\*', phrase)]\n",
    "#     tokens = [p.singular_noun(token) or token for token in tokens if token != '']\n",
    "#     return tokens\n",
    "\n",
    "# def get_mark(name, term):\n",
    "#     try:\n",
    "#         stir_fry_match = stir_fry_matches[term][stir_fry_matches['name'] == name].iloc[0]\n",
    "#     except:\n",
    "#         stir_fry_match = None \n",
    "#     if stir_fry_match:\n",
    "#         if str(stir_fry_match) in ['0', 'nan']:\n",
    "#             return ''\n",
    "#         else:\n",
    "#             print('FOUND stir fry match')\n",
    "#             return stir_fry_match\n",
    "    \n",
    "#     try:\n",
    "#         salad_match = salad_matches[term][salad_matches['name'] == name].iloc[0]\n",
    "#     except:\n",
    "#         salad_match = None \n",
    "#     if salad_match:\n",
    "#         if str(salad_match) in ['0', 'nan']:\n",
    "#             return ''\n",
    "#         else:\n",
    "#             print('FOUND salad match')\n",
    "#             return salad_match\n",
    "    \n",
    "#     name_tokens = [token.lower() for token in get_tokens(name)]  \n",
    "#     term_tokens_mixed = get_tokens(term)\n",
    "#     term_tokens = [token.lower() for token in term_tokens_mixed]\n",
    "    \n",
    "#     primary_name = re.split('\\(|—|e\\.g\\.', name)[0].strip()\n",
    "#     primary_name_split = primary_name.split(', ')\n",
    "#     single_comma_primary_name = len(primary_name_split) == 2\n",
    "    \n",
    "# #     print()\n",
    "#     if 'e.g.' in term:\n",
    "#         if single_comma_primary_name:\n",
    "#             if name_tokens[0] == term_tokens[0]:\n",
    "#                 if name_tokens[1] in term_tokens: # specific name in e.g. term\n",
    "# #                     print('NAME TOKENS In TERM TOKENS')\n",
    "#                     term_i = term_tokens.index(name_tokens[1])\n",
    "#                     if term_tokens_mixed[term_i] == term_tokens_mixed[term_i].upper():\n",
    "#                         match = 'D'\n",
    "#                     else:\n",
    "#                         if term_tokens_mixed[0] == term_tokens_mixed[0].upper():\n",
    "#                             match = 'D'\n",
    "#                         else:\n",
    "#                             match = 'd'\n",
    "#                 else: # name matches only generic (pre e.g.) part of term\n",
    "#                     if term_tokens_mixed[0] == term_tokens_mixed[0].upper():\n",
    "#                         match = 'C'\n",
    "#                     else:\n",
    "#                         match = 'c'\n",
    "#             else:\n",
    "#                 match = ''\n",
    "#         else:\n",
    "#             if name_tokens[0] == term_tokens[0]: # name matches term before e.g.\n",
    "#                 if term_tokens_mixed[0] == term_tokens_mixed[0].upper():\n",
    "#                     match = 'C'\n",
    "#                 else:\n",
    "#                     match = 'c'\n",
    "#             elif name_tokens[0] in term_tokens[1:]: # name matches term after e.g.\n",
    "#                 term_i = term_tokens.index(name_tokens[0])\n",
    "#                 if term_tokens_mixed[term_i] == term_tokens_mixed[term_i].upper():\n",
    "#                     match = 'D'\n",
    "#                 else:\n",
    "#                     if term_tokens_mixed[0] == term_tokens_mixed[0].upper():\n",
    "#                         match = 'D'\n",
    "#                     else:\n",
    "#                         match = 'd'\n",
    "#             else:\n",
    "#                 match = ''\n",
    "#     else:\n",
    "#         if single_comma_primary_name:\n",
    "#             if ' '.join(primary_name_split).lower() in term.lower() or primary_name.lower() in term.lower(): # if primary_name is in term, any order\n",
    "#                 if term_tokens_mixed[0] == term_tokens_mixed[0].upper():\n",
    "#                     match = 'D'\n",
    "#                 else:\n",
    "#                     match = 'd'\n",
    "#             elif name_tokens[0] == term_tokens[0]: # if first part of primary name matches first token in term\n",
    "#                 if term_tokens_mixed[0] == term_tokens_mixed[0].upper():\n",
    "#                     match = 'C'\n",
    "#                 else:\n",
    "#                     match = 'c'\n",
    "#             else:\n",
    "#                 match = ''\n",
    "#         else:\n",
    "#             if name_tokens[0] in term_tokens: # if non-comma name anywhere in non-e.g. term_tokens\n",
    "#                 if term_tokens_mixed[0] == term_tokens_mixed[0].upper():\n",
    "#                     match = 'D'\n",
    "#                 else:\n",
    "#                     match = 'd'\n",
    "#             elif len(set(name_tokens).intersection(set(term_tokens))) > 0: # if there are any common tokens\n",
    "#                 if term_tokens_mixed[0] == term_tokens_mixed[0].upper():\n",
    "#                     match = 'C'\n",
    "#                 else:\n",
    "#                     match = 'c'\n",
    "#             else:\n",
    "#                 match = ''\n",
    "                \n",
    "#     if match == '':\n",
    "#         n_common = len(set(name_tokens).intersection(set(term_tokens)))\n",
    "#         if n_common != 0:\n",
    "#             match = n_common\n",
    "#     return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# umbrella_mark_data = []\n",
    "# for name in stir_fry_data['name']:\n",
    "#     print(name)\n",
    "#     umbrella_mark_data.append([get_mark(name, term) for term in umbrella_all_terms])\n",
    "\n",
    "# umbrella_term_name_marks = pd.DataFrame(umbrella_mark_data, columns = umbrella_all_terms)\n",
    "# umbrella_term_name_marks['name'] = pd.Series(stir_fry_data['name'].values.tolist())\n",
    "\n",
    "# umbrella_term_name_marks.to_csv(os.path.join(root_path, 'DATA/stir_fry_useful_umbrella_term_name_marks.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# umbrella_term_name_matches_raw = pd.read_csv(os.path.join(root_path, 'DATA/stir_fry_umbrella_term_name_matches.csv'))\n",
    "# umbrella_term_name_matches = umbrella_term_name_matches_raw.replace(['0', '1', '2', '3', '4', '5', 0, 1, 2, 3, 4, 5, float('nan')], '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.69 s, sys: 0 ns, total: 2.69 s\n",
      "Wall time: 2.68 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# # CREATE PAIRING DATA MATRIX (names x names)\n",
    "\n",
    "# umbrella_pairing_data = pd.DataFrame({\n",
    "#     'name': umbrella_data['name'],\n",
    "#     'pairs_with_terms': umbrella_ingredient_pairs_with_terms\n",
    "# })\n",
    "\n",
    "# for name in stir_fry_data['name']:\n",
    "#     umbrella_pairing_data[name] = pd.Series(['']*len(umbrella_data))\n",
    "\n",
    "# def get_pairs_with_names(row):\n",
    "#     lower_category_names = []\n",
    "#     lower_direct_names = []\n",
    "#     upper_category_names = []\n",
    "#     upper_direct_names = []\n",
    "#     for term in row['pairs_with_terms']:\n",
    "#         if term in umbrella_term_name_matches.columns.values.tolist():\n",
    "#             lower_category_names += umbrella_term_name_matches[umbrella_term_name_matches[term] == 'c']['name'].values.tolist()\n",
    "#             lower_direct_names += umbrella_term_name_matches[umbrella_term_name_matches[term] == 'd']['name'].values.tolist()\n",
    "#             upper_category_names += umbrella_term_name_matches[umbrella_term_name_matches[term] == 'C']['name'].values.tolist()\n",
    "#             upper_direct_names += umbrella_term_name_matches[umbrella_term_name_matches[term] == 'D']['name'].values.tolist()\n",
    "#         else:\n",
    "#             pass\n",
    "#             print(term)\n",
    "    \n",
    "#     for lower_category_name in lower_category_names:\n",
    "#         row[lower_category_name] = 'c'\n",
    "#     for lower_direct_name in lower_direct_names:\n",
    "#         row[lower_direct_name] = 'd'\n",
    "#     for upper_category_name in upper_category_names:\n",
    "#         row[upper_category_name] = 'C'\n",
    "#     for upper_direct_name in upper_direct_names:\n",
    "#         row[upper_direct_name] = 'D'\n",
    "\n",
    "#     return row\n",
    "\n",
    "# umbrella_pairing_data = umbrella_pairing_data.apply(get_pairs_with_names, axis=1)\n",
    "# umbrella_pairing_data.replace(float('nan'), '', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating template to match ingredients to their umbrellas\n",
    "# umbrella_covered_blanks = pd.DataFrame({\n",
    "#     umbrella_name: ['']*len(stir_fry_data)\n",
    "# for umbrella_name in umbrella_data['name']})\n",
    "# umbrella_covered_blanks['name'] = stir_fry_data['name']\n",
    "# umbrella_covered_blanks.to_csv(os.path.join(root_path, 'DATA/stir_fry_umbrella_covered_blanks.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# umbrella_covered_matches_raw = pd.read_csv(os.path.join(root_path, 'DATA/stir_fry_umbrella_covered_matches.csv'))\n",
    "# umbrella_covered_matches = umbrella_covered_matches_raw.replace([float('nan')], '')\n",
    "# def get_covered_list(umbrella_name):\n",
    "#     return umbrella_covered_matches[umbrella_covered_matches[umbrella_name] == 'y']['name'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.8 s, sys: 44 ms, total: 38.9 s\n",
      "Wall time: 38.9 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# # 40s\n",
    "\n",
    "# pairing_data_with_umbrella_matches = pairing_data.copy()\n",
    "\n",
    "# for umbrella_name in umbrella_data['name']: # eg. onions\n",
    "#     for covered_name in get_covered_list(umbrella_name): # eg. white onion\n",
    "# #         print(umbrella_name, covered_name)\n",
    "#         for stir_fry_name in stir_fry_data['name']: # eg. olive oil\n",
    "#             umbrella_match_value = umbrella_pairing_data[umbrella_pairing_data['name'] == umbrella_name][stir_fry_name].iloc[0] # eg. D (because onions match with OLIVE OIL)\n",
    "# #             print(umbrella_match_value, umbrella_name, stir_fry_name)\n",
    "#             stir_fry_match_value = pairing_data[pairing_data['name'] == covered_name][stir_fry_name].iloc[0] # eg. 'c' (because white onions match with oil)\n",
    "# #             print(umbrella_match_value, stir_fry_match_value)\n",
    "        \n",
    "#             if umbrella_match_value == 'D' or stir_fry_match_value == 'D':\n",
    "#                 proper_match_value = 'D'\n",
    "#             elif umbrella_match_value == 'C' or stir_fry_match_value == 'C':\n",
    "#                 proper_match_value = 'C'\n",
    "#             elif umbrella_match_value == 'd' or stir_fry_match_value == 'd':\n",
    "#                 proper_match_value = 'd'\n",
    "#             elif umbrella_match_value == 'c' or stir_fry_match_value == 'c':\n",
    "#                 proper_match_value = 'c'\n",
    "#             else:\n",
    "#                 proper_match_value = ''\n",
    "                \n",
    "#             pairing_data_with_umbrella_matches.loc[pairing_data_with_umbrella_matches['name'] == covered_name, stir_fry_name] = proper_match_value\n",
    "#             pairing_data_with_umbrella_matches.loc[pairing_data_with_umbrella_matches['name'] == stir_fry_name, covered_name] = proper_match_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.07 s, sys: 0 ns, total: 2.07 s\n",
      "Wall time: 2.07 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "# # REPRESENT PAIRING DATA AS LISTS\n",
    "\n",
    "# def get_pairs_with_names(row):\n",
    "#     lower_category_name = pairing_data_with_umbrella_matches[pairing_data_with_umbrella_matches[row['name']] == 'c']['name'].values.tolist()\n",
    "#     lower_direct_name = pairing_data_with_umbrella_matches[pairing_data_with_umbrella_matches[row['name']] == 'd']['name'].values.tolist()\n",
    "#     upper_category_name = pairing_data_with_umbrella_matches[pairing_data_with_umbrella_matches[row['name']] == 'C']['name'].values.tolist()\n",
    "#     upper_direct_name = pairing_data_with_umbrella_matches[pairing_data_with_umbrella_matches[row['name']] == 'D']['name'].values.tolist()\n",
    "    \n",
    "#     row['lower_category_names'] = pairing_data_with_umbrella_matches[pairing_data_with_umbrella_matches[row['name']] == 'c']['name'].values.tolist()\n",
    "#     row['lower_direct_names'] = pairing_data_with_umbrella_matches[pairing_data_with_umbrella_matches[row['name']] == 'd']['name'].values.tolist()\n",
    "#     row['upper_category_names'] = pairing_data_with_umbrella_matches[pairing_data_with_umbrella_matches[row['name']] == 'C']['name'].values.tolist()\n",
    "#     row['upper_direct_names'] = pairing_data_with_umbrella_matches[pairing_data_with_umbrella_matches[row['name']] == 'D']['name'].values.tolist()\n",
    "#     row['lower_names'] = row['lower_category_names'] + row['lower_direct_names']\n",
    "#     row['upper_names'] = row['upper_category_names'] + row['upper_direct_names']\n",
    "#     row['all_names'] = row['lower_names'] + row['upper_names']\n",
    "    \n",
    "# #     row['lower_category_pairs_with_names'] = list(set([lower_category_name for lower_category_name in row['lower_category_names'] if lower_category_name != row['name']]))\n",
    "# #     row['lower_direct_pairs_with_names'] = list(set([lower_direct_name for lower_direct_name in row['lower_direct_names'] if lower_direct_name != row['name']]))\n",
    "# #     row['upper_category_pairs_with_names'] = list(set([upper_category_name for upper_category_name in row['upper_category_names'] if upper_category_name != row['name']]))\n",
    "# #     row['upper_direct_pairs_with_names'] = list(set([upper_direct_name for upper_direct_name in row['upper_direct_names'] if upper_direct_name != row['name']]))\n",
    "# #     row['lower_pairs_with_names'] = list(set(row['lower_category_pairs_with_names'] + row['lower_direct_pairs_with_names']))\n",
    "# #     row['upper_pairs_with_names'] = list(set(row['upper_category_pairs_with_names'] + row['upper_direct_pairs_with_names']))\n",
    "# #     row['all_pairs_with_names'] = list(set(row['lower_pairs_with_names'] + row['upper_pairs_with_names']))\n",
    "    \n",
    "#     row['lc_sorted_pairs'] = [tuple(sorted((row['name'], other_name,))) for other_name in row['lower_category_names']]\n",
    "#     row['ld_sorted_pairs'] = [tuple(sorted((row['name'], other_name,))) for other_name in row['lower_direct_names']]\n",
    "#     row['uc_sorted_pairs'] = [tuple(sorted((row['name'], other_name,))) for other_name in row['upper_category_names']]\n",
    "#     row['ud_sorted_pairs'] = [tuple(sorted((row['name'], other_name,))) for other_name in row['upper_direct_names']]\n",
    "#     row['l_sorted_pairs'] = [tuple(sorted((row['name'], other_name,))) for other_name in row['lower_names']]\n",
    "#     row['u_sorted_pairs'] = [tuple(sorted((row['name'], other_name,))) for other_name in row['upper_names']]\n",
    "#     row['a_sorted_pairs'] = [tuple(sorted((row['name'], other_name,))) for other_name in row['all_names']]\n",
    "    \n",
    "#     return row\n",
    "\n",
    "# pairing_data_with_umbrella_matches = pairing_data_with_umbrella_matches.apply(get_pairs_with_names, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairing_data_with_umbrella_matches.to_pickle(os.path.join(root_path, 'DATA/stir_fry_pairing_data_with_umbrella_matches.pickle'))\n",
    "pairing_data_with_umbrella_matches = pd.read_pickle(os.path.join(root_path, 'DATA/stir_fry_pairing_data_with_umbrella_matches.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Creating connection data \\[without clashes data, for now\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/russell/flavor_tool/venv/lib/python3.6/site-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.1 s, sys: 11.7 ms, total: 27.1 s\n",
      "Wall time: 27.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/russell/flavor_tool/venv/lib/python3.6/site-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# # 23s\n",
    "\n",
    "# # CREATING CONNECTION DATA AND ADDING IT TO stir_fry DATA (& getting stir_fry_data in order)\n",
    "# # takes a minute\n",
    "# stir_fry_names = stir_fry_data['name'].values.tolist()\n",
    "# for col_name in stir_fry_names:\n",
    "#     col_values = []\n",
    "#     for row_name in stir_fry_names:\n",
    "#         pairing_value = pairing_data_with_umbrella_matches[col_name][pairing_data_with_umbrella_matches['name'] == row_name].iloc[0]\n",
    "# #         clashing_value = clashes_with_data[col_name][clashes_with_data['name'] == row_name].iloc[0]\n",
    "\n",
    "# #         if clashing_value == 'Y':\n",
    "# #             col_values.append('N')\n",
    "# #         elif clashing_value == 'y':\n",
    "# #             col_values.append('n')\n",
    "# #         elif pairing_value == 'D':\n",
    "# #             col_values.append('D')\n",
    "# #         elif pairing_value == 'C':\n",
    "# #             col_values.append('C')\n",
    "# #         elif pairing_value == 'd':\n",
    "# #             col_values.append('d')\n",
    "# #         elif pairing_value == 'c':\n",
    "# #             col_values.append('c')\n",
    "# #         else:\n",
    "# #             col_values.append('')\n",
    "#         col_values.append(pairing_value)\n",
    "\n",
    "#     stir_fry_data[col_name] = pd.Series(col_values)\n",
    "    \n",
    "# # just in case\n",
    "# stir_fry_data.sort_values('name', inplace=True)\n",
    "# stir_fry_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stir_fry_data.to_pickle(os.path.join(root_path, 'DATA/stir_fry_data_with_names_and_umbrella_matches.pickle'))\n",
    "stir_fry_data = pd.read_pickle(os.path.join(root_path, 'DATA/stir_fry_data_with_names_and_umbrella_matches.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Creating flavor tool data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stir_fry_flavor_data = stir_fry_data[['staple', 'not_vegan', 'gluten', 'flavoring_sweet', 'flavoring_fresh', 'flavoring_wet', 'flavoring_concentrate', 'flavoring', 'flavoring_dry', 'protein', 'protein_cheese_sub', 'protein_milk_sub', 'protein_meat_sub', 'protein_bean', 'veg', 'veg_leafy', 'grain', 'grain_flour', 'fat_oil', 'oil', 'fat', 'fruit', 'fruit_berry', 'stir_fry', 'stir_fry_basic', 'stir_fry_yes', 'stir_fry_early', 'stir_fry_mid', 'stir_fry_late', 'stir_fry_garnish', 'stir_fry_umbrella', 'protein_nut_seed', 'protein_nut', 'protein_seed', 'redirect', 'salty', 'sour', 'spicy', 'bitter', 'savory', 'sweet','name', 'flavor', 'volume', 'pairs_with', 'phonetic', 'techniques', 'dishes', 'tip', 'possible_substitutes', 'flavor_affinities', 'nutritional_profile', 'season', 'botanical_relatives', 'protein_content', 'what_they_are', 'brands', 'vegan_substitutes', 'vegan_brands', 'stir_fry_protein', 'stir_fry_protein_nut_seed', 'stir_fry_protein_nut', 'stir_fry_protein_seed', 'stir_fry_flavoring', 'stir_fry_oil', 'stir_fry_fat', 'stir_fry_fat_oil', 'stir_fry_fruit', 'stir_fry_allium', 'stir_fry_protein_bean', 'stir_fry_veg', 'stir_fry_grain', 'stir_fry_salt', 'stir_fry_pepper', 'stir_fry_vinegar', 'strong', 'stir_fry_mushroom', 'stir_fry_main', 'stir_fry_useful_umbrella'] + stir_fry_data['name'].tolist()].copy()\n",
    "# stir_fry_flavor_data['id'] = range(len(stir_fry_flavor_data))\n",
    "\n",
    "# selected_pairing_data = pairing_data_with_umbrella_matches[['name', 'pairs_with_terms', 'lower_category_names', 'lower_direct_names', 'upper_category_names', 'upper_direct_names', 'lower_names', 'upper_names', 'all_names', 'lc_sorted_pairs', 'ld_sorted_pairs', 'uc_sorted_pairs', 'ud_sorted_pairs', 'l_sorted_pairs', 'u_sorted_pairs', 'a_sorted_pairs']]\n",
    "# stir_fry_flavor_data = stir_fry_flavor_data.merge(selected_pairing_data, how='inner', on='name')\n",
    "\n",
    "# # selected_clashes_with_data = clashes_with_data[['name', 'lower_clashes_with_names', 'upper_clashes_with_names', 'all_clashes_with_names', 'lower_clashes_with_pairs', 'upper_clashes_with_pairs', 'all_clashes_with_pairs']]\n",
    "# # stir_fry_flavor_data = stir_fry_flavor_data.merge(selected_clashes_with_data, how='inner', on='name')\n",
    "\n",
    "# stir_fry_flavor_data.replace(float('nan'), '', inplace=True)\n",
    "# stir_fry_flavor_data.set_index('name', drop=False, inplace=True)\n",
    "# stir_fry_flavor_data.rename_axis('name_index', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/russell/flavor_tool/venv/lib/python3.6/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.1 s, sys: 40 ms, total: 13.1 s\n",
      "Wall time: 13.1 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# # 13s\n",
    "\n",
    "# # ADD STRENGTH VALUES\n",
    "# ref_flavor_data = stir_fry_flavor_data.copy()\n",
    "# for i_1, row_1 in ref_flavor_data.iterrows():\n",
    "#     for i_2, row_2 in ref_flavor_data.iterrows():\n",
    "#         pairs_with_value = ref_flavor_data[row_1['name']][row_2['name']] or '_'\n",
    "#         if row_1['strong'] in ['y', 'Y'] and row_2['strong'] in ['y', 'Y']:\n",
    "#             strength_value = 'S'\n",
    "#         elif row_1['strong'] in ['y', 'Y'] or row_2['strong'] in ['y', 'Y']:\n",
    "#             strength_value = 's'\n",
    "#         else:\n",
    "#             strength_value = '_'\n",
    "            \n",
    "#         stir_fry_flavor_data[row_1['name']][row_2['name']] = pairs_with_value + strength_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stir_fry_flavor_data.to_pickle(os.path.join(root_path, 'DATA/stir_fry_flavor_data_with_umbrella.pickle'))\n",
    "# stir_fry_flavor_data.to_pickle(os.path.join(root_path, '../data/stir_fry_flavor_data_with_umbrella.pickle'))\n",
    "stir_fry_flavor_data = pd.read_pickle(os.path.join(root_path, 'DATA/stir_fry_flavor_data_with_umbrella.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. stir_fry recipe generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. Regular generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install networkx==2.4\n",
    "# !pip install pyvis\n",
    "\n",
    "import networkx as nx\n",
    "from pyvis import network as net\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_salts = 1\n",
    "n_fat_oils = 1\n",
    "n_other_flavorings_min = 1\n",
    "n_other_flavorings_max = 3\n",
    "n_foodstuffs_min = 3\n",
    "n_foodstuffs_max = 7\n",
    "\n",
    "# n_locked = random.randrage(0, 4)\n",
    "n_locked = 3\n",
    "locked_names = stir_fry_flavor_data.sample(n_locked)['name'].tolist()\n",
    "\n",
    "n_present = len(stir_fry_flavor_data)\n",
    "present_names = stir_fry_flavor_data.sample(n_present)['name'].tolist()\n",
    "\n",
    "stir_fry_data = stir_fry_flavor_data[stir_fry_flavor_data['name'].isin(present_names)].copy()\n",
    "\n",
    "locked = stir_fry_data[stir_fry_data['name'].isin(locked_names)]\n",
    "locked_fat_oils = locked[locked['stir_fry_fat_oil'] == 'y']\n",
    "locked_salts = locked[locked['stir_fry_salt'] == 'y']\n",
    "locked_other_flavorings = locked[(locked['stir_fry_flavoring'] == 'y') & (locked['stir_fry_salt'] != 'y')]\n",
    "locked_foodstuffs = locked[(locked['stir_fry_fat_oil'] != 'y') & (locked['stir_fry_salt'] != 'y') & (locked['stir_fry_flavoring'] != 'y')]\n",
    "\n",
    "the_rest = stir_fry_data[~stir_fry_data['name'].isin(locked['name'])]\n",
    "the_rest_fat_oils = the_rest[the_rest['stir_fry_fat_oil'] == 'y']\n",
    "the_rest_salts = the_rest[the_rest['stir_fry_salt'] == 'y']\n",
    "the_rest_other_flavorings = the_rest[(the_rest['stir_fry_flavoring'] == 'y') & (the_rest['stir_fry_salt'] != 'y')]\n",
    "the_rest_foodstuffs = the_rest[(the_rest['stir_fry_fat_oil'] != 'y') & (the_rest['stir_fry_salt'] != 'y') & (the_rest['stir_fry_flavoring'] != 'y')]\n",
    "\n",
    "n_additional_salts_needed = max(n_salts - len(locked_salts), 0)\n",
    "n_additional_salts_actual = min(n_additional_salts_needed, len(the_rest_salts))\n",
    "n_total_salts_actual = n_additional_salts_actual + len(locked_salts)\n",
    "\n",
    "n_additional_fat_oils_needed = max(n_fat_oils - len(locked_fat_oils), 0)\n",
    "n_additional_fat_oils_actual = min(n_additional_fat_oils_needed, len(the_rest_fat_oils))\n",
    "n_total_fat_oils_actual = n_additional_fat_oils_actual + len(locked_fat_oils)\n",
    "\n",
    "n_additional_other_flavorings_needed_min = max(n_other_flavorings_min - len(locked_other_flavorings), 0)\n",
    "n_additional_other_flavorings_actual_min = min(n_additional_other_flavorings_needed_min, len(the_rest_other_flavorings))\n",
    "n_total_other_flavorings_actual_min = n_additional_other_flavorings_actual_min + len(locked_other_flavorings)\n",
    "\n",
    "n_additional_other_flavorings_needed_max = max(n_other_flavorings_max - len(locked_other_flavorings), 0) # yikes, I had this as min..\n",
    "n_additional_other_flavorings_actual_max = min(n_additional_other_flavorings_needed_max, len(the_rest_other_flavorings))\n",
    "n_total_other_flavorings_actual_max = n_additional_other_flavorings_actual_max + len(locked_other_flavorings)\n",
    "\n",
    "n_additional_foodstuffs_needed_min = max(n_foodstuffs_min - len(locked_foodstuffs), 0)\n",
    "n_additional_foodstuffs_actual_min = min(n_additional_foodstuffs_needed_min, len(the_rest_foodstuffs))\n",
    "n_total_foodstuffs_actual_min = n_additional_foodstuffs_actual_min + len(locked_foodstuffs)\n",
    "\n",
    "n_additional_foodstuffs_needed_max = max(n_foodstuffs_max - len(locked_foodstuffs), 0)\n",
    "n_additional_foodstuffs_actual_max = min(n_additional_foodstuffs_needed_max, len(the_rest_foodstuffs))\n",
    "n_total_foodstuffs_actual_max = n_additional_foodstuffs_actual_max + len(locked_foodstuffs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"500px\"\n",
       "            height=\"500px\"\n",
       "            src=\"regular_net.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f06a545c1d0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_iterations = 300\n",
    "top_score = None\n",
    "for iteration in range(n_iterations):\n",
    "    n_additional_other_flavorings_actual = random.randrange(n_additional_other_flavorings_actual_min, n_additional_other_flavorings_actual_max+1)\n",
    "    n_additional_foodstuffs_actual = random.randrange(n_additional_foodstuffs_actual_min, n_additional_foodstuffs_actual_max+1)\n",
    "\n",
    "    selected_ingredients = locked\n",
    "    if n_additional_salts_actual > 0:\n",
    "        selected_ingredients = selected_ingredients.append(the_rest_salts.sample(n_additional_salts_actual))\n",
    "    if n_additional_fat_oils_actual > 0:\n",
    "        selected_ingredients = selected_ingredients.append(the_rest_fat_oils.sample(n_additional_fat_oils_actual))\n",
    "    if n_additional_other_flavorings_actual > 0:\n",
    "        selected_ingredients = selected_ingredients.append(the_rest_other_flavorings.sample(n_additional_other_flavorings_actual))\n",
    "    if n_additional_foodstuffs_actual > 0:\n",
    "        selected_ingredients = selected_ingredients.append(the_rest_foodstuffs.sample(n_additional_foodstuffs_actual))\n",
    "\n",
    "    selected_names = selected_ingredients['name'].values.tolist()\n",
    "\n",
    "    selected_g = nx.Graph()\n",
    "    selected_g.add_nodes_from(selected_names)\n",
    "\n",
    "    for i_1, name_1 in enumerate(selected_names[:-1]):\n",
    "        for i_2, name_2 in enumerate(selected_names[i_1+1:], i_1+1):\n",
    "            connection = selected_ingredients[name_1][name_2]\n",
    "\n",
    "            # Weights super guess-y\n",
    "            if connection[0] == 'c':\n",
    "                selected_g.add_edge(name_1, name_2, length=1, weight=.3) # prev .8\n",
    "            elif connection[0] == 'd':\n",
    "                # pairs_with_demerit = .5 # prev .6666\n",
    "                selected_g.add_edge(name_1, name_2, length=.7, weight=.6) # prev .8\n",
    "            elif connection[0] == 'C':\n",
    "                # pairs_with_demerit = .4 # prev .5333\n",
    "                selected_g.add_edge(name_1, name_2, length=.6, weight=.7) # prev .8\n",
    "            elif connection[0] == 'D':\n",
    "                # pairs_with_demerit = .3 # prev. .4\n",
    "                selected_g.add_edge(name_1, name_2, length=.3, weight=1) # prev .8\n",
    "\n",
    "    # Try again, friend\n",
    "    if not nx.is_connected(selected_g):\n",
    "#         print(str(iteration)+': NOT CONNECTED; SKIPPING TO NEXT ITERATION')\n",
    "        continue\n",
    "\n",
    "    score = 0\n",
    "\n",
    "    # CONNECTED PAIRING BONUS ==============================================\n",
    "    # I want this to be VERY important. I feel this holds a lot of the strength of recipe,\n",
    "    # and also encompasses strength-ness and locked-ness\n",
    "    # ranges from roughly (0 to 1) * 3, tho could be a lil over or under that range\n",
    "    average_shortest_path_length = nx.average_shortest_path_length(selected_g, weight='length')\n",
    "    pairing_score = 1 / average_shortest_path_length * 1.4 - 1 # good enough (for small, large pools)\n",
    "    # print('PAIRING SCORE', pairing_score)\n",
    "    score += pairing_score * 5\n",
    "\n",
    "    # Used for both strength and locked bonus:\n",
    "    node_degrees = list(selected_g.degree(weight='weight'))\n",
    "    average_degree = sum([node_degree[1] for node_degree in node_degrees]) / len(node_degrees)\n",
    "\n",
    "    # STRENGTH BONUS =======================================================\n",
    "    strength_above_average = 0\n",
    "    for node_degree in node_degrees:\n",
    "        if selected_ingredients['strong'][node_degree[0]] == 'Y':\n",
    "            strength_above_average += (node_degree[1] - average_degree)\n",
    "        elif selected_ingredients['strong'][node_degree[0]] == 'y':\n",
    "            strength_above_average += (node_degree[1] - average_degree) * .5\n",
    "    strength_score = strength_above_average * .2 + .5 # reasonable for small and full pools\n",
    "    # print('STRENGTH SCORE', strength_score)\n",
    "    score += strength_score\n",
    "\n",
    "    # LOCKED BONUS =========================================================\n",
    "    locked_above_average = 0\n",
    "    for node_degree in node_degrees:\n",
    "        if node_degree[0] in locked_names:\n",
    "            locked_above_average += node_degree[1] - average_degree\n",
    "    locked_score = locked_above_average * .2 + .5 # close enough (has to cover few locked, lotta locked, small pool, big pool - yeesh.)\n",
    "    # print('LOCKED SCORE', locked_score)\n",
    "    score += locked_score * 2\n",
    "\n",
    "# FLAVOR BALANCE BONUS =============================================================================================\n",
    "# ranges from roughly (0 to 1) * 1 (could be a lil over/under)\n",
    "    n_sweet_lower = (selected_ingredients['sweet'] == 'y').sum()\n",
    "    n_sweet_upper = (selected_ingredients['sweet'] == 'Y').sum()\n",
    "    n_salty_lower = (selected_ingredients['salty'] == 'y').sum()\n",
    "    n_salty_upper = (selected_ingredients['salty'] == 'Y').sum()\n",
    "    n_sour_lower = (selected_ingredients['sour'] == 'y').sum()\n",
    "    n_sour_upper = (selected_ingredients['sour'] == 'Y').sum()\n",
    "    n_savory_lower = (selected_ingredients['savory'] == 'y').sum()\n",
    "    n_savory_upper = (selected_ingredients['savory'] == 'Y').sum()\n",
    "    n_bitter_lower = (selected_ingredients['bitter'] == 'y').sum()\n",
    "    n_bitter_upper = (selected_ingredients['bitter'] == 'Y').sum()\n",
    "    n_spicy_lower = (selected_ingredients['spicy'] == 'y').sum()\n",
    "    n_spicy_upper = (selected_ingredients['spicy'] == 'Y').sum()\n",
    "\n",
    "    sweet_score = min(n_sweet_lower/2 + n_sweet_upper, 1)\n",
    "    salty_score = min(n_salty_lower/2 + n_salty_upper, 1)\n",
    "    sour_score = min(n_sour_lower/2 + n_sour_upper, 1)\n",
    "    savory_score = min(n_savory_lower/2 + n_savory_upper, 1)\n",
    "    bitter_score = min(n_bitter_lower/2 + n_bitter_upper, 1)\n",
    "    spicy_score = min(n_spicy_lower/2 + n_spicy_upper, 1)\n",
    "\n",
    "    flavor_score = 0\n",
    "    flavor_score += sweet_score*3 # rly want something sweet in there\n",
    "    flavor_score += salty_score*.5 # can always use salt\n",
    "    flavor_score += sour_score*2 # like me some sour\n",
    "    flavor_score += savory_score*3 # LOVE me some savory\n",
    "    flavor_score += bitter_score # idk\n",
    "    flavor_score += spicy_score*2 # can be nice\n",
    "    flavor_score = flavor_score * .15 - .75\n",
    "\n",
    "    # print('FLAVOR BALANCE SCORE', flavor_balance_score)\n",
    "    score += flavor_score\n",
    "\n",
    "    # FOOD GROUPS BONUS ==========================================================================================\n",
    "    if 'y' in selected_ingredients['stir_fry_protein'].values:\n",
    "        protein_score = .5\n",
    "    else:\n",
    "        protein_score = 0\n",
    "\n",
    "    if 'y' in selected_ingredients['stir_fry_fruit'].values:\n",
    "        fruit_score = .5\n",
    "    else:\n",
    "        fruit_score = 0\n",
    "\n",
    "    food_group_score = protein_score + fruit_score\n",
    "    # print('PROTEIN FRUIT', protein_score, fruit_score)\n",
    "    score += food_group_score\n",
    "\n",
    "\n",
    "    if top_score == None or score > top_score:\n",
    "        top_selected_ingredients = selected_ingredients\n",
    "        top_pairing_score = pairing_score\n",
    "        top_strength_score = strength_score\n",
    "        top_locked_score = locked_score\n",
    "        top_flavor_score = flavor_score\n",
    "        top_food_group_score = food_group_score\n",
    "        top_score = score\n",
    "\n",
    "regular_net = net.Network(notebook=True)\n",
    "\n",
    "top_selected_names = top_selected_ingredients['name'].tolist()\n",
    "nodes = top_selected_names\n",
    "\n",
    "def get_color(row):\n",
    "#     return 'grey'\n",
    "    if row['stir_fry_veg'] == 'y':\n",
    "        return 'green'\n",
    "    elif row['stir_fry_fruit'] == 'y':\n",
    "        return 'orange'\n",
    "    elif row['stir_fry_protein'] == 'y':\n",
    "        return 'brown'\n",
    "    elif row['stir_fry_grain'] == 'y':\n",
    "        return 'tan'\n",
    "    else:\n",
    "        return 'lightgrey'\n",
    "nodes_color = top_selected_ingredients.apply(get_color, axis=1).tolist()\n",
    "\n",
    "regular_net.add_nodes(\n",
    "    nodes=nodes,\n",
    "    color=nodes_color\n",
    ")\n",
    "\n",
    "for i_1, name_1 in enumerate(top_selected_names[:-1]):\n",
    "    for i_2, name_2 in enumerate(top_selected_names[i_1+1:], i_1+1):\n",
    "        connection = top_selected_ingredients[name_1][name_2]\n",
    "#         if connection[0] == 'c':\n",
    "#             clique_net.add_edge(name_1, name_2, physics=False, color='lightgrey')\n",
    "        if connection[0] == 'd':\n",
    "            clique_net.add_edge(name_1, name_2, physics=True, color='whitesmoke')\n",
    "        elif connection[0] == 'C':\n",
    "            clique_net.add_edge(name_1, name_2, physics=True, color='lightgrey')\n",
    "        elif connection[0] == 'D':\n",
    "            clique_net.add_edge(name_1, name_2, physics=True, color='black')\n",
    "\n",
    "regular_net.show('regular_net.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. Black magic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.1. Preparing subgraph data\n",
    "(stuff that can be pre-calculated, pickled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "# import community\n",
    "from networkx.algorithms.clique import enumerate_all_cliques, find_cliques, cliques_containing_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_ingredients = stir_fry_flavor_data.copy()#.sample(200)\n",
    "selected_names = selected_ingredients['name'].tolist()\n",
    "\n",
    "lower_category_pairs = []\n",
    "lower_direct_pairs = []\n",
    "upper_category_pairs = []\n",
    "upper_direct_pairs = []\n",
    "\n",
    "for i_1, name_1 in enumerate(selected_names[:-1]):\n",
    "    for i_2, name_2 in enumerate(selected_names[i_1+1:], i_1+1):\n",
    "        connection = selected_ingredients[name_1][name_2] # this is what is finicky\n",
    "        if connection[0] == 'c':\n",
    "            lower_category_pairs.append((name_1, name_2,))\n",
    "        elif connection[0] == 'd':\n",
    "            lower_direct_pairs.append((name_1, name_2,))\n",
    "        elif connection[0] == 'C':\n",
    "            upper_category_pairs.append((name_1, name_2,))\n",
    "        elif connection[0] == 'D':\n",
    "            upper_direct_pairs.append((name_1, name_2,))\n",
    "lower_pairs = lower_category_pairs + lower_direct_pairs\n",
    "upper_pairs = upper_category_pairs + upper_direct_pairs\n",
    "all_pairs = lower_pairs + upper_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# G = nx.Graph()\n",
    "# G.add_nodes_from(selected_names)\n",
    "# G.add_edges_from(upper_category_pairs)\n",
    "# G.add_edges_from(upper_direct_pairs)\n",
    "\n",
    "# cliques_gen = find_cliques(G)\n",
    "# cliques_all = list(cliques_gen)\n",
    "# cliques = [clique for clique in cliques_all if len(clique) > 1]\n",
    "# clique_sets = [set(clique) for clique in cliques]\n",
    "\n",
    "# data = {\n",
    "#     s_name: ['y' if s_name in c_set else '' for c_set in clique_sets]\n",
    "# for s_name in selected_names}\n",
    "# clique_upper = pd.DataFrame(data)\n",
    "# clique_upper['type'] = ['clique_upper'] * len(clique_upper)\n",
    "# clique_upper['list'] = cliques\n",
    "# clique_upper['set'] = clique_sets\n",
    "# clique_upper['length'] = clique_upper['list'].apply(len)\n",
    "\n",
    "# clique_upper.to_pickle(os.path.join(root_path, 'DATA/stir_fry_clique_upper.pickle'))\n",
    "clique_upper = pd.read_pickle(os.path.join(root_path, 'DATA/stir_fry_clique_upper.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 279 ms, sys: 12 ms, total: 291 ms\n",
      "Wall time: 290 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "salt_set = set(stir_fry_flavor_data[stir_fry_flavor_data['stir_fry_salt'] == 'y']['name'])\n",
    "fat_oil_set = set(stir_fry_flavor_data[stir_fry_flavor_data['stir_fry_fat_oil'] == 'y']['name'])\n",
    "other_flavoring_set = set(stir_fry_flavor_data[stir_fry_flavor_data['stir_fry_flavoring'] == 'y']['name']) - salt_set\n",
    "foodstuff_set = set(stir_fry_flavor_data['name']) - salt_set - fat_oil_set - other_flavoring_set\n",
    "mushroom_set = set(stir_fry_flavor_data[stir_fry_flavor_data['stir_fry_mushroom'] == 'y']['name'])\n",
    "bean_set = set(stir_fry_flavor_data[stir_fry_flavor_data['stir_fry_protein_bean'] == 'y'])\n",
    "grain_set = set(stir_fry_flavor_data[stir_fry_flavor_data['stir_fry_grain'] == 'y'])\n",
    "\n",
    "clique_upper['salts'] = [list(salt_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "clique_upper['fat_oils'] = [list(fat_oil_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "clique_upper['other_flavorings'] = [list(other_flavoring_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "clique_upper['foodstuffs'] = [list(foodstuff_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "clique_upper['mushrooms'] = [list(mushroom_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "clique_upper['beans'] = [list(bean_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "clique_upper['grains'] = [list(grain_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "\n",
    "clique_upper['salts_set'] = [set(salt_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "clique_upper['fat_oils_set'] = [set(fat_oil_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "clique_upper['other_flavorings_set'] = [set(other_flavoring_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "clique_upper['foodstuffs_set'] = [set(foodstuff_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "clique_upper['mushrooms_set'] = [set(mushroom_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "clique_upper['beans_set'] = [set(bean_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "clique_upper['grains_set'] = [set(grain_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "\n",
    "clique_upper['n_salts'] = [len(salt_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "clique_upper['n_fat_oils'] = [len(fat_oil_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "clique_upper['n_other_flavorings'] = [len(other_flavoring_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "clique_upper['n_foodstuffs'] = [len(foodstuff_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "clique_upper['n_mushrooms'] = [len(mushroom_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THESE ARE THE ACTUAL CONTROLS FOR THE GENERATOR (I think? it's been a bit.)\n",
    "n_salts = 1\n",
    "n_fat_oils = 1\n",
    "n_other_flavorings_min = 1\n",
    "n_other_flavorings_max = 3\n",
    "n_foodstuffs_min = 3\n",
    "n_foodstuffs_max = 7\n",
    "mushrooms_cap = 1 # possible to have more than this, if there are locked mushrooms (I don't update sets after locked/gen is established)\n",
    "n_beans_max = 1\n",
    "n_grains_max = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important, don't delete.\n",
    "\n",
    "reasonable_clique_upper = clique_upper.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.58 s, sys: 15.7 ms, total: 3.59 s\n",
      "Wall time: 3.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def get_reasonable_data(row):\n",
    "    # done first, since it messes w foodstuffs\n",
    "    reasonable_n_mushrooms = min(row['n_mushrooms'], mushrooms_cap)\n",
    "    mushrooms_to_remove_set = set(random.sample(row['mushrooms'], row['n_mushrooms'] - reasonable_n_mushrooms))\n",
    "    reasonable_mushrooms_set = row['mushrooms_set'] - mushrooms_to_remove_set\n",
    "    reasonable_mushrooms = list(reasonable_mushrooms_set)\n",
    "    deshroomed_foodstuffs_set = row['foodstuffs_set'] - mushrooms_to_remove_set\n",
    "    deshroomed_foodstuffs = list(deshroomed_foodstuffs_set)\n",
    "    deshroomed_n_foodstuffs = len(deshroomed_foodstuffs)\n",
    "\n",
    "    deshroomed_set = row['set'].copy()\n",
    "    deshroomed_set -= mushrooms_to_remove_set\n",
    "    deshroomed_list = list(deshroomed_set)\n",
    "    deshroomed_length = len(deshroomed_list)\n",
    "    \n",
    "    reasonable_n_salts = min(row['n_salts'], n_salts)\n",
    "    reasonable_n_fat_oils = min(row['n_fat_oils'], n_fat_oils)\n",
    "    reasonable_n_other_flavorings = min(row['n_other_flavorings'], n_other_flavorings_max)\n",
    "    reasonable_n_foodstuffs = min(deshroomed_n_foodstuffs, n_foodstuffs_max)\n",
    "\n",
    "    salts_to_remove_set = set(random.sample(row['salts'], row['n_salts'] - reasonable_n_salts))\n",
    "    fat_oils_to_remove_set = set(random.sample(row['fat_oils'], row['n_fat_oils'] - reasonable_n_fat_oils))\n",
    "    other_flavorings_to_remove_set = set(random.sample(row['other_flavorings'], row['n_other_flavorings'] - reasonable_n_other_flavorings))\n",
    "    foodstuffs_to_remove_set = set(random.sample(deshroomed_foodstuffs, deshroomed_n_foodstuffs - reasonable_n_foodstuffs))\n",
    "    \n",
    "    reasonable_salts_set = row['salts_set'] - salts_to_remove_set\n",
    "    reasonable_fat_oils_set = row['fat_oils_set'] - fat_oils_to_remove_set\n",
    "    reasonable_other_flavorings_set = row['other_flavorings_set'] - other_flavorings_to_remove_set\n",
    "    reasonable_foodstuffs_set = deshroomed_foodstuffs_set - foodstuffs_to_remove_set\n",
    "    \n",
    "    reasonable_salts = list(reasonable_salts_set)\n",
    "    reasonable_fat_oils = list(reasonable_fat_oils_set)\n",
    "    reasonable_other_flavorings = list(reasonable_other_flavorings_set)\n",
    "    reasonable_foodstuffs = list(reasonable_foodstuffs_set)\n",
    "    \n",
    "    reasonable_set = deshroomed_set # omitting the copy here (for negligible speed)\n",
    "    reasonable_set -= salts_to_remove_set\n",
    "    reasonable_set -= fat_oils_to_remove_set\n",
    "    reasonable_set -= other_flavorings_to_remove_set\n",
    "    reasonable_set -= foodstuffs_to_remove_set\n",
    "    \n",
    "    reasonable_list = list(reasonable_set)\n",
    "    reasonable_length = len(reasonable_list)\n",
    "\n",
    "    return (\n",
    "        reasonable_n_salts,\n",
    "        reasonable_n_fat_oils,\n",
    "        reasonable_n_other_flavorings,\n",
    "        reasonable_n_foodstuffs,\n",
    "        reasonable_salts_set,\n",
    "        reasonable_fat_oils_set,\n",
    "        reasonable_other_flavorings_set,\n",
    "        reasonable_foodstuffs_set,\n",
    "        reasonable_salts,\n",
    "        reasonable_fat_oils,\n",
    "        reasonable_other_flavorings,\n",
    "        reasonable_foodstuffs,\n",
    "        reasonable_set,\n",
    "        reasonable_list,\n",
    "        reasonable_length\n",
    "    )\n",
    "\n",
    "reasonable_data = reasonable_clique_upper.apply(get_reasonable_data, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 61.4 ms, sys: 0 ns, total: 61.4 ms\n",
      "Wall time: 60.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "reasonable_clique_upper['reasonable_n_salts'] = reasonable_data.apply(lambda x: x[0])\n",
    "reasonable_clique_upper['reasonable_n_fat_oils'] = reasonable_data.apply(lambda x: x[1])\n",
    "reasonable_clique_upper['reasonable_n_other_flavorings'] = reasonable_data.apply(lambda x: x[2])\n",
    "reasonable_clique_upper['reasonable_n_foodstuffs'] = reasonable_data.apply(lambda x: x[3])\n",
    "reasonable_clique_upper['reasonable_salts_set'] = reasonable_data.apply(lambda x: x[4])\n",
    "reasonable_clique_upper['reasonable_fat_oils_set'] = reasonable_data.apply(lambda x: x[5])\n",
    "reasonable_clique_upper['reasonable_other_flavorings_set'] = reasonable_data.apply(lambda x: x[6])\n",
    "reasonable_clique_upper['reasonable_foodstuffs_set'] = reasonable_data.apply(lambda x: x[7])\n",
    "reasonable_clique_upper['reasonable_salts'] = reasonable_data.apply(lambda x: x[8])\n",
    "reasonable_clique_upper['reasonable_fat_oils'] = reasonable_data.apply(lambda x: x[9])\n",
    "reasonable_clique_upper['reasonable_other_flavorings'] = reasonable_data.apply(lambda x: x[10])\n",
    "reasonable_clique_upper['reasonable_foodstuffs'] = reasonable_data.apply(lambda x: x[11])\n",
    "reasonable_clique_upper['reasonable_set'] = reasonable_data.apply(lambda x: x[12])\n",
    "reasonable_clique_upper['reasonable_list'] = reasonable_data.apply(lambda x: x[13])\n",
    "reasonable_clique_upper['reasonable_length'] = reasonable_data.apply(lambda x: x[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT don't remove:\n",
    "\n",
    "reasonable_clique_upper = reasonable_clique_upper[reasonable_clique_upper['reasonable_list'].apply(lambda x: len(x) >= 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.11 s, sys: 3.81 ms, total: 3.11 s\n",
      "Wall time: 3.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "  \n",
    "G = nx.Graph()\n",
    "def get_connections_data(clique_list):\n",
    "    G.clear()\n",
    "    G.add_nodes_from(clique_list)\n",
    "    weighted_edges = []\n",
    "    connections = []\n",
    "    for i_1, name_1 in enumerate(clique_list[:-1]):\n",
    "        for i_2, name_2 in enumerate(clique_list[i_1+1:], i_1+1):\n",
    "            connection = stir_fry_flavor_data[name_1][name_2]\n",
    "    \n",
    "            # Really, since I'm using upper the only demerit values here will be .5333 and .4. but, I think I've adjusted variation to 0-1 anyway, so it should be ok (?)\n",
    "            if connection[0] == 'c':\n",
    "                pairs_with_demerit = .8\n",
    "            elif connection[0] == 'd':\n",
    "                pairs_with_demerit = .6666\n",
    "            elif connection[0] == 'C':\n",
    "                pairs_with_demerit = .5333\n",
    "            elif connection[0] == 'D':\n",
    "                pairs_with_demerit = .4\n",
    "            else:\n",
    "                print('OH NO! BAD PAIRING VALUE.')\n",
    "\n",
    "            if connection[1] == '_':\n",
    "                strength_demerit = .2\n",
    "            elif connection[1] == 's':\n",
    "                strength_demerit = .15\n",
    "            elif connection[1] == 'S':\n",
    "                strength_demerit = .1\n",
    "            else:\n",
    "                print('OH NO! BAD STRENGTH VALUE.')\n",
    "                \n",
    "            connection_length = pairs_with_demerit + strength_demerit\n",
    "            weighted_edges.append((name_1, name_2, connection_length))\n",
    "            connections.append((name_1, name_2, connection))\n",
    "            \n",
    "    G.add_weighted_edges_from(weighted_edges)\n",
    "    path_length = nx.average_shortest_path_length(G, weight='weight')\n",
    "    return (path_length, connections)\n",
    "\n",
    "connections_data = reasonable_clique_upper['reasonable_list'].apply(get_connections_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasonable_clique_upper['path_length'] = connections_data.apply(lambda x: x[0])\n",
    "reasonable_clique_upper['connections'] = connections_data.apply(lambda x: x[1])\n",
    "reasonable_clique_upper['connection_values'] = connections_data.apply(lambda x: [c[2] for c in x[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install networkx==2.4\n",
    "# !pip install pyvis\n",
    "# !pip install python-louvain\n",
    "# !pip install matplotlib\n",
    "import networkx as nx\n",
    "from pyvis import network as net\n",
    "import community\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.42 s, sys: 20 ms, total: 1.44 s\n",
      "Wall time: 1.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "fruit_set = set(stir_fry_flavor_data[stir_fry_flavor_data['stir_fry_fruit'] == 'y']['name'])\n",
    "protein_set = set(stir_fry_flavor_data[stir_fry_flavor_data['stir_fry_protein'] == 'y']['name'])\n",
    "bean_set = set(stir_fry_flavor_data[stir_fry_flavor_data['stir_fry_protein_bean'] == 'y']['name'])\n",
    "grain_set = set(stir_fry_flavor_data[stir_fry_flavor_data['stir_fry_grain'] == 'y']['name'])\n",
    "    \n",
    "max_length_upper = max(reasonable_clique_upper['reasonable_length'].tolist())\n",
    "\n",
    "def get_score(row):\n",
    "    \n",
    "    # NODE LENGTH BONUS\n",
    "    # scale lengths so that max is 10, then square to skew sampling toward long cliques, then scale to 1 max\n",
    "    nodes_length_score = (row['reasonable_length'] * 10/max_length_upper)**2 / 100\n",
    "    \n",
    "    # FOOD GROUPS BONUS\n",
    "    n_fruit = len(row['reasonable_set'].intersection(fruit_set))\n",
    "    n_protein = len(row['reasonable_set'].intersection(protein_set))\n",
    "    protein_score = (n_protein/2)**.5\n",
    "\n",
    "    fruit_score = (n_fruit/2)**.5\n",
    "    \n",
    "    food_group_score = protein_score + fruit_score\n",
    "    \n",
    "    # PATH LENGTH BONUS\n",
    "    path_length_score = 2.3/row['path_length'] - 3.1\n",
    "    \n",
    "    # RICE BEANS DEMERIT\n",
    "    n_beans = len(row['reasonable_set'].intersection(bean_set))\n",
    "    n_grains = len(row['reasonable_set'].intersection(grain_set))\n",
    "\n",
    "    # trying to keep this low, to tack onto 0-1 score\n",
    "    rice_beans_demerit = 0\n",
    "    if n_beans == 2:\n",
    "        rice_beans_demerit += .1\n",
    "    elif n_beans > 2:\n",
    "        rice_beans_demerit += .25\n",
    "    if n_grains == 2:\n",
    "        rice_beans_demerit += .1\n",
    "    elif n_grains > 2:\n",
    "        rice_beans_demerit += .25\n",
    "        \n",
    "    score = (nodes_length_score*1.6 + path_length_score*1 + food_group_score*.4)/3 - rice_beans_demerit\n",
    "    if score > 0:\n",
    "        return score\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "reasonable_clique_upper['score'] = reasonable_clique_upper.apply(get_score, axis=1)\n",
    "reasonable_clique_upper.sort_values('score', ascending=False, inplace=True) #IMPORTANT\n",
    "reasonable_clique_upper.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reasonable_clique_upper.to_pickle(os.path.join(root_path, 'DATA/stir_fry_reasonable_clique_upper.pickle'))\n",
    "# reasonable_clique_upper.to_pickle(os.path.join(root_path, '../data/stir_fry_reasonable_clique_upper.pickle'))\n",
    "# reasonable_clique_upper = pd.read_pickle(os.path.join(root_path, 'DATA/stir_fry_reasonable_clique_upper.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.57 ms, sys: 0 ns, total: 3.57 ms\n",
      "Wall time: 2.92 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "average_score_for_length = dict(reasonable_clique_upper.groupby('reasonable_length')['score'].mean())\n",
    "def get_average_score(length):\n",
    "    if length >= 2:\n",
    "        return average_score_for_length[length]\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weak_score(strong):\n",
    "    if strong == 'Y':\n",
    "        return 1\n",
    "    elif strong == 'y':\n",
    "        return 3\n",
    "    else:\n",
    "        return 9\n",
    "                        \n",
    "stir_fry_flavor_data['weak_score'] = stir_fry_flavor_data['strong'].apply(get_weak_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 382 ms, sys: 12 ms, total: 394 ms\n",
      "Wall time: 393 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "stir_fry_names = stir_fry_flavor_data['name']\n",
    "\n",
    "stir_fry_g = nx.Graph()\n",
    "stir_fry_g.add_nodes_from(stir_fry_names)\n",
    "weighted_edges = []\n",
    "for i_1, name_1 in enumerate(stir_fry_names[:-1]):\n",
    "    for i_2, name_2 in enumerate(stir_fry_names[i_1+1:], i_1+1):\n",
    "        connection = stir_fry_flavor_data[name_1][name_2]\n",
    "\n",
    "        if connection[0] == 'c':\n",
    "            pairs_with_demerit = .8\n",
    "        elif connection[0] == 'd':\n",
    "            pairs_with_demerit = .6666\n",
    "        elif connection[0] == 'C':\n",
    "            pairs_with_demerit = .5333\n",
    "        elif connection[0] == 'D':\n",
    "            pairs_with_demerit = .4\n",
    "\n",
    "        if connection[1] == '_':\n",
    "            strength_demerit = .2\n",
    "        elif connection[1] == 's':\n",
    "            strength_demerit = .15\n",
    "        elif connection[1] == 'S':\n",
    "            strength_demerit = .1\n",
    "\n",
    "        connection_length = pairs_with_demerit + strength_demerit\n",
    "        weighted_edges.append((name_1, name_2, connection_length))\n",
    "\n",
    "stir_fry_g.add_weighted_edges_from(weighted_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.99 s, sys: 3.73 ms, total: 3.99 s\n",
      "Wall time: 4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 3.98s\n",
    "\n",
    "stir_fry_shortest_path_lengths = {key: value for key, value in nx.shortest_path_length(stir_fry_g, weight='weight')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'to_pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-a2dd0a382c32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstir_fry_shortest_path_lengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DATA/stir_fry_shortest_path_lengths.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# stir_fry_shortest_path_lengths = pd.read_pickle(os.path.join(root_path, 'DATA/stir_fry_shortest_path_lengths.pickle'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'to_pickle'"
     ]
    }
   ],
   "source": [
    "# stir_fry_shortest_path_lengths.to_pickle(os.path.join(root_path, 'DATA/stir_fry_shortest_path_lengths.pickle'))\n",
    "# stir_fry_shortest_path_lengths = pd.read_pickle(os.path.join(root_path, 'DATA/stir_fry_shortest_path_lengths.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# import pickle\n",
    "# with open(os.path.join(root_path, 'DATA/stir_fry_shortest_path_lengths.pickle'), 'wb') as handle:\n",
    "#     pickle.dump(stir_fry_shortest_path_lengths, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open(os.path.join(root_path, '../data/stir_fry_shortest_path_lengths.pickle'), 'wb') as handle:\n",
    "#     pickle.dump(stir_fry_shortest_path_lengths, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open(os.path.join(root_path, 'DATA/stir_fry_shortest_path_lengths.pickle'), 'rb') as handle:\n",
    "#     stir_fry_shortest_path_lengths = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.2. Generator set-up\n",
    "(stuff to do once/call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.4 ms, sys: 24 µs, total: 17.4 ms\n",
      "Wall time: 16 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "n_present = len(stir_fry_flavor_data)\n",
    "present = stir_fry_flavor_data.sample(n_present)\n",
    "present_set = set(present['name'])\n",
    "present_strong_set = set(present[present['strong'].isin(['Y', 'y'])]['name'])\n",
    "not_present_set = set(stir_fry_flavor_data['name']) - present_set\n",
    "\n",
    "# n_locked = random.randrange(0, 6)\n",
    "n_locked = 3\n",
    "locked = present.sample(n_locked) # random for now, but will be handed down\n",
    "locked_set = set(locked['name'])\n",
    "locked_fat_oils = locked[locked['stir_fry_fat_oil'] == 'y']\n",
    "locked_salts = locked[locked['stir_fry_salt'] == 'y']\n",
    "locked_other_flavorings = locked[(locked['stir_fry_flavoring'] == 'y') & (locked['stir_fry_salt'] != 'y')]\n",
    "locked_foodstuffs = locked[(locked['stir_fry_fat_oil'] != 'y') & (locked['stir_fry_salt'] != 'y') & (locked['stir_fry_flavoring'] != 'y')] # why am I selecting not salt? already selecting not flavoring. don't wanna mess tho..\n",
    "locked_fat_oils_set = set(locked_fat_oils['name'])\n",
    "locked_salts_set = set(locked_salts['name'])\n",
    "locked_other_flavorings_set = set(locked_other_flavorings['name'])\n",
    "locked_foodstuffs_set = set(locked_foodstuffs['name'])\n",
    "\n",
    "the_rest = present[~present['name'].isin(locked['name'])]\n",
    "the_rest_set = set(the_rest['name'])\n",
    "the_rest_fat_oils = the_rest[the_rest['stir_fry_fat_oil'] == 'y']\n",
    "the_rest_salts = the_rest[the_rest['stir_fry_salt'] == 'y']\n",
    "the_rest_other_flavorings = the_rest[(the_rest['stir_fry_flavoring'] == 'y') & (the_rest['stir_fry_salt'] != 'y')]\n",
    "the_rest_foodstuffs = the_rest[(the_rest['stir_fry_fat_oil'] != 'y') & (the_rest['stir_fry_salt'] != 'y') & (the_rest['stir_fry_flavoring'] != 'y')]\n",
    "the_rest_fat_oils_set = set(the_rest_fat_oils['name'])\n",
    "the_rest_salts_set = set(the_rest_salts['name'])\n",
    "the_rest_other_flavorings_set = set(the_rest_other_flavorings['name'])\n",
    "the_rest_foodstuffs_set = set(the_rest_foodstuffs['name'])\n",
    "\n",
    "n_additional_salts_needed = max(n_salts - len(locked_salts), 0)\n",
    "n_additional_salts_actual = min(n_additional_salts_needed, len(the_rest_salts))\n",
    "n_total_salts_actual = n_additional_salts_actual + len(locked_salts)\n",
    "\n",
    "n_additional_fat_oils_needed = max(n_fat_oils - len(locked_fat_oils), 0)\n",
    "n_additional_fat_oils_actual = min(n_additional_fat_oils_needed, len(the_rest_fat_oils))\n",
    "n_total_fat_oils_actual = n_additional_fat_oils_actual + len(locked_fat_oils)\n",
    "\n",
    "n_additional_other_flavorings_needed_min = max(n_other_flavorings_min - len(locked_other_flavorings), 0)\n",
    "n_additional_other_flavorings_actual_min = min(n_additional_other_flavorings_needed_min, len(the_rest_other_flavorings))\n",
    "n_total_other_flavorings_actual_min = n_additional_other_flavorings_actual_min + len(locked_other_flavorings)\n",
    "\n",
    "n_additional_other_flavorings_needed_max = max(n_other_flavorings_max - len(locked_other_flavorings), 0) # yikes, I had this as min..\n",
    "n_additional_other_flavorings_actual_max = min(n_additional_other_flavorings_needed_max, len(the_rest_other_flavorings))\n",
    "n_total_other_flavorings_actual_max = n_additional_other_flavorings_actual_max + len(locked_other_flavorings)\n",
    "\n",
    "n_additional_foodstuffs_needed_min = max(n_foodstuffs_min - len(locked_foodstuffs), 0)\n",
    "n_additional_foodstuffs_actual_min = min(n_additional_foodstuffs_needed_min, len(the_rest_foodstuffs))\n",
    "n_total_foodstuffs_actual_min = n_additional_foodstuffs_actual_min + len(locked_foodstuffs)\n",
    "\n",
    "n_additional_foodstuffs_needed_max = max(n_foodstuffs_max - len(locked_foodstuffs), 0)\n",
    "n_additional_foodstuffs_actual_max = min(n_additional_foodstuffs_needed_max, len(the_rest_foodstuffs))\n",
    "n_total_foodstuffs_actual_max = n_additional_foodstuffs_actual_max + len(locked_foodstuffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 297 ms, sys: 0 ns, total: 297 ms\n",
      "Wall time: 298 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ok_cliques = reasonable_clique_upper[:1000].copy()\n",
    "\n",
    "def get_ok_data(row):\n",
    "    updated_set = row['reasonable_set'] - not_present_set\n",
    "    updated_salts_set = row['reasonable_salts_set'] - not_present_set\n",
    "    updated_fat_oils_set = row['reasonable_fat_oils_set'] - not_present_set\n",
    "    updated_other_flavorings_set = row['reasonable_other_flavorings_set'] - not_present_set\n",
    "    updated_foodstuffs_set = row['reasonable_foodstuffs_set'] - not_present_set\n",
    "\n",
    "    updated_n_salts = len(updated_salts_set)\n",
    "    updated_n_fat_oils = len(updated_fat_oils_set)\n",
    "    updated_n_other_flavorings = len(updated_other_flavorings_set)\n",
    "    updated_n_foodstuffs = len(updated_foodstuffs_set)\n",
    "\n",
    "    n_salts_so_far = len(updated_salts_set.union(locked_salts_set))\n",
    "    n_salts_to_remove = max(n_salts_so_far - n_total_salts_actual, 0)\n",
    "    salts_to_remove_from_set = updated_salts_set - locked_salts_set\n",
    "    more_salts_to_remove_set = set(random.sample(salts_to_remove_from_set, n_salts_to_remove))\n",
    "\n",
    "    n_fat_oils_so_far = len(updated_fat_oils_set.union(locked_fat_oils_set))\n",
    "    n_fat_oils_to_remove = max(n_fat_oils_so_far - n_total_fat_oils_actual, 0)\n",
    "    fat_oils_to_remove_from_set = updated_fat_oils_set - locked_fat_oils_set\n",
    "    more_fat_oils_to_remove_set = set(random.sample(fat_oils_to_remove_from_set, n_fat_oils_to_remove))\n",
    "\n",
    "    n_other_flavorings_so_far = len(updated_other_flavorings_set.union(locked_other_flavorings_set))\n",
    "    n_other_flavorings_to_remove = max(n_other_flavorings_so_far - n_total_other_flavorings_actual_max, 0) # is this right?\n",
    "    other_flavorings_to_remove_from_set = updated_other_flavorings_set - locked_other_flavorings_set\n",
    "    more_other_flavorings_to_remove_set = set(random.sample(salts_to_remove_from_set, n_salts_to_remove))\n",
    "\n",
    "    n_foodstuffs_so_far = len(updated_foodstuffs_set.union(locked_foodstuffs_set))\n",
    "    n_foodstuffs_to_remove = max(n_foodstuffs_so_far - n_total_foodstuffs_actual_max, 0) # is this right? thinks so - it's keeping foodstuffs from above max\n",
    "    foodstuffs_to_remove_from_set = updated_foodstuffs_set - locked_foodstuffs_set\n",
    "    more_foodstuffs_to_remove_set = set(random.sample(foodstuffs_to_remove_from_set, n_foodstuffs_to_remove))\n",
    "\n",
    "    ok_other_flavorings_set = updated_other_flavorings_set - more_other_flavorings_to_remove_set\n",
    "    ok_foodstuffs_set = updated_foodstuffs_set - more_foodstuffs_to_remove_set\n",
    "\n",
    "    ok_set = updated_set # not bothering w copy, for (negligible) speed reasons\n",
    "    ok_set -= more_salts_to_remove_set\n",
    "    ok_set -= more_fat_oils_to_remove_set\n",
    "    ok_set -= more_other_flavorings_to_remove_set\n",
    "    ok_set -= more_foodstuffs_to_remove_set\n",
    "\n",
    "    ok_list = list(ok_set)\n",
    "    ok_length = len(ok_list)\n",
    "    ok_n_locked = len(ok_set.intersection(locked_set))\n",
    "    ok_n_other_flavorings = len(ok_other_flavorings_set)\n",
    "    ok_n_foodstuffs = len(ok_foodstuffs_set)\n",
    "\n",
    "    old_score_bonus_factor = row['score']/get_average_score(row['reasonable_length']) # how above/below avg score was previously\n",
    "    ok_score = get_average_score(ok_length)*old_score_bonus_factor\n",
    "    ok_score_xtreme = ok_score**2 # skew scores way upward, for use as weights\n",
    "\n",
    "    ok_strong_set = ok_set.intersection(present_strong_set)\n",
    "\n",
    "    return (\n",
    "        ok_set,\n",
    "        ok_list,\n",
    "        ok_length,\n",
    "        ok_score,\n",
    "        ok_score_xtreme,\n",
    "        ok_n_locked,\n",
    "        ok_strong_set,\n",
    "        ok_n_other_flavorings,\n",
    "        ok_n_foodstuffs,\n",
    "    )\n",
    "ok_data = ok_cliques.apply(get_ok_data, axis=1)\n",
    "\n",
    "ok_cliques['ok_set'] = ok_data.apply(lambda x: x[0])\n",
    "ok_cliques['ok_list'] = ok_data.apply(lambda x: x[1])\n",
    "ok_cliques['ok_length'] = ok_data.apply(lambda x: x[2])\n",
    "ok_cliques['ok_score'] = ok_data.apply(lambda x: x[3])\n",
    "ok_cliques['ok_score_xtreme'] = ok_data.apply(lambda x: x[4])\n",
    "ok_cliques['ok_n_locked'] = ok_data.apply(lambda x: x[5])\n",
    "ok_cliques['ok_strong_set'] = ok_data.apply(lambda x: x[6])\n",
    "ok_cliques['ok_n_other_flavorings'] = ok_data.apply(lambda x: x[7])\n",
    "ok_cliques['ok_n_foodstuffs'] = ok_data.apply(lambda x: x[8])\n",
    "\n",
    "ok_cliques = ok_cliques[ok_cliques['ok_list'].apply(lambda x: len(x) >= 2)]\n",
    "ok_cliques = ok_cliques.sort_values('ok_score', ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.3. Generator\n",
    "(contains code that will execute many times / call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.87 s, sys: 7.79 ms, total: 3.88 s\n",
      "Wall time: 3.88 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"500px\"\n",
       "            height=\"500px\"\n",
       "            src=\"clique_net.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f069d5aae48>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def get_sort_key(length_tuple):\n",
    "    return length_tuple[1]\n",
    "\n",
    "def n_possible_edges(n_nodes):\n",
    "    return int((n_nodes*(n_nodes-1)) / 2)\n",
    "\n",
    "def get_first_name_in_set(sorted_tuples, food_set):\n",
    "    for sorted_tuple in sorted_tuples:\n",
    "        if sorted_tuple[0] in food_set:\n",
    "            return sorted_tuple[0]\n",
    "\n",
    "top_score = None\n",
    "n_iterations = 200\n",
    "for iteration in range(n_iterations):\n",
    "    n_total_other_flavorings_actual = random.randrange(n_total_other_flavorings_actual_min, n_total_other_flavorings_actual_max+1)\n",
    "    n_total_foodstuffs_actual = random.randrange(n_total_foodstuffs_actual_min, n_total_foodstuffs_actual_max+1)\n",
    "\n",
    "    n_additional_other_flavorings_actual = n_total_other_flavorings_actual - len(locked_other_flavorings)\n",
    "    n_additional_foodstuffs_actual = n_total_foodstuffs_actual - len(locked_foodstuffs)\n",
    "\n",
    "    final_cliques = ok_cliques[(ok_cliques['ok_n_other_flavorings'] <= n_additional_other_flavorings_actual) & (ok_cliques['ok_n_foodstuffs'] <= n_additional_foodstuffs_actual)]\n",
    "\n",
    "    if len(final_cliques) > 0: # BLACK MAGIC\n",
    "#         print('LET\\'S DO MAGIC!')\n",
    "        clique = final_cliques.sample(1, weights='ok_score_xtreme').iloc[0] # using xtreme to skew sample toward top\n",
    "\n",
    "        try:\n",
    "            clique_ingredients = present.loc[clique['ok_list']]\n",
    "        except:\n",
    "            print('MAJOR PROBLEM! Likely cause: clique_data contains not-present ingredients.')\n",
    "            clique_ingredients = stir_fry_flavor_data.loc[clique['ok_list']]\n",
    "\n",
    "        salts_so_far_set = clique['ok_set'].intersection(salt_set).union(locked_salts_set)\n",
    "        fat_oils_so_far_set = clique['ok_set'].intersection(fat_oil_set).union(locked_fat_oils_set)\n",
    "        other_flavorings_so_far_set = clique['ok_set'].intersection(other_flavoring_set).union(locked_other_flavorings_set)\n",
    "        foodstuffs_so_far_set = clique['ok_set'].intersection(foodstuff_set).union(locked_foodstuffs_set)\n",
    "        ingredients_so_far_set = clique['ok_set'].union(locked_set)\n",
    "\n",
    "        n_additional_non_clique_salts = n_total_salts_actual - len(salts_so_far_set) # pretty sure there shouldn't be more salts so far than salts actual\n",
    "        n_additional_non_clique_fat_oils = n_total_fat_oils_actual - len(fat_oils_so_far_set) # pretty sure there shouldn't be more salts so far than salts actual\n",
    "        n_additional_non_clique_other_flavorings = n_total_other_flavorings_actual - len(other_flavorings_so_far_set)\n",
    "        n_additional_non_clique_foodstuffs = n_total_foodstuffs_actual - len(foodstuffs_so_far_set)\n",
    "\n",
    "        selected_ingredients = present[present['name'].isin(ingredients_so_far_set)]\n",
    "\n",
    "        additional_salts_pool = the_rest_salts[~the_rest_salts['name'].isin(clique_ingredients['name'])]\n",
    "        if n_additional_non_clique_salts > 0:\n",
    "            additional_salts = additional_salts_pool.sample(n_additional_non_clique_salts, weights='weak_score')\n",
    "            selected_ingredients = selected_ingredients.append(additional_salts)\n",
    "\n",
    "        additional_fat_oils_pool = the_rest_fat_oils[~the_rest_fat_oils['name'].isin(clique_ingredients['name'])]\n",
    "        if n_additional_non_clique_fat_oils > 0:\n",
    "            additional_fat_oils = additional_fat_oils_pool.sample(n_additional_non_clique_fat_oils, weights='weak_score')\n",
    "            selected_ingredients = selected_ingredients.append(additional_fat_oils)\n",
    "\n",
    "        additional_other_flavorings_pool = the_rest_other_flavorings[~the_rest_other_flavorings['name'].isin(clique_ingredients['name'])]\n",
    "        if n_additional_non_clique_other_flavorings > 0:\n",
    "            additional_other_flavorings = additional_other_flavorings_pool.sample(n_additional_non_clique_other_flavorings, weights='weak_score')\n",
    "            selected_ingredients = selected_ingredients.append(additional_other_flavorings)\n",
    "\n",
    "        additional_foodstuffs_pool = the_rest_foodstuffs[~the_rest_foodstuffs['name'].isin(clique_ingredients['name'])]\n",
    "        if n_additional_non_clique_foodstuffs > 0:\n",
    "            additional_foodstuffs = additional_foodstuffs_pool.sample(n_additional_non_clique_foodstuffs, weights='weak_score')\n",
    "            selected_ingredients = selected_ingredients.append(additional_foodstuffs)\n",
    "    else: # REGULAR\n",
    "#         print('REGULAR IT IS.')\n",
    "        selected_ingredients = locked\n",
    "        if n_additional_salts_actual > 0:\n",
    "            selected_ingredients = selected_ingredients.append(the_rest_salts.sample(n_additional_salts_actual))\n",
    "        if n_additional_fat_oils_actual > 0:\n",
    "            selected_ingredients = selected_ingredients.append(the_rest_fat_oils.sample(n_additional_fat_oils_actual))\n",
    "        if n_additional_other_flavorings_actual > 0:\n",
    "            selected_ingredients = selected_ingredients.append(the_rest_other_flavorings.sample(n_additional_other_flavorings_actual))\n",
    "        if n_additional_foodstuffs_actual > 0:\n",
    "            selected_ingredients = selected_ingredients.append(the_rest_foodstuffs.sample(n_additional_foodstuffs_actual))\n",
    "\n",
    "    selected_names = selected_ingredients['name'].values.tolist()\n",
    "\n",
    "    selected_g = nx.Graph()\n",
    "    selected_g.add_nodes_from(selected_names)\n",
    "\n",
    "    for i_1, name_1 in enumerate(selected_names[:-1]):\n",
    "        for i_2, name_2 in enumerate(selected_names[i_1+1:], i_1+1):\n",
    "            connection = selected_ingredients[name_1][name_2]\n",
    "\n",
    "            # Weights super guess-y\n",
    "            if connection[0] == 'c':\n",
    "                selected_g.add_edge(name_1, name_2, length=1, weight=.3) # prev .8\n",
    "            elif connection[0] == 'd':\n",
    "                # pairs_with_demerit = .5 # prev .6666\n",
    "                selected_g.add_edge(name_1, name_2, length=.7, weight=.6) # prev .8\n",
    "            elif connection[0] == 'C':\n",
    "                # pairs_with_demerit = .4 # prev .5333\n",
    "                selected_g.add_edge(name_1, name_2, length=.6, weight=.7) # prev .8\n",
    "            elif connection[0] == 'D':\n",
    "                # pairs_with_demerit = .3 # prev. .4\n",
    "                selected_g.add_edge(name_1, name_2, length=.3, weight=1) # prev .8\n",
    "\n",
    "    # Try again, friend\n",
    "    if not nx.is_connected(selected_g):\n",
    "#         print(str(iteration)+': NOT CONNECTED; SKIPPING TO NEXT ITERATION')\n",
    "        continue\n",
    "\n",
    "    score = 0\n",
    "\n",
    "    # CONNECTED PAIRING BONUS ==============================================\n",
    "    # I want this to be VERY important. I feel this holds a lot of the strength of recipe,\n",
    "    # and also encompasses strength-ness and locked-ness\n",
    "    # ranges from roughly (0 to 1) * 5, tho could be a lil over or under that range\n",
    "    average_shortest_path_length = nx.average_shortest_path_length(selected_g, weight='length')\n",
    "    # average_shortest_path_score = 1 / average_shortest_path_length * 1.2 - 1.1 # normalizes full house\n",
    "    pairing_score = 1 / average_shortest_path_length * 1.5 - 1.3 # normalizes small pool (& doesn't do bad w full house)\n",
    "    score += pairing_score * 5\n",
    "    # print('AVERAGE SHORTEST PATH SCORE', average_shortest_path_score)\n",
    "\n",
    "    # Used for both strength and locked bonus:\n",
    "    node_degrees = list(selected_g.degree(weight='weight'))\n",
    "    average_degree = sum([node_degree[1] for node_degree in node_degrees]) / len(node_degrees)\n",
    "\n",
    "    # STRENGTH BONUS =======================================================\n",
    "    strength_above_average = 0\n",
    "    for node_degree in node_degrees:\n",
    "        if selected_ingredients['strong'][node_degree[0]] == 'Y':\n",
    "            strength_above_average += (node_degree[1] - average_degree)\n",
    "        elif selected_ingredients['strong'][node_degree[0]] == 'y':\n",
    "            strength_above_average += (node_degree[1] - average_degree) * .5\n",
    "    strength_score = strength_above_average * .2 + .5 # reasonable for small and full pools\n",
    "    # print('STRENGTH SCORE', strength_score)\n",
    "    score += strength_score\n",
    "\n",
    "    # LOCKED BONUS =========================================================\n",
    "    locked_above_average = 0\n",
    "    for node_degree in node_degrees:\n",
    "        if node_degree[0] in locked_names:\n",
    "            # print(node_degree[1])\n",
    "            locked_above_average += node_degree[1] - average_degree\n",
    "    locked_score = locked_above_average * .2 + .5 # close enough (has to cover few locked, lotta locked, small pool, big pool - yeesh.)\n",
    "    # print('LOCKED SCORE', locked_score)\n",
    "    score += locked_score * 2\n",
    "\n",
    "    # FLAVOR BALANCE BONUS =================================================\n",
    "    # ranges from roughly (0 to 1) * 1 (could be a lil over/under)\n",
    "    n_sweet_lower = (selected_ingredients['sweet'] == 'y').sum()\n",
    "    n_sweet_upper = (selected_ingredients['sweet'] == 'Y').sum()\n",
    "    n_salty_lower = (selected_ingredients['salty'] == 'y').sum()\n",
    "    n_salty_upper = (selected_ingredients['salty'] == 'Y').sum()\n",
    "    n_sour_lower = (selected_ingredients['sour'] == 'y').sum()\n",
    "    n_sour_upper = (selected_ingredients['sour'] == 'Y').sum()\n",
    "    n_savory_lower = (selected_ingredients['savory'] == 'y').sum()\n",
    "    n_savory_upper = (selected_ingredients['savory'] == 'Y').sum()\n",
    "    n_bitter_lower = (selected_ingredients['bitter'] == 'y').sum()\n",
    "    n_bitter_upper = (selected_ingredients['bitter'] == 'Y').sum()\n",
    "    n_spicy_lower = (selected_ingredients['spicy'] == 'y').sum()\n",
    "    n_spicy_upper = (selected_ingredients['spicy'] == 'Y').sum()\n",
    "\n",
    "    sweet_score = min(n_sweet_lower/2 + n_sweet_upper, 1)\n",
    "    salty_score = min(n_salty_lower/2 + n_salty_upper, 1)\n",
    "    sour_score = min(n_sour_lower/2 + n_sour_upper, 1)\n",
    "    savory_score = min(n_savory_lower/2 + n_savory_upper, 1)\n",
    "    bitter_score = min(n_bitter_lower/2 + n_bitter_upper, 1)\n",
    "    spicy_score = min(n_spicy_lower/2 + n_spicy_upper, 1)\n",
    "\n",
    "    flavor_score = 0\n",
    "    flavor_score += sweet_score*3 # rly want something sweet in there\n",
    "    flavor_score += salty_score*.5 # can always use salt\n",
    "    flavor_score += sour_score*2 # like me some sour\n",
    "    flavor_score += savory_score*3 # LOVE me some savory\n",
    "    flavor_score += bitter_score # idk\n",
    "    flavor_score += spicy_score*2 # can be nice\n",
    "    flavor_score = flavor_score * .2 - 1.3 # yields reasonable scores w full or small pool\n",
    "\n",
    "    score += flavor_score\n",
    "    # print('FLAVOR SCORE', flavor_score)\n",
    "\n",
    "    # FOOD GROUPS BONUS ==========================================================================================\n",
    "    if 'y' in selected_ingredients['stir_fry_protein'].values:\n",
    "        protein_score = .5\n",
    "    else:\n",
    "        protein_score = 0\n",
    "\n",
    "    if 'y' in selected_ingredients['stir_fry_fruit'].values:\n",
    "        fruit_score = .5\n",
    "    else:\n",
    "        fruit_score = 0\n",
    "\n",
    "    food_group_score = protein_score + fruit_score\n",
    "    score += food_group_score\n",
    "    # print('FOOD GROUP SCORE', food_group_score)\n",
    "\n",
    "    # print()\n",
    "    if top_score == None or score > top_score:\n",
    "        top_selected_ingredients = selected_ingredients\n",
    "        top_pairing_score = pairing_score\n",
    "        top_strength_score = strength_score\n",
    "        top_locked_score = locked_score\n",
    "        top_flavor_score = flavor_score\n",
    "        top_food_group_score = food_group_score\n",
    "        top_score = score\n",
    "\n",
    "clique_net = net.Network(notebook=True)\n",
    "\n",
    "top_selected_names = top_selected_ingredients['name'].tolist()\n",
    "nodes = top_selected_names\n",
    "\n",
    "def get_color(row):\n",
    "#     return 'grey'\n",
    "    if row['stir_fry_veg'] == 'y':\n",
    "        return 'green'\n",
    "    elif row['stir_fry_fruit'] == 'y':\n",
    "        return 'orange'\n",
    "    elif row['stir_fry_protein'] == 'y':\n",
    "        return 'brown'\n",
    "    elif row['stir_fry_grain'] == 'y':\n",
    "        return 'tan'\n",
    "    else:\n",
    "        return 'lightgrey'\n",
    "nodes_color = top_selected_ingredients.apply(get_color, axis=1).tolist()\n",
    "\n",
    "clique_net.add_nodes(\n",
    "    nodes=nodes,\n",
    "    color=nodes_color\n",
    ")\n",
    "\n",
    "for i_1, name_1 in enumerate(top_selected_names[:-1]):\n",
    "    for i_2, name_2 in enumerate(top_selected_names[i_1+1:], i_1+1):\n",
    "        connection = top_selected_ingredients[name_1][name_2]\n",
    "#         if connection[0] == 'c':\n",
    "#             clique_net.add_edge(name_1, name_2, physics=False, color='lightgrey')\n",
    "        if connection[0] == 'd':\n",
    "            clique_net.add_edge(name_1, name_2, physics=True, color='whitesmoke')\n",
    "        elif connection[0] == 'C':\n",
    "            clique_net.add_edge(name_1, name_2, physics=True, color='lightgrey')\n",
    "        elif connection[0] == 'D':\n",
    "            clique_net.add_edge(name_1, name_2, physics=True, color='black')\n",
    "\n",
    "clique_net.show('clique_net.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
