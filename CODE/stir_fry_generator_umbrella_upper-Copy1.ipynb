{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas==1.0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "import math\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = os.path.dirname(os.getcwd())\n",
    "ingredients_data_raw = pd.read_csv(os.path.join(root_path, 'DATA/ingredients_data.csv'))\n",
    "ingredients_data = ingredients_data_raw.replace(float('nan'), '')\n",
    "stir_fry_data_impractical = ingredients_data[ingredients_data['stir_fry']=='y']\n",
    "stir_fry_data_all = stir_fry_data_impractical[(stir_fry_data_impractical['stir_fry_umbrella'] != 'y') & (stir_fry_data_impractical['redirect'] != 'y')]\n",
    "stir_fry_data = stir_fry_data_all[stir_fry_data_all['stir_fry_yes'] == 'y']\n",
    "\n",
    "stir_fry_data_basic = stir_fry_data[stir_fry_data['stir_fry_basic'] == 'y']\n",
    "# stir_fry_data_current = stir_fry_data[stir_fry_data['2020_7_5'] == 'y']\n",
    "# stir_fry_data_with_umbrella = ingredients_data[(ingredients_data['stir_fry_umbrella'] != 'y') & (ingredients_data['stir_fry'] == 'y')]\n",
    "# stir_fry_data = stir_fry_data_basic\n",
    "# stir_fry_data = stir_fry_data_current\n",
    "# stir_fry_data = stir_fry_data_with_umbrella\n",
    "stir_fry_data_is_basic = False\n",
    "\n",
    "stir_fry_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = os.path.dirname(os.getcwd())\n",
    "ingredients_data_raw = pd.read_csv(os.path.join(root_path, 'DATA/ingredients_data.csv'))\n",
    "ingredients_data = ingredients_data_raw.replace(float('nan'), '')\n",
    "\n",
    "stir_fry_data = ingredients_data[(ingredients_data['stir_fry_yes'] == 'y') & (ingredients_data['stir_fry_umbrella'] != 'y') & (ingredients_data['redirect'] != 'y')]\n",
    "stir_fry_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Matching \"pairs with\" terms to ingredient names\n",
    "### (doesn't need to be run regularly) (except the first bit?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_terms_from_pairs_with(pairs_with):\n",
    "    if str(pairs_with) == 'nan':\n",
    "        return []\n",
    "    else:\n",
    "        return [term.strip() for term in pairs_with.split('\\n\\n') if term.strip() != '']\n",
    "\n",
    "# break entries in column that has 'pairs with' strings into lists of ingredient terms\n",
    "ingredient_pairs_with_terms = stir_fry_data['pairs_with'].apply(get_terms_from_pairs_with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create list of all terms, ignoring case and excluding duplicates\n",
    "# all_terms = list(set(ingredient_pairs_with_terms.sum()))\n",
    "# all_terms_lower = list(set([term.lower() for term in ingredient_pairs_with_terms.sum()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !pip install inflect\n",
    "# import inflect\n",
    "# p = inflect.engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# salad_matches = pd.read_csv(os.path.join(root_path, 'DATA/term_name_matches_specific.csv'))\n",
    "    \n",
    "# def get_tokens(phrase):\n",
    "#     tokens = [token.strip() for token in re.split('\\(|\\)|,|e\\.g\\.|esp\\.|and|—|or|aka|see|see also|;|and\\/or|\\*', phrase)]\n",
    "# #     print(tokens)\n",
    "#     tokens = [p.singular_noun(token) or token for token in tokens if token != '']\n",
    "# #     print(tokens)\n",
    "#     return tokens\n",
    "\n",
    "# def get_mark(name, term):\n",
    "# #     print()\n",
    "# #     print('NAME', name)\n",
    "# #     print('TERM', term)\n",
    "#     try:\n",
    "#         salad_match = salad_matches[term][salad_matches['name'] == name].iloc[0]\n",
    "# #         print('SALAD MATCH', salad_match)\n",
    "#     except:\n",
    "# #         print('NO SALAD MATCH')\n",
    "#         salad_match = None \n",
    "#     if salad_match:\n",
    "#         if str(salad_match) in ['0', 'nan']:\n",
    "# #             print('BAD SALAD MATCH')\n",
    "#             return ''\n",
    "#         else:\n",
    "# #             print('GOOD SALAD MATCH', salad_match)\n",
    "# #             print('NAME', name)\n",
    "# #             print('TERM', term)\n",
    "# #             print()\n",
    "#             return salad_match\n",
    "    \n",
    "#     name_tokens = [token.lower() for token in get_tokens(name)]  \n",
    "#     term_tokens_mixed = get_tokens(term)\n",
    "#     term_tokens = [token.lower() for token in term_tokens_mixed]\n",
    "    \n",
    "#     primary_name = re.split('\\(|—|e\\.g\\.', name)[0].strip()\n",
    "#     primary_name_split = primary_name.split(', ')\n",
    "#     single_comma_primary_name = len(primary_name_split) == 2\n",
    "    \n",
    "# #     print()\n",
    "#     if 'e.g.' in term:\n",
    "#         if single_comma_primary_name:\n",
    "#             if name_tokens[0] == term_tokens[0]:\n",
    "#                 if name_tokens[1] in term_tokens: # specific name in e.g. term\n",
    "# #                     print('NAME TOKENS In TERM TOKENS')\n",
    "#                     term_i = term_tokens.index(name_tokens[1])\n",
    "#                     if term_tokens_mixed[term_i] == term_tokens_mixed[term_i].upper():\n",
    "#                         match = 'D'\n",
    "#                     else:\n",
    "#                         if term_tokens_mixed[0] == term_tokens_mixed[0].upper():\n",
    "#                             match = 'D'\n",
    "#                         else:\n",
    "#                             match = 'd'\n",
    "#                 else: # name matches only generic (pre e.g.) part of term\n",
    "#                     if term_tokens_mixed[0] == term_tokens_mixed[0].upper():\n",
    "#                         match = 'C'\n",
    "#                     else:\n",
    "#                         match = 'c'\n",
    "#             else:\n",
    "#                 match = ''\n",
    "#         else:\n",
    "#             if name_tokens[0] == term_tokens[0]: # name matches term before e.g.\n",
    "#                 if term_tokens_mixed[0] == term_tokens_mixed[0].upper():\n",
    "#                     match = 'C'\n",
    "#                 else:\n",
    "#                     match = 'c'\n",
    "#             elif name_tokens[0] in term_tokens[1:]: # name matches term after e.g.\n",
    "#                 term_i = term_tokens.index(name_tokens[0])\n",
    "#                 if term_tokens_mixed[term_i] == term_tokens_mixed[term_i].upper():\n",
    "#                     match = 'D'\n",
    "#                 else:\n",
    "#                     if term_tokens_mixed[0] == term_tokens_mixed[0].upper():\n",
    "#                         match = 'D'\n",
    "#                     else:\n",
    "#                         match = 'd'\n",
    "#             else:\n",
    "#                 match = ''\n",
    "#     else:\n",
    "#         if single_comma_primary_name:\n",
    "#             if ' '.join(primary_name_split).lower() in term.lower() or primary_name.lower() in term.lower(): # if primary_name is in term, any order\n",
    "#                 if term_tokens_mixed[0] == term_tokens_mixed[0].upper():\n",
    "#                     match = 'D'\n",
    "#                 else:\n",
    "#                     match = 'd'\n",
    "#             elif name_tokens[0] == term_tokens[0]: # if first part of primary name matches first token in term\n",
    "#                 if term_tokens_mixed[0] == term_tokens_mixed[0].upper():\n",
    "#                     match = 'C'\n",
    "#                 else:\n",
    "#                     match = 'c'\n",
    "#             else:\n",
    "#                 match = ''\n",
    "#         else:\n",
    "#             if name_tokens[0] in term_tokens: # if non-comma name anywhere in non-e.g. term_tokens\n",
    "#                 if term_tokens_mixed[0] == term_tokens_mixed[0].upper():\n",
    "#                     match = 'D'\n",
    "#                 else:\n",
    "#                     match = 'd'\n",
    "#             elif len(set(name_tokens).intersection(set(term_tokens))) > 0: # if there are any common tokens\n",
    "#                 if term_tokens_mixed[0] == term_tokens_mixed[0].upper():\n",
    "#                     match = 'C'\n",
    "#                 else:\n",
    "#                     match = 'c'\n",
    "#             else:\n",
    "#                 match = ''\n",
    "                \n",
    "#     if match == '':\n",
    "#         n_common = len(set(name_tokens).intersection(set(term_tokens)))\n",
    "#         if n_common != 0:\n",
    "#             match = n_common\n",
    "#     return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # doing it this way so I can add 'print' to monitor progress\n",
    "# mark_data = []\n",
    "# for name in stir_fry_data['name']:\n",
    "#     print(name)\n",
    "#     mark_data.append([get_mark(name, term) for term in all_terms])\n",
    "\n",
    "# term_name_marks = pd.DataFrame(mark_data, columns = all_terms)\n",
    "# term_name_marks['name'] = pd.Series(stir_fry_data['name'].values.tolist())\n",
    "\n",
    "# term_name_marks.to_csv(os.path.join(root_path, 'DATA/stir_fry_term_name_marks.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Extracting \"pairs with\" data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# term_name_matches_raw = pd.read_csv(os.path.join(root_path, 'DATA/stir_fry_term_name_matches_plus.csv'))\n",
    "# term_name_matches = term_name_matches_raw.replace(['0', '1', '2', '3', '4', '5', 0, 1, 2, 3, 4, 5, float('nan')], '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Native and North) American cuisines\n",
      "BAKED GOODS, e.g., breads, cakes, cookies, pastries, pies, scones\n",
      "butter, e.g., brown\n",
      "cereals, breakfast, e.g., granola\n",
      "cheese, e.g., blue, goat, ricotta\n",
      "cherries, esp. dried\n",
      "ice cream, e.g., butter pecan\n",
      "PIES, e.g., pecan, sweet potato\n",
      "purees, vegetable\n",
      "waffles\n",
      "wheat germ\n",
      "bibimbap\n",
      "cereals, hot breakfast, e.g., with fruit and nuts\n",
      "“meatballs” (e.g., brown rice + onion + parsley + walnuts)\n",
      "soups, e.g., tomato\n",
      "sprouts, e.g., bean, pea\n",
      "sweeteners, e.g., mirin, brown sugar\n",
      "milk, e.g., nondairy (almond, coconut, rice)\n",
      "cheese, e.g., blue, mozzarella\n",
      "citrus, e.g., grapefruit, lemon, lime, orange, tangerine; juice, zest\n",
      "eggs, e.g., frittatas, hard-boiled, tortillas\n",
      "OILS, e.g., nut, olive, walnut\n",
      "salad dressings, e.g., sherry vinaigrette\n",
      "SALADS, e.g., fruit, vegetable\n",
      "sauces, e.g., butter\n",
      "SPANISH CUISINE\n",
      "vinegar, other, e.g., balsamic, red wine, white wine\n",
      "CPU times: user 27.2 s, sys: 7.79 ms, total: 27.2 s\n",
      "Wall time: 27.2 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "# # CREATE PAIRING DATA MATRIX (names x names)\n",
    "# # takes a few minutes\n",
    "\n",
    "# pairing_data = pd.DataFrame({\n",
    "#     'name': stir_fry_data['name'],\n",
    "#     'pairs_with_terms': ingredient_pairs_with_terms\n",
    "# })\n",
    "\n",
    "# for name in stir_fry_data['name']:\n",
    "#     pairing_data[name] = pd.Series(['']*len(stir_fry_data['name']))\n",
    "\n",
    "# def get_pairs_with_names(row):\n",
    "#     lower_category_names = []\n",
    "#     lower_direct_names = []\n",
    "#     upper_category_names = []\n",
    "#     upper_direct_names = []\n",
    "#     for term in row['pairs_with_terms']:\n",
    "#         if term in term_name_matches.columns.values.tolist():\n",
    "#             lower_category_names += term_name_matches[term_name_matches[term] == 'c']['name'].values.tolist()\n",
    "#             lower_direct_names += term_name_matches[term_name_matches[term] == 'd']['name'].values.tolist()\n",
    "#             upper_category_names += term_name_matches[term_name_matches[term] == 'C']['name'].values.tolist()\n",
    "#             upper_direct_names += term_name_matches[term_name_matches[term] == 'D']['name'].values.tolist()\n",
    "#         else:\n",
    "#             pass\n",
    "#             print(term)\n",
    "    \n",
    "#     for lower_category_name in lower_category_names:\n",
    "#         row[lower_category_name] = 'c'\n",
    "#     for lower_direct_name in lower_direct_names:\n",
    "#         row[lower_direct_name] = 'd'\n",
    "#     for upper_category_name in upper_category_names:\n",
    "#         row[upper_category_name] = 'C'\n",
    "#     for upper_direct_name in upper_direct_names:\n",
    "#         row[upper_direct_name] = 'D'\n",
    "\n",
    "#     return row\n",
    "\n",
    "# pairing_data = pairing_data.apply(get_pairs_with_names, axis=1)\n",
    "# pairing_data.replace(float('nan'), '', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.25 s, sys: 152 ms, total: 6.41 s\n",
      "Wall time: 6.27 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "# # SYNC PAIRING DATA MATRIX (make sure [a][b] agrees with [b][a])\n",
    "\n",
    "# for index_1, name_1 in enumerate(pairing_data['name'].values.tolist()):\n",
    "#     for index_2, name_2 in enumerate(pairing_data['name'].values.tolist()):\n",
    "#         value_1 = pairing_data[name_1][index_2]\n",
    "#         value_2 = pairing_data[name_2][index_1]\n",
    "        \n",
    "#         if name_1 == name_2:\n",
    "#             proper_value = ''\n",
    "#         elif value_1 == 'D' or value_2 == 'D':\n",
    "#             proper_value = 'D'\n",
    "#         elif value_1 == 'C' or value_2 == 'C':\n",
    "#             proper_value = 'C'\n",
    "#         elif value_1 == 'd' or value_2 == 'd':\n",
    "#             proper_value = 'd'\n",
    "#         elif value_1 == 'c' or value_2 == 'c':\n",
    "#             proper_value = 'c'\n",
    "#         else:\n",
    "#             proper_value = ''\n",
    "        \n",
    "#         pairing_data[name_1][index_2] = proper_value\n",
    "\n",
    "        \n",
    "#         pairing_data[name_2][index_1] = proper_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.6 s, sys: 7.94 ms, total: 4.6 s\n",
      "Wall time: 4.61 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "# # REPRESENT PAIRING DATA AS LISTS\n",
    "\n",
    "# def get_pairs_with_names(row):\n",
    "#     lower_category_name = pairing_data[pairing_data[row['name']] == 'c']['name'].values.tolist()\n",
    "#     lower_direct_name = pairing_data[pairing_data[row['name']] == 'd']['name'].values.tolist()\n",
    "#     upper_category_name = pairing_data[pairing_data[row['name']] == 'C']['name'].values.tolist()\n",
    "#     upper_direct_name = pairing_data[pairing_data[row['name']] == 'D']['name'].values.tolist()\n",
    "    \n",
    "#     row['lower_category_names'] = pairing_data[pairing_data[row['name']] == 'c']['name'].values.tolist()\n",
    "#     row['lower_direct_names'] = pairing_data[pairing_data[row['name']] == 'd']['name'].values.tolist()\n",
    "#     row['upper_category_names'] = pairing_data[pairing_data[row['name']] == 'C']['name'].values.tolist()\n",
    "#     row['upper_direct_names'] = pairing_data[pairing_data[row['name']] == 'D']['name'].values.tolist()\n",
    "#     row['lower_names'] = row['lower_category_names'] + row['lower_direct_names']\n",
    "#     row['upper_names'] = row['upper_category_names'] + row['upper_direct_names']\n",
    "#     row['all_names'] = row['lower_names'] + row['upper_names']\n",
    "    \n",
    "# #     row['lower_category_pairs_with_names'] = list(set([lower_category_name for lower_category_name in row['lower_category_names'] if lower_category_name != row['name']]))\n",
    "# #     row['lower_direct_pairs_with_names'] = list(set([lower_direct_name for lower_direct_name in row['lower_direct_names'] if lower_direct_name != row['name']]))\n",
    "# #     row['upper_category_pairs_with_names'] = list(set([upper_category_name for upper_category_name in row['upper_category_names'] if upper_category_name != row['name']]))\n",
    "# #     row['upper_direct_pairs_with_names'] = list(set([upper_direct_name for upper_direct_name in row['upper_direct_names'] if upper_direct_name != row['name']]))\n",
    "# #     row['lower_pairs_with_names'] = list(set(row['lower_category_pairs_with_names'] + row['lower_direct_pairs_with_names']))\n",
    "# #     row['upper_pairs_with_names'] = list(set(row['upper_category_pairs_with_names'] + row['upper_direct_pairs_with_names']))\n",
    "# #     row['all_pairs_with_names'] = list(set(row['lower_pairs_with_names'] + row['upper_pairs_with_names']))\n",
    "    \n",
    "#     row['lc_sorted_pairs'] = [tuple(sorted((row['name'], other_name,))) for other_name in row['lower_category_names']]\n",
    "#     row['ld_sorted_pairs'] = [tuple(sorted((row['name'], other_name,))) for other_name in row['lower_direct_names']]\n",
    "#     row['uc_sorted_pairs'] = [tuple(sorted((row['name'], other_name,))) for other_name in row['upper_category_names']]\n",
    "#     row['ud_sorted_pairs'] = [tuple(sorted((row['name'], other_name,))) for other_name in row['upper_direct_names']]\n",
    "#     row['l_sorted_pairs'] = [tuple(sorted((row['name'], other_name,))) for other_name in row['lower_names']]\n",
    "#     row['u_sorted_pairs'] = [tuple(sorted((row['name'], other_name,))) for other_name in row['upper_names']]\n",
    "#     row['a_sorted_pairs'] = [tuple(sorted((row['name'], other_name,))) for other_name in row['all_names']]\n",
    "    \n",
    "#     return row\n",
    "\n",
    "# pairing_data = pairing_data.apply(get_pairs_with_names, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small enough to git commit\n",
    "# pairing_data.to_pickle(os.path.join(root_path, 'DATA/stir_fry_pairing_data.pickle'))\n",
    "pairing_data = pd.read_pickle(os.path.join(root_path, 'DATA/stir_fry_pairing_data.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Creating \"clashes with\" data \\[gonna start out without this one, for stir fry\\]\n",
    "### (but still run through, to add it to data just in case?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_name_clashes_blank = stir_fry_data[['name', 'protein_cheese_sub', 'stir_fry_allium', 'fruit', 'veg']].copy()\n",
    "# names = stir_fry_data['name'].values.tolist()\n",
    "\n",
    "# for i, col_name in enumerate(names):\n",
    "#     name_name_clashes_blank[col_name] = pd.Series(['x']*(i+1) + ['']*(len(names)-(i+1)))\n",
    "\n",
    "# name_name_clashes_blank.to_csv(os.path.join(root_path, 'DATA/name_name_clashes_blank.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_name_clashes_input = pd.read_csv(os.path.join(root_path, 'DATA/name_name_clashes_input.csv'))\n",
    "# name_name_clashes_input = name_name_clashes_input.replace([float('nan'), 'x'], '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/russell/flavor_tool/venv/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# columns = ['name'] + name_name_clashes_input['name'].values.tolist()\n",
    "# name_name_clashes_input = name_name_clashes_input[columns]\n",
    "\n",
    "# def get_clashes_with_data(row):\n",
    "#     data = pd.Series([])\n",
    "#     name = row['name']\n",
    "#     data['name'] = name\n",
    "    \n",
    "#     lower_names = name_name_clashes_input['name'][name_name_clashes_input[name] == 'y'].values.tolist()\n",
    "#     upper_names = name_name_clashes_input['name'][name_name_clashes_input[name] == 'Y'].values.tolist()\n",
    "#     if 'y' in lower_names:\n",
    "#         print(lower_names)\n",
    "    \n",
    "#     for name in name_name_clashes_input['name']:\n",
    "#         if row[name] == 'y':\n",
    "#             lower_names.append(name)\n",
    "#         elif row[name] == 'Y':\n",
    "#             upper_names.append(name)\n",
    "    \n",
    "#     lower_names = list(set(lower_names))\n",
    "#     upper_names = list(set(upper_names))\n",
    "    \n",
    "#     data['lower_clashes_with_names'] = lower_names\n",
    "#     data['upper_clashes_with_names'] = upper_names\n",
    "#     data['all_clashes_with_names'] = list(set(lower_names + upper_names)) # shouldn't be overlap here, but hey\n",
    "    \n",
    "#     data['lower_clashes_with_pairs'] = [tuple(sorted([name, lower_name])) for lower_name in data['lower_clashes_with_names']]\n",
    "#     data['upper_clashes_with_pairs'] = [tuple(sorted([name, lower_name])) for lower_name in data['upper_clashes_with_names']]\n",
    "#     data['all_clashes_with_pairs'] = [tuple(sorted([name, lower_name])) for lower_name in data['all_clashes_with_names']]\n",
    "    \n",
    "#     return data\n",
    "\n",
    "# clashes_with_data = name_name_clashes_input.apply(get_clashes_with_data, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SYNC PAIRING DATA MATRIX (make sure [a][b] agrees with [b][a])\n",
    "\n",
    "# name_name_clashes_synced = name_name_clashes_input.copy()\n",
    "\n",
    "# for index_1, name_1 in enumerate(name_name_clashes_input['name'].values.tolist()):\n",
    "#     for index_2, name_2 in enumerate(name_name_clashes_input['name'].values.tolist()):\n",
    "#         value_1 = name_name_clashes_input[name_1][index_2]\n",
    "#         value_2 = name_name_clashes_input[name_2][index_1]\n",
    "        \n",
    "#         if name_1 == name_2:\n",
    "#             proper_value = ''\n",
    "#         elif value_1 == 'Y' or value_2 == 'Y':\n",
    "#             proper_value = 'Y'\n",
    "#         elif value_1 == 'y' or value_2 == 'y':\n",
    "#             proper_value = 'y'\n",
    "#         else:\n",
    "#             proper_value = ''\n",
    "        \n",
    "#         name_name_clashes_synced[name_1][index_2] = proper_value\n",
    "#         name_name_clashes_synced[name_2][index_1] = proper_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add name_name_clashes_synced to clashes_with_data\n",
    "# clashes_with_data[name_name_clashes_synced['name'].tolist()] = name_name_clashes_synced[name_name_clashes_synced['name'].tolist()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clashes_with_data.to_pickle(os.path.join(root_path, 'DATA/stir_fry_clashes_with_data.pickle'))\n",
    "clashes_with_data = pd.read_pickle(os.path.join(root_path, 'DATA/stir_fry_clashes_with_data.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Adding umbrella data\n",
    "- Because if 'onion' umbrella pairs with eg. olive oil, all onions should be paired with olive oil as at least c/C.\n",
    "- So what I'm doing here is\n",
    "    - matching umbrella-names' terms to regular ol names\n",
    "    - then marking all ingredients that fall under a specific umbrella\n",
    "    - then for each reg-term for each umbrella-name, I'll add a c/C connection value if existing connection is lower\n",
    "- and let the records show I'm deciding that\n",
    "    - if onions D pair with olive oil, but white onions c pair with olive oil, then imma overwrite c with D\n",
    "    - and if onions c pair, white onions D pair I'd write D for white onions\n",
    "    - (so I'm going with the bigger mark, just like when syncing)\n",
    "### (also, the first part of this doesn't need to be run regularly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# umbrella_data = ingredients_data[(ingredients_data['stir_fry_yes'] == 'y') & (ingredients_data['stir_fry_useful_umbrella'] == 'y') & (ingredients_data['redirect'] != 'y')]\n",
    "# umbrella_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# stir_fry_data_with_umbrella = ingredients_data[(ingredients_data['stir_fry_yes'] == 'y') & (ingredients_data['redirect'] != 'y')]\n",
    "# stir_fry_data_with_umbrella.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # break entries in column that has 'pairs with' strings into lists of ingredient terms\n",
    "# umbrella_ingredient_pairs_with_terms = umbrella_data['pairs_with'].apply(get_terms_from_pairs_with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create list of all terms, ignoring case and excluding duplicates\n",
    "# umbrella_all_terms = list(set(umbrella_ingredient_pairs_with_terms.sum()))\n",
    "# umbrella_all_terms_lower = list(set([term.lower() for term in umbrella_ingredient_pairs_with_terms.sum()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !pip install inflect\n",
    "# import inflect\n",
    "# p = inflect.engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salad_matches = pd.read_csv(os.path.join(root_path, 'DATA/term_name_matches_specific.csv'))\n",
    "# stir_fry_matches = pd.read_csv(os.path.join(root_path, 'DATA/stir_fry_term_name_matches_plus.csv'))\n",
    "    \n",
    "# def get_tokens(phrase):\n",
    "#     tokens = [token.strip() for token in re.split('\\(|\\)|,|e\\.g\\.|esp\\.|and|—|or|aka|see|see also|;|and\\/or|\\*', phrase)]\n",
    "#     tokens = [p.singular_noun(token) or token for token in tokens if token != '']\n",
    "#     return tokens\n",
    "\n",
    "# def get_mark(name, term):\n",
    "#     try:\n",
    "#         stir_fry_match = stir_fry_matches[term][stir_fry_matches['name'] == name].iloc[0]\n",
    "#     except:\n",
    "#         stir_fry_match = None \n",
    "#     if stir_fry_match:\n",
    "#         if str(stir_fry_match) in ['0', 'nan']:\n",
    "#             return ''\n",
    "#         else:\n",
    "#             print('FOUND stir fry match')\n",
    "#             return stir_fry_match\n",
    "    \n",
    "#     try:\n",
    "#         salad_match = salad_matches[term][salad_matches['name'] == name].iloc[0]\n",
    "#     except:\n",
    "#         salad_match = None \n",
    "#     if salad_match:\n",
    "#         if str(salad_match) in ['0', 'nan']:\n",
    "#             return ''\n",
    "#         else:\n",
    "#             print('FOUND salad match')\n",
    "#             return salad_match\n",
    "    \n",
    "#     name_tokens = [token.lower() for token in get_tokens(name)]  \n",
    "#     term_tokens_mixed = get_tokens(term)\n",
    "#     term_tokens = [token.lower() for token in term_tokens_mixed]\n",
    "    \n",
    "#     primary_name = re.split('\\(|—|e\\.g\\.', name)[0].strip()\n",
    "#     primary_name_split = primary_name.split(', ')\n",
    "#     single_comma_primary_name = len(primary_name_split) == 2\n",
    "    \n",
    "# #     print()\n",
    "#     if 'e.g.' in term:\n",
    "#         if single_comma_primary_name:\n",
    "#             if name_tokens[0] == term_tokens[0]:\n",
    "#                 if name_tokens[1] in term_tokens: # specific name in e.g. term\n",
    "# #                     print('NAME TOKENS In TERM TOKENS')\n",
    "#                     term_i = term_tokens.index(name_tokens[1])\n",
    "#                     if term_tokens_mixed[term_i] == term_tokens_mixed[term_i].upper():\n",
    "#                         match = 'D'\n",
    "#                     else:\n",
    "#                         if term_tokens_mixed[0] == term_tokens_mixed[0].upper():\n",
    "#                             match = 'D'\n",
    "#                         else:\n",
    "#                             match = 'd'\n",
    "#                 else: # name matches only generic (pre e.g.) part of term\n",
    "#                     if term_tokens_mixed[0] == term_tokens_mixed[0].upper():\n",
    "#                         match = 'C'\n",
    "#                     else:\n",
    "#                         match = 'c'\n",
    "#             else:\n",
    "#                 match = ''\n",
    "#         else:\n",
    "#             if name_tokens[0] == term_tokens[0]: # name matches term before e.g.\n",
    "#                 if term_tokens_mixed[0] == term_tokens_mixed[0].upper():\n",
    "#                     match = 'C'\n",
    "#                 else:\n",
    "#                     match = 'c'\n",
    "#             elif name_tokens[0] in term_tokens[1:]: # name matches term after e.g.\n",
    "#                 term_i = term_tokens.index(name_tokens[0])\n",
    "#                 if term_tokens_mixed[term_i] == term_tokens_mixed[term_i].upper():\n",
    "#                     match = 'D'\n",
    "#                 else:\n",
    "#                     if term_tokens_mixed[0] == term_tokens_mixed[0].upper():\n",
    "#                         match = 'D'\n",
    "#                     else:\n",
    "#                         match = 'd'\n",
    "#             else:\n",
    "#                 match = ''\n",
    "#     else:\n",
    "#         if single_comma_primary_name:\n",
    "#             if ' '.join(primary_name_split).lower() in term.lower() or primary_name.lower() in term.lower(): # if primary_name is in term, any order\n",
    "#                 if term_tokens_mixed[0] == term_tokens_mixed[0].upper():\n",
    "#                     match = 'D'\n",
    "#                 else:\n",
    "#                     match = 'd'\n",
    "#             elif name_tokens[0] == term_tokens[0]: # if first part of primary name matches first token in term\n",
    "#                 if term_tokens_mixed[0] == term_tokens_mixed[0].upper():\n",
    "#                     match = 'C'\n",
    "#                 else:\n",
    "#                     match = 'c'\n",
    "#             else:\n",
    "#                 match = ''\n",
    "#         else:\n",
    "#             if name_tokens[0] in term_tokens: # if non-comma name anywhere in non-e.g. term_tokens\n",
    "#                 if term_tokens_mixed[0] == term_tokens_mixed[0].upper():\n",
    "#                     match = 'D'\n",
    "#                 else:\n",
    "#                     match = 'd'\n",
    "#             elif len(set(name_tokens).intersection(set(term_tokens))) > 0: # if there are any common tokens\n",
    "#                 if term_tokens_mixed[0] == term_tokens_mixed[0].upper():\n",
    "#                     match = 'C'\n",
    "#                 else:\n",
    "#                     match = 'c'\n",
    "#             else:\n",
    "#                 match = ''\n",
    "                \n",
    "#     if match == '':\n",
    "#         n_common = len(set(name_tokens).intersection(set(term_tokens)))\n",
    "#         if n_common != 0:\n",
    "#             match = n_common\n",
    "#     return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# umbrella_mark_data = []\n",
    "# for name in stir_fry_data['name']:\n",
    "#     print(name)\n",
    "#     umbrella_mark_data.append([get_mark(name, term) for term in umbrella_all_terms])\n",
    "\n",
    "# umbrella_term_name_marks = pd.DataFrame(umbrella_mark_data, columns = umbrella_all_terms)\n",
    "# umbrella_term_name_marks['name'] = pd.Series(stir_fry_data['name'].values.tolist())\n",
    "\n",
    "# umbrella_term_name_marks.to_csv(os.path.join(root_path, 'DATA/stir_fry_useful_umbrella_term_name_marks.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# umbrella_term_name_matches_raw = pd.read_csv(os.path.join(root_path, 'DATA/stir_fry_umbrella_term_name_matches.csv'))\n",
    "# umbrella_term_name_matches = umbrella_term_name_matches_raw.replace(['0', '1', '2', '3', '4', '5', 0, 1, 2, 3, 4, 5, float('nan')], '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.69 s, sys: 0 ns, total: 2.69 s\n",
      "Wall time: 2.68 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# # CREATE PAIRING DATA MATRIX (names x names)\n",
    "\n",
    "# umbrella_pairing_data = pd.DataFrame({\n",
    "#     'name': umbrella_data['name'],\n",
    "#     'pairs_with_terms': umbrella_ingredient_pairs_with_terms\n",
    "# })\n",
    "\n",
    "# for name in stir_fry_data['name']:\n",
    "#     umbrella_pairing_data[name] = pd.Series(['']*len(umbrella_data))\n",
    "\n",
    "# def get_pairs_with_names(row):\n",
    "#     lower_category_names = []\n",
    "#     lower_direct_names = []\n",
    "#     upper_category_names = []\n",
    "#     upper_direct_names = []\n",
    "#     for term in row['pairs_with_terms']:\n",
    "#         if term in umbrella_term_name_matches.columns.values.tolist():\n",
    "#             lower_category_names += umbrella_term_name_matches[umbrella_term_name_matches[term] == 'c']['name'].values.tolist()\n",
    "#             lower_direct_names += umbrella_term_name_matches[umbrella_term_name_matches[term] == 'd']['name'].values.tolist()\n",
    "#             upper_category_names += umbrella_term_name_matches[umbrella_term_name_matches[term] == 'C']['name'].values.tolist()\n",
    "#             upper_direct_names += umbrella_term_name_matches[umbrella_term_name_matches[term] == 'D']['name'].values.tolist()\n",
    "#         else:\n",
    "#             pass\n",
    "#             print(term)\n",
    "    \n",
    "#     for lower_category_name in lower_category_names:\n",
    "#         row[lower_category_name] = 'c'\n",
    "#     for lower_direct_name in lower_direct_names:\n",
    "#         row[lower_direct_name] = 'd'\n",
    "#     for upper_category_name in upper_category_names:\n",
    "#         row[upper_category_name] = 'C'\n",
    "#     for upper_direct_name in upper_direct_names:\n",
    "#         row[upper_direct_name] = 'D'\n",
    "\n",
    "#     return row\n",
    "\n",
    "# umbrella_pairing_data = umbrella_pairing_data.apply(get_pairs_with_names, axis=1)\n",
    "# umbrella_pairing_data.replace(float('nan'), '', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating template to match ingredients to their umbrellas\n",
    "# umbrella_covered_blanks = pd.DataFrame({\n",
    "#     umbrella_name: ['']*len(stir_fry_data)\n",
    "# for umbrella_name in umbrella_data['name']})\n",
    "# umbrella_covered_blanks['name'] = stir_fry_data['name']\n",
    "# umbrella_covered_blanks.to_csv(os.path.join(root_path, 'DATA/stir_fry_umbrella_covered_blanks.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# umbrella_covered_matches_raw = pd.read_csv(os.path.join(root_path, 'DATA/stir_fry_umbrella_covered_matches.csv'))\n",
    "# umbrella_covered_matches = umbrella_covered_matches_raw.replace([float('nan')], '')\n",
    "# def get_covered_list(umbrella_name):\n",
    "#     return umbrella_covered_matches[umbrella_covered_matches[umbrella_name] == 'y']['name'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.8 s, sys: 44 ms, total: 38.9 s\n",
      "Wall time: 38.9 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# # 40s\n",
    "\n",
    "# pairing_data_with_umbrella_matches = pairing_data.copy()\n",
    "\n",
    "# for umbrella_name in umbrella_data['name']: # eg. onions\n",
    "#     for covered_name in get_covered_list(umbrella_name): # eg. white onion\n",
    "# #         print(umbrella_name, covered_name)\n",
    "#         for stir_fry_name in stir_fry_data['name']: # eg. olive oil\n",
    "#             umbrella_match_value = umbrella_pairing_data[umbrella_pairing_data['name'] == umbrella_name][stir_fry_name].iloc[0] # eg. D (because onions match with OLIVE OIL)\n",
    "# #             print(umbrella_match_value, umbrella_name, stir_fry_name)\n",
    "#             stir_fry_match_value = pairing_data[pairing_data['name'] == covered_name][stir_fry_name].iloc[0] # eg. 'c' (because white onions match with oil)\n",
    "# #             print(umbrella_match_value, stir_fry_match_value)\n",
    "        \n",
    "#             if umbrella_match_value == 'D' or stir_fry_match_value == 'D':\n",
    "#                 proper_match_value = 'D'\n",
    "#             elif umbrella_match_value == 'C' or stir_fry_match_value == 'C':\n",
    "#                 proper_match_value = 'C'\n",
    "#             elif umbrella_match_value == 'd' or stir_fry_match_value == 'd':\n",
    "#                 proper_match_value = 'd'\n",
    "#             elif umbrella_match_value == 'c' or stir_fry_match_value == 'c':\n",
    "#                 proper_match_value = 'c'\n",
    "#             else:\n",
    "#                 proper_match_value = ''\n",
    "                \n",
    "#             pairing_data_with_umbrella_matches.loc[pairing_data_with_umbrella_matches['name'] == covered_name, stir_fry_name] = proper_match_value\n",
    "#             pairing_data_with_umbrella_matches.loc[pairing_data_with_umbrella_matches['name'] == stir_fry_name, covered_name] = proper_match_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.07 s, sys: 0 ns, total: 2.07 s\n",
      "Wall time: 2.07 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "# # REPRESENT PAIRING DATA AS LISTS\n",
    "\n",
    "# def get_pairs_with_names(row):\n",
    "#     lower_category_name = pairing_data_with_umbrella_matches[pairing_data_with_umbrella_matches[row['name']] == 'c']['name'].values.tolist()\n",
    "#     lower_direct_name = pairing_data_with_umbrella_matches[pairing_data_with_umbrella_matches[row['name']] == 'd']['name'].values.tolist()\n",
    "#     upper_category_name = pairing_data_with_umbrella_matches[pairing_data_with_umbrella_matches[row['name']] == 'C']['name'].values.tolist()\n",
    "#     upper_direct_name = pairing_data_with_umbrella_matches[pairing_data_with_umbrella_matches[row['name']] == 'D']['name'].values.tolist()\n",
    "    \n",
    "#     row['lower_category_names'] = pairing_data_with_umbrella_matches[pairing_data_with_umbrella_matches[row['name']] == 'c']['name'].values.tolist()\n",
    "#     row['lower_direct_names'] = pairing_data_with_umbrella_matches[pairing_data_with_umbrella_matches[row['name']] == 'd']['name'].values.tolist()\n",
    "#     row['upper_category_names'] = pairing_data_with_umbrella_matches[pairing_data_with_umbrella_matches[row['name']] == 'C']['name'].values.tolist()\n",
    "#     row['upper_direct_names'] = pairing_data_with_umbrella_matches[pairing_data_with_umbrella_matches[row['name']] == 'D']['name'].values.tolist()\n",
    "#     row['lower_names'] = row['lower_category_names'] + row['lower_direct_names']\n",
    "#     row['upper_names'] = row['upper_category_names'] + row['upper_direct_names']\n",
    "#     row['all_names'] = row['lower_names'] + row['upper_names']\n",
    "    \n",
    "# #     row['lower_category_pairs_with_names'] = list(set([lower_category_name for lower_category_name in row['lower_category_names'] if lower_category_name != row['name']]))\n",
    "# #     row['lower_direct_pairs_with_names'] = list(set([lower_direct_name for lower_direct_name in row['lower_direct_names'] if lower_direct_name != row['name']]))\n",
    "# #     row['upper_category_pairs_with_names'] = list(set([upper_category_name for upper_category_name in row['upper_category_names'] if upper_category_name != row['name']]))\n",
    "# #     row['upper_direct_pairs_with_names'] = list(set([upper_direct_name for upper_direct_name in row['upper_direct_names'] if upper_direct_name != row['name']]))\n",
    "# #     row['lower_pairs_with_names'] = list(set(row['lower_category_pairs_with_names'] + row['lower_direct_pairs_with_names']))\n",
    "# #     row['upper_pairs_with_names'] = list(set(row['upper_category_pairs_with_names'] + row['upper_direct_pairs_with_names']))\n",
    "# #     row['all_pairs_with_names'] = list(set(row['lower_pairs_with_names'] + row['upper_pairs_with_names']))\n",
    "    \n",
    "#     row['lc_sorted_pairs'] = [tuple(sorted((row['name'], other_name,))) for other_name in row['lower_category_names']]\n",
    "#     row['ld_sorted_pairs'] = [tuple(sorted((row['name'], other_name,))) for other_name in row['lower_direct_names']]\n",
    "#     row['uc_sorted_pairs'] = [tuple(sorted((row['name'], other_name,))) for other_name in row['upper_category_names']]\n",
    "#     row['ud_sorted_pairs'] = [tuple(sorted((row['name'], other_name,))) for other_name in row['upper_direct_names']]\n",
    "#     row['l_sorted_pairs'] = [tuple(sorted((row['name'], other_name,))) for other_name in row['lower_names']]\n",
    "#     row['u_sorted_pairs'] = [tuple(sorted((row['name'], other_name,))) for other_name in row['upper_names']]\n",
    "#     row['a_sorted_pairs'] = [tuple(sorted((row['name'], other_name,))) for other_name in row['all_names']]\n",
    "    \n",
    "#     return row\n",
    "\n",
    "# pairing_data_with_umbrella_matches = pairing_data_with_umbrella_matches.apply(get_pairs_with_names, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairing_data_with_umbrella_matches.to_pickle(os.path.join(root_path, 'DATA/stir_fry_pairing_data_with_umbrella_matches.pickle'))\n",
    "pairing_data_with_umbrella_matches = pd.read_pickle(os.path.join(root_path, 'DATA/stir_fry_pairing_data_with_umbrella_matches.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Creating connection data \\[without clashes data, for now\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/russell/flavor_tool/venv/lib/python3.6/site-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.1 s, sys: 11.7 ms, total: 27.1 s\n",
      "Wall time: 27.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/russell/flavor_tool/venv/lib/python3.6/site-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# # 23s\n",
    "\n",
    "# # CREATING CONNECTION DATA AND ADDING IT TO stir_fry DATA (& getting stir_fry_data in order)\n",
    "# # takes a minute\n",
    "# stir_fry_names = stir_fry_data['name'].values.tolist()\n",
    "# for col_name in stir_fry_names:\n",
    "#     col_values = []\n",
    "#     for row_name in stir_fry_names:\n",
    "#         pairing_value = pairing_data_with_umbrella_matches[col_name][pairing_data_with_umbrella_matches['name'] == row_name].iloc[0]\n",
    "# #         clashing_value = clashes_with_data[col_name][clashes_with_data['name'] == row_name].iloc[0]\n",
    "\n",
    "# #         if clashing_value == 'Y':\n",
    "# #             col_values.append('N')\n",
    "# #         elif clashing_value == 'y':\n",
    "# #             col_values.append('n')\n",
    "# #         elif pairing_value == 'D':\n",
    "# #             col_values.append('D')\n",
    "# #         elif pairing_value == 'C':\n",
    "# #             col_values.append('C')\n",
    "# #         elif pairing_value == 'd':\n",
    "# #             col_values.append('d')\n",
    "# #         elif pairing_value == 'c':\n",
    "# #             col_values.append('c')\n",
    "# #         else:\n",
    "# #             col_values.append('')\n",
    "#         col_values.append(pairing_value)\n",
    "\n",
    "#     stir_fry_data[col_name] = pd.Series(col_values)\n",
    "    \n",
    "# # just in case\n",
    "# stir_fry_data.sort_values('name', inplace=True)\n",
    "# stir_fry_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stir_fry_data.to_pickle(os.path.join(root_path, 'DATA/stir_fry_data_with_names_and_umbrella_matches.pickle'))\n",
    "stir_fry_data = pd.read_pickle(os.path.join(root_path, 'DATA/stir_fry_data_with_names_and_umbrella_matches.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Creating flavor tool data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stir_fry_flavor_data = stir_fry_data[['staple', 'not_vegan', 'gluten', 'flavoring_sweet', 'flavoring_fresh', 'flavoring_wet', 'flavoring_concentrate', 'flavoring', 'flavoring_dry', 'protein', 'protein_cheese_sub', 'protein_milk_sub', 'protein_meat_sub', 'protein_bean', 'veg', 'veg_leafy', 'grain', 'grain_flour', 'fat_oil', 'oil', 'fat', 'fruit', 'fruit_berry', 'stir_fry', 'stir_fry_basic', 'stir_fry_yes', 'stir_fry_early', 'stir_fry_mid', 'stir_fry_late', 'stir_fry_garnish', 'stir_fry_umbrella', 'protein_nut_seed', 'protein_nut', 'protein_seed', 'redirect', 'salty', 'sour', 'spicy', 'bitter', 'savory', 'sweet','name', 'flavor', 'volume', 'pairs_with', 'phonetic', 'techniques', 'dishes', 'tip', 'possible_substitutes', 'flavor_affinities', 'nutritional_profile', 'season', 'botanical_relatives', 'protein_content', 'what_they_are', 'brands', 'vegan_substitutes', 'vegan_brands', 'stir_fry_protein', 'stir_fry_protein_nut_seed', 'stir_fry_protein_nut', 'stir_fry_protein_seed', 'stir_fry_flavoring', 'stir_fry_oil', 'stir_fry_fat', 'stir_fry_fat_oil', 'stir_fry_fruit', 'stir_fry_allium', 'stir_fry_protein_bean', 'stir_fry_veg', 'stir_fry_grain', 'stir_fry_salt', 'stir_fry_pepper', 'stir_fry_vinegar', 'strong', 'stir_fry_mushroom', 'stir_fry_main', 'stir_fry_useful_umbrella'] + stir_fry_data['name'].tolist()].copy()\n",
    "# stir_fry_flavor_data['id'] = range(len(stir_fry_flavor_data))\n",
    "\n",
    "# selected_pairing_data = pairing_data_with_umbrella_matches[['name', 'pairs_with_terms', 'lower_category_names', 'lower_direct_names', 'upper_category_names', 'upper_direct_names', 'lower_names', 'upper_names', 'all_names', 'lc_sorted_pairs', 'ld_sorted_pairs', 'uc_sorted_pairs', 'ud_sorted_pairs', 'l_sorted_pairs', 'u_sorted_pairs', 'a_sorted_pairs']]\n",
    "# stir_fry_flavor_data = stir_fry_flavor_data.merge(selected_pairing_data, how='inner', on='name')\n",
    "\n",
    "# # selected_clashes_with_data = clashes_with_data[['name', 'lower_clashes_with_names', 'upper_clashes_with_names', 'all_clashes_with_names', 'lower_clashes_with_pairs', 'upper_clashes_with_pairs', 'all_clashes_with_pairs']]\n",
    "# # stir_fry_flavor_data = stir_fry_flavor_data.merge(selected_clashes_with_data, how='inner', on='name')\n",
    "\n",
    "# stir_fry_flavor_data.replace(float('nan'), '', inplace=True)\n",
    "# stir_fry_flavor_data.set_index('name', drop=False, inplace=True)\n",
    "# stir_fry_flavor_data.rename_axis('name_index', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/russell/flavor_tool/venv/lib/python3.6/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.1 s, sys: 40 ms, total: 13.1 s\n",
      "Wall time: 13.1 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# # 13s\n",
    "\n",
    "# # ADD STRENGTH VALUES\n",
    "# ref_flavor_data = stir_fry_flavor_data.copy()\n",
    "# for i_1, row_1 in ref_flavor_data.iterrows():\n",
    "#     for i_2, row_2 in ref_flavor_data.iterrows():\n",
    "#         pairs_with_value = ref_flavor_data[row_1['name']][row_2['name']] or '_'\n",
    "#         if row_1['strong'] in ['y', 'Y'] and row_2['strong'] in ['y', 'Y']:\n",
    "#             strength_value = 'S'\n",
    "#         elif row_1['strong'] in ['y', 'Y'] or row_2['strong'] in ['y', 'Y']:\n",
    "#             strength_value = 's'\n",
    "#         else:\n",
    "#             strength_value = '_'\n",
    "            \n",
    "#         stir_fry_flavor_data[row_1['name']][row_2['name']] = pairs_with_value + strength_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stir_fry_flavor_data.to_pickle(os.path.join(root_path, 'DATA/stir_fry_flavor_data_with_umbrella.pickle'))\n",
    "# stir_fry_flavor_data.to_pickle(os.path.join(root_path, '../data/stir_fry_flavor_data_with_umbrella.pickle'))\n",
    "stir_fry_flavor_data = pd.read_pickle(os.path.join(root_path, 'DATA/stir_fry_flavor_data_with_umbrella.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. stir_fry recipe generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. Flavor tool generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: networkx in /home/russell/flavor_tool/venv/lib/python3.6/site-packages (2.4)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/russell/flavor_tool/venv/lib/python3.6/site-packages (from networkx) (4.4.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the '/home/russell/flavor_tool/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'networkx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-ca894b7c65aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# !pip install pyvis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# from pyvis import network as net\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'networkx'"
     ]
    }
   ],
   "source": [
    "# !pip install networkx==2.4\n",
    "# !pip install pyvis\n",
    "\n",
    "import networkx as nx\n",
    "from pyvis import network as net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE PLAN\n",
    "    # is to let 'flavor balance bonus' take care of salt, vinegar, fruit etc. selection\n",
    "    # the only exception is 1 picked salt in every dish. salt also appears in flavorings, so I'll discourage doubling but not forbid it.\n",
    "\n",
    "    # Not gonna worry about balancing food groups too much.\n",
    "        # factor for protein balance? (YES, THIS)\n",
    "        # or selected protein? (0-3?)\n",
    "            # but without food group balance OR texture balance, only regulator would be pairs w\n",
    "    # oh! and maybe a hard limit on flavorings?\n",
    "        # or a bonus.. but that'd reward a lot, and I really only want a few flavorings per dish? (and for sure a few flavorings)\n",
    "        # HARD LIMIT IT IS\n",
    "            # but I think I'll draw from other_flavorings after all, if flavorings and salt are the only other distinct groups\n",
    "     # and I think fat/oil, like salt, is one of those things where you gotta add a certain amt, but if there's a lot you can just add a little\n",
    "        # so that's a distinct category, then\n",
    "        \n",
    "# UNKNOWNS\n",
    "    # 7 or 8 max? and will I need to reward balance?\n",
    "    # special allium category? or just reward alliums? or just onions?\n",
    "    \n",
    "n_locked = random.randrange(0, 1)\n",
    "locked = stir_fry_data.sample(n_locked)\n",
    "# locked_proteins = locked[locked['protein'] == 'y']\n",
    "# locked_protein_nut_seeds = locked[locked['stir_fry_protein_nut_seed'] == 'y']\n",
    "# locked_protein_nuts = locked[locked['stir_fry_protein_nut'] == 'y']\n",
    "# locked_protein_seeds = locked[locked['stir_fry_protein_seed'] == 'y']\n",
    "# locked_alliums = locked[locked['stir_fry_allium'] == 'y']\n",
    "# locked_flavorings = locked[locked['stir_fry_flavoring'] == 'y']\n",
    "locked_fat_oils = locked[locked['stir_fry_fat_oil'] == 'y']\n",
    "# locked_fats = locked[locked['stir_fry_fat'] == 'y']\n",
    "# locked_oils = locked[locked['stir_fry_oil'] == 'y']\n",
    "# locked_fruits = locked[locked['stir_fry_fruit'] == 'y']\n",
    "# locked_protein_beans = locked[locked['stir_fry_protein_bean'] == 'y']\n",
    "# locked_veg = locked[locked['stir_fry_veg'] == 'y']\n",
    "# locked_grains = locked[locked['stir_fry_grain'] == 'y']\n",
    "locked_salts = locked[locked['stir_fry_salt'] == 'y']\n",
    "# locked_peppers = locked[locked['stir_fry_pepper'] == 'y']\n",
    "# locked_vinegars = locked[locked['stir_fry_vinegar'] == 'y']\n",
    "locked_other_flavorings = locked[(locked['stir_fry_flavoring'] == 'y') & (locked['stir_fry_salt'] != 'y')]\n",
    "locked_foodstuffs = locked[(locked['stir_fry_fat_oil'] != 'y') & (locked['stir_fry_salt'] != 'y') & (locked['stir_fry_flavoring'] != 'y')]\n",
    "\n",
    "the_rest = stir_fry_data[~stir_fry_data['name'].isin(locked['name'])]\n",
    "# the_rest_proteins = the_rest[the_rest['protein'] == 'y']\n",
    "# the_rest_protein_nut_seeds = the_rest[the_rest['stir_fry_protein_nut_seed'] == 'y']\n",
    "# the_rest_protein_nuts = the_rest[the_rest['stir_fry_protein_nut'] == 'y']\n",
    "# the_rest_protein_seeds = the_rest[the_rest['stir_fry_protein_seed'] == 'y']\n",
    "# the_rest_alliums = the_rest[the_rest['stir_fry_allium'] == 'y']\n",
    "# the_rest_flavorings = the_rest[the_rest['stir_fry_flavoring'] == 'y']\n",
    "the_rest_fat_oils = the_rest[the_rest['stir_fry_fat_oil'] == 'y']\n",
    "# the_rest_fats = the_rest[the_rest['stir_fry_fat'] == 'y']\n",
    "# the_rest_oils = the_rest[the_rest['stir_fry_oil'] == 'y']\n",
    "# the_rest_fruits = the_rest[the_rest['stir_fry_fruit'] == 'y']\n",
    "# the_rest_protein_beans = the_rest[the_rest['stir_fry_protein_bean'] == 'y']\n",
    "# the_rest_veg = the_rest[the_rest['stir_fry_veg'] == 'y']\n",
    "# the_rest_grains = the_rest[the_rest['stir_fry_grain'] == 'y']\n",
    "the_rest_salts = the_rest[the_rest['stir_fry_salt'] == 'y']\n",
    "# the_rest_peppers = the_rest[the_rest['stir_fry_pepper'] == 'y']\n",
    "# the_rest_vinegars = the_rest[the_rest['stir_fry_vinegar'] == 'y']\n",
    "the_rest_other_flavorings = the_rest[(the_rest['stir_fry_flavoring'] == 'y') & (the_rest['stir_fry_salt'] != 'y')]\n",
    "the_rest_foodstuffs = the_rest[(the_rest['stir_fry_fat_oil'] != 'y') & (the_rest['stir_fry_salt'] != 'y') & (the_rest['stir_fry_flavoring'] != 'y')]\n",
    "\n",
    "n_gen_salts = max(1 - len(locked_salts), 0)\n",
    "n_gen_fat_oils = max(1 - len(locked_fat_oils), 0)\n",
    "n_gen_other_flavorings_min = max(1 - len(locked_other_flavorings), 0)\n",
    "n_gen_other_flavorings_max = max(3 - len(locked_other_flavorings), 0)\n",
    "n_gen_foodstuffs_min = max(3 - len(locked_foodstuffs), 0)\n",
    "n_gen_foodstuffs_max = max(7 - len(locked_foodstuffs), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-4a6461d1f32f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mupper_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupper_category_pairs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mupper_direct_pairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mall_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlower_pairs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mupper_pairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_nodes_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edges_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlower_category_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nx' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO: account for if connected subgraph is impossible\n",
    "\n",
    "top_score = 0\n",
    "for try_i in range(100):\n",
    "    n_subgraphs = 2\n",
    "    while n_subgraphs > 1: # keep shuffling until you get a well connected graph\n",
    "        n_gen_other_flavorings = min(random.randrange(n_gen_other_flavorings_min, n_gen_other_flavorings_max+1), len(the_rest_other_flavorings))\n",
    "        n_gen_foodstuffs = min(random.randrange(n_gen_foodstuffs_min, n_gen_foodstuffs_max+1), len(the_rest_foodstuffs))\n",
    "        \n",
    "        selected_salts = locked_salts.append(the_rest_salts.sample(n_gen_salts))\n",
    "        selected_fat_oils = locked_fat_oils.append(the_rest_fat_oils.sample(n_gen_fat_oils))\n",
    "        selected_other_flavorings = locked_other_flavorings.append(the_rest_other_flavorings.sample(n_gen_other_flavorings))\n",
    "        selected_foodstuffs = locked_foodstuffs.append(the_rest_foodstuffs.sample(n_gen_foodstuffs))\n",
    "        selected_ingredients = selected_salts.append(selected_fat_oils).append(selected_other_flavorings).append(selected_foodstuffs)\n",
    "        selected_names = selected_ingredients['name'].values.tolist()\n",
    "\n",
    "        lower_category_pairs = []\n",
    "        lower_direct_pairs = []\n",
    "        upper_category_pairs = []\n",
    "        upper_direct_pairs = []\n",
    "\n",
    "        # finicky but pretty fast\n",
    "        for i, col_name in enumerate(selected_names):\n",
    "            for j, row_name in enumerate(selected_names[i+1:]):\n",
    "                connection = selected_ingredients[col_name].tolist()[i+1+j] # this is what is finicky\n",
    "                if connection == 'c':\n",
    "                    lower_category_pairs.append((col_name, row_name,))\n",
    "                elif connection == 'd':\n",
    "                    lower_direct_pairs.append((col_name, row_name,))\n",
    "                elif connection == 'C':\n",
    "                    upper_category_pairs.append((col_name, row_name,))\n",
    "                elif connection == 'D':\n",
    "                    upper_direct_pairs.append((col_name, row_name,))\n",
    "        lower_pairs = lower_category_pairs + lower_direct_pairs\n",
    "        upper_pairs = upper_category_pairs + upper_direct_pairs\n",
    "        all_pairs = lower_pairs + upper_pairs\n",
    "        G = nx.Graph()\n",
    "        G.add_nodes_from(selected_names)\n",
    "        G.add_edges_from(lower_category_pairs, length=2)\n",
    "        G.add_edges_from(lower_direct_pairs, length=1.5)\n",
    "        G.add_edges_from(upper_category_pairs, length=1.2)\n",
    "        G.add_edges_from(upper_direct_pairs, length=1)\n",
    "        n_subgraphs = len(list(nx.connected_component_subgraphs(G)))\n",
    "    score = 0\n",
    "\n",
    "# PAIRING BONUS ============================================================================================\n",
    "# ranges from roughly (0 to 1) * 3, tho could be a lil over or under that range\n",
    "    average_shortest_path_length = nx.average_shortest_path_length(G, weight='length')\n",
    "    average_shortest_path_score = 1 / average_shortest_path_length * 4 - 1\n",
    "#     print(average_shortest_path_score)\n",
    "    score += average_shortest_path_score * 3\n",
    "\n",
    "# FLAVOR BALANCE BONUS =============================================================================================\n",
    "# ranges from roughly (0 to 1) * 1 (could be a lil over/under)\n",
    "    n_sweet_lower = (selected_ingredients['sweet'] == 'y').sum()\n",
    "    n_sweet_upper = (selected_ingredients['sweet'] == 'Y').sum()\n",
    "    n_salty_lower = (selected_ingredients['salty'] == 'y').sum()\n",
    "    n_salty_upper = (selected_ingredients['salty'] == 'Y').sum()\n",
    "    n_sour_lower = (selected_ingredients['sour'] == 'y').sum()\n",
    "    n_sour_upper = (selected_ingredients['sour'] == 'Y').sum()\n",
    "    n_savory_lower = (selected_ingredients['savory'] == 'y').sum()\n",
    "    n_savory_upper = (selected_ingredients['savory'] == 'Y').sum()\n",
    "    n_bitter_lower = (selected_ingredients['bitter'] == 'y').sum()\n",
    "    n_bitter_upper = (selected_ingredients['bitter'] == 'Y').sum()\n",
    "    n_spicy_lower = (selected_ingredients['spicy'] == 'y').sum()\n",
    "    n_spicy_upper = (selected_ingredients['spicy'] == 'Y').sum()\n",
    "\n",
    "    # each varies from roughly .5 to 1 (normalized to the average flavor score)\n",
    "    sweet_score = (n_sweet_lower/2 + n_sweet_upper)/6\n",
    "    salty_score = (n_salty_lower/2 + n_salty_upper)/4\n",
    "    sour_score = (n_sour_lower/2 + n_sour_upper)/2.5\n",
    "    savory_score = (n_savory_lower/2 + n_savory_upper)/3\n",
    "    bitter_score = (n_bitter_lower/2 + n_bitter_upper)/3\n",
    "    spicy_score = (n_spicy_lower/2 + n_spicy_upper)/3\n",
    "\n",
    "    flavor_balance_score = 5 / (1 + abs(1-sweet_score) + abs(1-salty_score) + abs(1-sour_score) + abs(1-savory_score) + abs(1-spicy_score)) - .9\n",
    "#     print(flavor_balance_score)\n",
    "    score += flavor_balance_score\n",
    "#     print()\n",
    "\n",
    "# will bias toward larger stir_frys, slightly\n",
    "# PROTEIN BONUS =========================================================================================\n",
    "# ranges from roughly (0 to 1) * .5 (mostly balanced on its own)\n",
    "    n_protein = (selected_ingredients['stir_fry_protein'] == 'y').sum()\n",
    "    \n",
    "    # /2 for steep diminishing returns (?)\n",
    "    protein_score = (n_protein/2)**.5 * .75\n",
    "#     print(protein_score)\n",
    "    score += protein_score * .5\n",
    "    \n",
    "    if score > top_score:\n",
    "        top_average_shortest_path_score = average_shortest_path_score\n",
    "        top_flavor_balance_score = flavor_balance_score\n",
    "        top_protein_score = protein_score\n",
    "        top_score = score\n",
    "        top_lc_pairs = lower_category_pairs\n",
    "        top_ld_pairs = lower_direct_pairs\n",
    "        top_uc_pairs = upper_category_pairs\n",
    "        top_ud_pairs = upper_direct_pairs\n",
    "        top_selected_ingredients = selected_ingredients\n",
    "print('RAW PAIRING BONUS', top_average_shortest_path_score)\n",
    "print('RAW FLAVOR BALANCE BONUS', top_flavor_balance_score)\n",
    "print('RAW PROTEIN BONUS', top_protein_score)\n",
    "print('SCORE', top_score)\n",
    "\n",
    "top_net = net.Network(notebook=True)\n",
    "\n",
    "nodes = top_selected_ingredients['name'].tolist()\n",
    "\n",
    "def get_color(row):\n",
    "#     return 'grey'\n",
    "    if row['stir_fry_veg'] == 'y':\n",
    "        return 'green'\n",
    "    elif row['stir_fry_fruit'] == 'y':\n",
    "        return 'orange'\n",
    "    elif row['stir_fry_protein'] == 'y':\n",
    "        return 'brown'\n",
    "    elif row['stir_fry_grain'] == 'y':\n",
    "        return 'tan'\n",
    "    else:\n",
    "        return 'lightgrey'\n",
    "nodes_color = top_selected_ingredients.apply(get_color, axis=1).tolist()\n",
    "\n",
    "top_net.add_nodes(\n",
    "    nodes=nodes,\n",
    "    color=nodes_color\n",
    ")\n",
    "\n",
    "for pair in top_lc_pairs:\n",
    "    top_net.add_edge(pair[0], pair[1], physics=False, color='lightgrey')\n",
    "for pair in top_ld_pairs:\n",
    "    top_net.add_edge(pair[0], pair[1], physics=False, color='grey')\n",
    "for pair in top_uc_pairs:\n",
    "    top_net.add_edge(pair[0], pair[1], color='darkgrey')\n",
    "for pair in top_ud_pairs:\n",
    "    top_net.add_edge(pair[0], pair[1], color='black')\n",
    "\n",
    "top_net.show('top_net.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. Black magic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.1. Preparing subgraph data\n",
    "(stuff that can be pre-calculated, pickled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "# import community\n",
    "from networkx.algorithms.clique import enumerate_all_cliques, find_cliques, cliques_containing_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_ingredients = stir_fry_flavor_data.copy()#.sample(200)\n",
    "selected_names = selected_ingredients['name'].tolist()\n",
    "\n",
    "lower_category_pairs = []\n",
    "lower_direct_pairs = []\n",
    "upper_category_pairs = []\n",
    "upper_direct_pairs = []\n",
    "\n",
    "for i_1, name_1 in enumerate(selected_names[:-1]):\n",
    "    for i_2, name_2 in enumerate(selected_names[i_1+1:], i_1+1):\n",
    "        connection = selected_ingredients[name_1][name_2] # this is what is finicky\n",
    "        if connection[0] == 'c':\n",
    "            lower_category_pairs.append((name_1, name_2,))\n",
    "        elif connection[0] == 'd':\n",
    "            lower_direct_pairs.append((name_1, name_2,))\n",
    "        elif connection[0] == 'C':\n",
    "            upper_category_pairs.append((name_1, name_2,))\n",
    "        elif connection[0] == 'D':\n",
    "            upper_direct_pairs.append((name_1, name_2,))\n",
    "lower_pairs = lower_category_pairs + lower_direct_pairs\n",
    "upper_pairs = upper_category_pairs + upper_direct_pairs\n",
    "all_pairs = lower_pairs + upper_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 523 ms, sys: 12 ms, total: 535 ms\n",
      "Wall time: 535 ms\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# G = nx.Graph()\n",
    "# G.add_nodes_from(selected_names)\n",
    "# G.add_edges_from(upper_category_pairs)\n",
    "# G.add_edges_from(upper_direct_pairs)\n",
    "\n",
    "# cliques_gen = find_cliques(G)\n",
    "# cliques_all = list(cliques_gen)\n",
    "# cliques = [clique for clique in cliques_all if len(clique) > 1]\n",
    "# clique_sets = [set(clique) for clique in cliques]\n",
    "\n",
    "# data = {\n",
    "#     s_name: ['y' if s_name in c_set else '' for c_set in clique_sets]\n",
    "# for s_name in selected_names}\n",
    "# clique_upper = pd.DataFrame(data)\n",
    "# clique_upper['type'] = ['clique_upper'] * len(clique_upper)\n",
    "# clique_upper['list'] = cliques\n",
    "# clique_upper['set'] = clique_sets\n",
    "# clique_upper['length'] = clique_upper['list'].apply(len)\n",
    "\n",
    "# clique_upper.to_pickle(os.path.join(root_path, 'DATA/stir_fry_clique_upper.pickle'))\n",
    "clique_upper = pd.read_pickle(os.path.join(root_path, 'DATA/stir_fry_clique_upper.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# G = nx.Graph()\n",
    "# G.add_nodes_from(selected_names)\n",
    "# G.add_edges_from(lower_direct_pairs)\n",
    "# G.add_edges_from(upper_category_pairs)\n",
    "# G.add_edges_from(upper_direct_pairs)\n",
    "\n",
    "# cliques_gen = find_cliques(G)\n",
    "# cliques_all = list(cliques_gen)\n",
    "# cliques = [clique for clique in cliques_all if len(clique) > 1]\n",
    "# clique_sets = [set(clique) for clique in cliques]\n",
    "\n",
    "# data = {\n",
    "#     s_name: ['y' if s_name in c_set else '' for c_set in clique_sets]\n",
    "# for s_name in selected_names}\n",
    "# clique_u_ld = pd.DataFrame(data)\n",
    "# clique_u_ld['type'] = ['clique_u_ld'] * len(clique_u_ld)\n",
    "# clique_u_ld['list'] = cliques\n",
    "# clique_u_ld['set'] = clique_sets\n",
    "# clique_u_ld['length'] = clique_u_ld['list'].apply(len)\n",
    "\n",
    "# clique_u_ld.to_pickle(os.path.join(root_path, 'DATA/clique_u_ld.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_ingredients = stir_fry_flavor_data[stir_fry_flavor_data['strong'].isin(['y', 'Y'])]\n",
    "# selected_names = selected_ingredients['name'].tolist()\n",
    "\n",
    "# lower_category_pairs = []\n",
    "# lower_direct_pairs = []\n",
    "# upper_category_pairs = []\n",
    "# upper_direct_pairs = []\n",
    "\n",
    "# for i_1, name_1 in enumerate(selected_names[:-1]):\n",
    "#     for i_2, name_2 in enumerate(selected_names[i_1+1:], i_1+1):\n",
    "#         connection = selected_ingredients[name_1][name_2] # this is what is finicky\n",
    "#         if connection[0] == 'c':\n",
    "#             lower_category_pairs.append((name_1, name_2,))\n",
    "#         elif connection[0] == 'd':\n",
    "#             lower_direct_pairs.append((name_1, name_2,))\n",
    "#         elif connection[0] == 'C':\n",
    "#             upper_category_pairs.append((name_1, name_2,))\n",
    "#         elif connection[0] == 'D':\n",
    "#             upper_direct_pairs.append((name_1, name_2,))\n",
    "# lower_pairs = lower_category_pairs + lower_direct_pairs\n",
    "# upper_pairs = upper_category_pairs + upper_direct_pairs\n",
    "# all_pairs = lower_pairs + upper_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# G = nx.Graph()\n",
    "# G.add_nodes_from(selected_names)\n",
    "# G.add_edges_from(upper_category_pairs)\n",
    "# G.add_edges_from(upper_direct_pairs)\n",
    "\n",
    "# cliques_gen = find_cliques(G)\n",
    "# cliques_all = list(cliques_gen)\n",
    "# cliques = [clique for clique in cliques_all if len(clique) > 1]\n",
    "# clique_sets = [set(clique) for clique in cliques]\n",
    "\n",
    "# data = {\n",
    "#     s_name: ['y' if s_name in c_set else '' for c_set in clique_sets]\n",
    "# for s_name in selected_names}\n",
    "# clique_upper_strong = pd.DataFrame(data)\n",
    "# clique_upper_strong['type'] = ['clique_upper_strong'] * len(clique_upper_strong)\n",
    "# clique_upper_strong['list'] = cliques\n",
    "# clique_upper_strong['set'] = clique_sets\n",
    "# clique_upper_strong['length'] = clique_upper_strong['list'].apply(len)\n",
    "\n",
    "# clique_upper_strong.to_pickle(os.path.join(root_path, 'DATA/clique_upper_strong.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# G = nx.Graph()\n",
    "# G.add_nodes_from(selected_names)\n",
    "# G.add_edges_from(lower_direct_pairs)\n",
    "# G.add_edges_from(upper_category_pairs)\n",
    "# G.add_edges_from(upper_direct_pairs)\n",
    "\n",
    "# cliques_gen = find_cliques(G)\n",
    "# cliques_all = list(cliques_gen)\n",
    "# cliques = [clique for clique in cliques_all if len(clique) > 1]\n",
    "# clique_sets = [set(clique) for clique in cliques]\n",
    "\n",
    "# data = {\n",
    "#     s_name: ['y' if s_name in c_set else '' for c_set in clique_sets]\n",
    "# for s_name in selected_names}\n",
    "# clique_u_ld_strong = pd.DataFrame(data)\n",
    "# clique_u_ld_strong['type'] = ['clique_u_ld_strong'] * len(clique_u_ld_strong)\n",
    "# clique_u_ld_strong['list'] = cliques\n",
    "# clique_u_ld_strong['set'] = clique_sets\n",
    "# clique_u_ld_strong['length'] = clique_u_ld_strong['list'].apply(len)\n",
    "\n",
    "# clique_u_ld_strong.to_pickle(os.path.join(root_path, 'DATA/clique_u_ld_strong.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# G = nx.Graph()\n",
    "# G.add_nodes_from(selected_names)\n",
    "# G.add_edges_from(lower_category_pairs)\n",
    "# G.add_edges_from(lower_direct_pairs)\n",
    "# G.add_edges_from(upper_category_pairs)\n",
    "# G.add_edges_from(upper_direct_pairs)\n",
    "\n",
    "# cliques_gen = find_cliques(G)\n",
    "# cliques_all = list(cliques_gen)\n",
    "# cliques = [clique for clique in cliques_all if len(clique) > 1]\n",
    "# clique_sets = [set(clique) for clique in cliques]\n",
    "\n",
    "# data = {\n",
    "#     s_name: ['y' if s_name in c_set else '' for c_set in clique_sets]\n",
    "# for s_name in selected_names}\n",
    "# clique_all_strong = pd.DataFrame(data)\n",
    "# clique_all_strong['type'] = ['clique_all_strong'] * len(clique_all_strong)\n",
    "# clique_all_strong['list'] = cliques\n",
    "# clique_all_strong['set'] = clique_sets\n",
    "# clique_all_strong['length'] = clique_all_strong['list'].apply(len)\n",
    "\n",
    "# clique_all_strong.to_pickle(os.path.join(root_path, 'DATA/clique_all_strong.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 290 ms, sys: 8 ms, total: 298 ms\n",
      "Wall time: 298 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# sweet_upper_set = set(stir_fry_flavor_data[stir_fry_flavor_data['sweet'] == 'Y']['name'])\n",
    "# sweet_lower_set = set(stir_fry_flavor_data[stir_fry_flavor_data['sweet'] == 'y']['name'])\n",
    "# sour_upper_set = set(stir_fry_flavor_data[stir_fry_flavor_data['sour'] == 'Y']['name'])\n",
    "# sour_lower_set = set(stir_fry_flavor_data[stir_fry_flavor_data['sour'] == 'y']['name'])\n",
    "# salty_upper_set = set(stir_fry_flavor_data[stir_fry_flavor_data['salty'] == 'Y']['name'])\n",
    "# salty_lower_set = set(stir_fry_flavor_data[stir_fry_flavor_data['salty'] == 'y']['name'])\n",
    "# savory_upper_set = set(stir_fry_flavor_data[stir_fry_flavor_data['savory'] == 'Y']['name'])\n",
    "# savory_lower_set = set(stir_fry_flavor_data[stir_fry_flavor_data['savory'] == 'y']['name'])\n",
    "# spicy_upper_set = set(stir_fry_flavor_data[stir_fry_flavor_data['spicy'] == 'Y']['name'])\n",
    "# spicy_lower_set = set(stir_fry_flavor_data[stir_fry_flavor_data['spicy'] == 'y']['name'])\n",
    "# bitter_upper_set = set(stir_fry_flavor_data[stir_fry_flavor_data['bitter'] == 'Y']['name'])\n",
    "# bitter_lower_set = set(stir_fry_flavor_data[stir_fry_flavor_data['bitter'] == 'y']['name'])\n",
    "salt_set = set(stir_fry_flavor_data[stir_fry_flavor_data['stir_fry_salt'] == 'y']['name'])\n",
    "fat_oil_set = set(stir_fry_flavor_data[stir_fry_flavor_data['stir_fry_fat_oil'] == 'y']['name'])\n",
    "other_flavoring_set = set(stir_fry_flavor_data[stir_fry_flavor_data['stir_fry_flavoring'] == 'y']['name']) - salt_set\n",
    "foodstuff_set = set(stir_fry_flavor_data['name']) - salt_set - fat_oil_set - other_flavoring_set\n",
    "mushroom_set = set(stir_fry_flavor_data[stir_fry_flavor_data['stir_fry_mushroom'] == 'y']['name'])\n",
    "# PROBS main_set = set(stir_fry_flavor_data['stir_fry_main'] == 'y')\n",
    "bean_set = set(stir_fry_flavor_data[stir_fry_flavor_data['stir_fry_protein_bean'] == 'y'])\n",
    "grain_set = set(stir_fry_flavor_data[stir_fry_flavor_data['stir_fry_grain'] == 'y'])\n",
    "\n",
    "# clique_upper['n_sweet_upper'] = [len(sweet_upper_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "# clique_upper['n_sweet_lower'] = [len(sweet_lower_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "# clique_upper['n_sour_upper'] = [len(sour_upper_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "# clique_upper['n_sour_lower'] = [len(sour_lower_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "# clique_upper['n_salty_upper'] = [len(salty_upper_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "# clique_upper['n_salty_lower'] = [len(salty_lower_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "# clique_upper['n_savory_upper'] = [len(savory_upper_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "# clique_upper['n_savory_lower'] = [len(savory_lower_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "# clique_upper['n_spicy_upper'] = [len(spicy_upper_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "# clique_upper['n_spicy_lower'] = [len(spicy_lower_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "# clique_upper['n_bitter_upper'] = [len(bitter_upper_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "# clique_upper['n_bitter_lower'] = [len(bitter_lower_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "clique_upper['salts'] = [list(salt_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "clique_upper['fat_oils'] = [list(fat_oil_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "clique_upper['other_flavorings'] = [list(other_flavoring_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "clique_upper['foodstuffs'] = [list(foodstuff_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "clique_upper['mushrooms'] = [list(mushroom_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "# clique_upper['mains'] = [list(main_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "clique_upper['beans'] = [list(bean_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "clique_upper['grains'] = [list(grain_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "\n",
    "clique_upper['salts_set'] = [set(salt_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "clique_upper['fat_oils_set'] = [set(fat_oil_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "clique_upper['other_flavorings_set'] = [set(other_flavoring_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "clique_upper['foodstuffs_set'] = [set(foodstuff_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "clique_upper['mushrooms_set'] = [set(mushroom_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "# clique_upper['mains_set'] = [set(main_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "clique_upper['beans_set'] = [set(bean_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "clique_upper['grains_set'] = [set(grain_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "\n",
    "# clique_upper['n_sweet_upper'] = [len(sweet_upper_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "# clique_upper['n_sweet_lower'] = [len(sweet_lower_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "# clique_upper['n_sour_upper'] = [len(sour_upper_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "# clique_upper['n_sour_lower'] = [len(sour_lower_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "# clique_upper['n_salty_upper'] = [len(salty_upper_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "# clique_upper['n_salty_lower'] = [len(salty_lower_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "# clique_upper['n_savory_upper'] = [len(savory_upper_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "# clique_upper['n_savory_lower'] = [len(savory_lower_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "# clique_upper['n_spicy_upper'] = [len(spicy_upper_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "# clique_upper['n_spicy_lower'] = [len(spicy_lower_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "# clique_upper['n_bitter_upper'] = [len(bitter_upper_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "# clique_upper['n_bitter_lower'] = [len(bitter_lower_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "clique_upper['n_salts'] = [len(salt_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "clique_upper['n_fat_oils'] = [len(fat_oil_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "clique_upper['n_other_flavorings'] = [len(other_flavoring_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "clique_upper['n_foodstuffs'] = [len(foodstuff_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "clique_upper['n_mushrooms'] = [len(mushroom_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "# clique_upper['n_mains'] = [len(main_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "# clique_upper['n_beans'] = [len(bean_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "# clique_upper['n_grains'] = [len(grain_set.intersection(clique_set)) for clique_set in list(clique_upper['set'])]\n",
    "\n",
    "# clique_u_ld['n_sweet_upper'] = [len(sweet_upper_set.intersection(clique_set)) for clique_set in list(clique_u_ld['set'])]\n",
    "# clique_u_ld['n_sweet_lower'] = [len(sweet_lower_set.intersection(clique_set)) for clique_set in list(clique_u_ld['set'])]\n",
    "# clique_u_ld['n_sour_upper'] = [len(sour_upper_set.intersection(clique_set)) for clique_set in list(clique_u_ld['set'])]\n",
    "# clique_u_ld['n_sour_lower'] = [len(sour_lower_set.intersection(clique_set)) for clique_set in list(clique_u_ld['set'])]\n",
    "# clique_u_ld['n_salty_upper'] = [len(salty_upper_set.intersection(clique_set)) for clique_set in list(clique_u_ld['set'])]\n",
    "# clique_u_ld['n_salty_lower'] = [len(salty_lower_set.intersection(clique_set)) for clique_set in list(clique_u_ld['set'])]\n",
    "# clique_u_ld['n_savory_upper'] = [len(savory_upper_set.intersection(clique_set)) for clique_set in list(clique_u_ld['set'])]\n",
    "# clique_u_ld['n_savory_lower'] = [len(savory_lower_set.intersection(clique_set)) for clique_set in list(clique_u_ld['set'])]\n",
    "# clique_u_ld['n_spicy_upper'] = [len(spicy_upper_set.intersection(clique_set)) for clique_set in list(clique_u_ld['set'])]\n",
    "# clique_u_ld['n_spicy_lower'] = [len(spicy_lower_set.intersection(clique_set)) for clique_set in list(clique_u_ld['set'])]\n",
    "# clique_u_ld['n_bitter_upper'] = [len(bitter_upper_set.intersection(clique_set)) for clique_set in list(clique_u_ld['set'])]\n",
    "# clique_u_ld['n_bitter_lower'] = [len(bitter_lower_set.intersection(clique_set)) for clique_set in list(clique_u_ld['set'])]\n",
    "# clique_u_ld['n_salts'] = [len(salt_set.intersection(clique_set)) for clique_set in list(clique_u_ld['set'])]\n",
    "# clique_u_ld['n_fat_oils'] = [len(fat_oil_set.intersection(clique_set)) for clique_set in list(clique_u_ld['set'])]\n",
    "# clique_u_ld['n_other_flavorings'] = [len(other_flavoring_set.intersection(clique_set)) for clique_set in list(clique_u_ld['set'])]\n",
    "# clique_u_ld['n_foodstuffs'] = [len(foodstuff_set.intersection(clique_set)) for clique_set in list(clique_u_ld['set'])]\n",
    "# clique_u_ld['n_mushrooms'] = [len(mushroom_set.intersection(clique_set)) for clique_set in list(clique_u_ld['set'])]\n",
    "# clique_u_ld['n_mains'] = [len(main_set.intersection(clique_set)) for clique_set in list(clique_u_ld['set'])]\n",
    "# clique_u_ld['n_beans'] = [len(bean_set.intersection(clique_set)) for clique_set in list(clique_u_ld['set'])]\n",
    "# clique_u_ld['n_grains'] = [len(grain_set.intersection(clique_set)) for clique_set in list(clique_u_ld['set'])]\n",
    "\n",
    "# clique_upper_strong['n_sweet_upper'] = [len(sweet_upper_set.intersection(clique_set)) for clique_set in list(clique_upper_strong['set'])]\n",
    "# clique_upper_strong['n_sweet_lower'] = [len(sweet_lower_set.intersection(clique_set)) for clique_set in list(clique_upper_strong['set'])]\n",
    "# clique_upper_strong['n_sour_upper'] = [len(sour_upper_set.intersection(clique_set)) for clique_set in list(clique_upper_strong['set'])]\n",
    "# clique_upper_strong['n_sour_lower'] = [len(sour_lower_set.intersection(clique_set)) for clique_set in list(clique_upper_strong['set'])]\n",
    "# clique_upper_strong['n_salty_upper'] = [len(salty_upper_set.intersection(clique_set)) for clique_set in list(clique_upper_strong['set'])]\n",
    "# clique_upper_strong['n_salty_lower'] = [len(salty_lower_set.intersection(clique_set)) for clique_set in list(clique_upper_strong['set'])]\n",
    "# clique_upper_strong['n_savory_upper'] = [len(savory_upper_set.intersection(clique_set)) for clique_set in list(clique_upper_strong['set'])]\n",
    "# clique_upper_strong['n_savory_lower'] = [len(savory_lower_set.intersection(clique_set)) for clique_set in list(clique_upper_strong['set'])]\n",
    "# clique_upper_strong['n_spicy_upper'] = [len(spicy_upper_set.intersection(clique_set)) for clique_set in list(clique_upper_strong['set'])]\n",
    "# clique_upper_strong['n_spicy_lower'] = [len(spicy_lower_set.intersection(clique_set)) for clique_set in list(clique_upper_strong['set'])]\n",
    "# clique_upper_strong['n_bitter_upper'] = [len(bitter_upper_set.intersection(clique_set)) for clique_set in list(clique_upper_strong['set'])]\n",
    "# clique_upper_strong['n_bitter_lower'] = [len(bitter_lower_set.intersection(clique_set)) for clique_set in list(clique_upper_strong['set'])]\n",
    "# clique_upper_strong['n_salt'] = [len(salt_set.intersection(clique_set)) for clique_set in list(clique_upper_strong['set'])]\n",
    "# clique_upper_strong['n_fat_oil'] = [len(fat_oil_set.intersection(clique_set)) for clique_set in list(clique_upper_strong['set'])]\n",
    "# clique_upper_strong['n_other_flavoring'] = [len(other_flavoring_set.intersection(clique_set)) for clique_set in list(clique_upper_strong['set'])]\n",
    "\n",
    "# clique_u_ld_strong['n_sweet_upper'] = [len(sweet_upper_set.intersection(clique_set)) for clique_set in list(clique_u_ld_strong['set'])]\n",
    "# clique_u_ld_strong['n_sweet_lower'] = [len(sweet_lower_set.intersection(clique_set)) for clique_set in list(clique_u_ld_strong['set'])]\n",
    "# clique_u_ld_strong['n_sour_upper'] = [len(sour_upper_set.intersection(clique_set)) for clique_set in list(clique_u_ld_strong['set'])]\n",
    "# clique_u_ld_strong['n_sour_lower'] = [len(sour_lower_set.intersection(clique_set)) for clique_set in list(clique_u_ld_strong['set'])]\n",
    "# clique_u_ld_strong['n_salty_upper'] = [len(salty_upper_set.intersection(clique_set)) for clique_set in list(clique_u_ld_strong['set'])]\n",
    "# clique_u_ld_strong['n_salty_lower'] = [len(salty_lower_set.intersection(clique_set)) for clique_set in list(clique_u_ld_strong['set'])]\n",
    "# clique_u_ld_strong['n_savory_upper'] = [len(savory_upper_set.intersection(clique_set)) for clique_set in list(clique_u_ld_strong['set'])]\n",
    "# clique_u_ld_strong['n_savory_lower'] = [len(savory_lower_set.intersection(clique_set)) for clique_set in list(clique_u_ld_strong['set'])]\n",
    "# clique_u_ld_strong['n_spicy_upper'] = [len(spicy_upper_set.intersection(clique_set)) for clique_set in list(clique_u_ld_strong['set'])]\n",
    "# clique_u_ld_strong['n_spicy_lower'] = [len(spicy_lower_set.intersection(clique_set)) for clique_set in list(clique_u_ld_strong['set'])]\n",
    "# clique_u_ld_strong['n_bitter_upper'] = [len(bitter_upper_set.intersection(clique_set)) for clique_set in list(clique_u_ld_strong['set'])]\n",
    "# clique_u_ld_strong['n_bitter_lower'] = [len(bitter_lower_set.intersection(clique_set)) for clique_set in list(clique_u_ld_strong['set'])]\n",
    "# clique_u_ld_strong['n_salts'] = [len(salt_set.intersection(clique_set)) for clique_set in list(clique_u_ld_strong['set'])]\n",
    "# clique_u_ld_strong['n_fat_oils'] = [len(fat_oil_set.intersection(clique_set)) for clique_set in list(clique_u_ld_strong['set'])]\n",
    "# clique_u_ld_strong['n_other_flavorings'] = [len(other_flavoring_set.intersection(clique_set)) for clique_set in list(clique_u_ld_strong['set'])]\n",
    "# clique_u_ld_strong['n_foodstuffs'] = [len(foodstuff_set.intersection(clique_set)) for clique_set in list(clique_u_ld_strong['set'])]\n",
    "# clique_u_ld_strong['n_mushrooms'] = [len(mushroom_set.intersection(clique_set)) for clique_set in list(clique_u_ld_strong['set'])]\n",
    "# clique_u_ld_strong['n_mains'] = [len(main_set.intersection(clique_set)) for clique_set in list(clique_u_ld_strong['set'])]\n",
    "# clique_u_ld_strong['n_beans'] = [len(bean_set.intersection(clique_set)) for clique_set in list(clique_u_ld_strong['set'])]\n",
    "# clique_u_ld_strong['n_grains'] = [len(grain_set.intersection(clique_set)) for clique_set in list(clique_u_ld_strong['set'])]\n",
    "\n",
    "# clique_all_strong['n_sweet_upper'] = [len(sweet_upper_set.intersection(clique_set)) for clique_set in list(clique_all_strong['set'])]\n",
    "# clique_all_strong['n_sweet_lower'] = [len(sweet_lower_set.intersection(clique_set)) for clique_set in list(clique_all_strong['set'])]\n",
    "# clique_all_strong['n_sour_upper'] = [len(sour_upper_set.intersection(clique_set)) for clique_set in list(clique_all_strong['set'])]\n",
    "# clique_all_strong['n_sour_lower'] = [len(sour_lower_set.intersection(clique_set)) for clique_set in list(clique_all_strong['set'])]\n",
    "# clique_all_strong['n_salty_upper'] = [len(salty_upper_set.intersection(clique_set)) for clique_set in list(clique_all_strong['set'])]\n",
    "# clique_all_strong['n_salty_lower'] = [len(salty_lower_set.intersection(clique_set)) for clique_set in list(clique_all_strong['set'])]\n",
    "# clique_all_strong['n_savory_upper'] = [len(savory_upper_set.intersection(clique_set)) for clique_set in list(clique_all_strong['set'])]\n",
    "# clique_all_strong['n_savory_lower'] = [len(savory_lower_set.intersection(clique_set)) for clique_set in list(clique_all_strong['set'])]\n",
    "# clique_all_strong['n_spicy_upper'] = [len(spicy_upper_set.intersection(clique_set)) for clique_set in list(clique_all_strong['set'])]\n",
    "# clique_all_strong['n_spicy_lower'] = [len(spicy_lower_set.intersection(clique_set)) for clique_set in list(clique_all_strong['set'])]\n",
    "# clique_all_strong['n_bitter_upper'] = [len(bitter_upper_set.intersection(clique_set)) for clique_set in list(clique_all_strong['set'])]\n",
    "# clique_all_strong['n_bitter_lower'] = [len(bitter_lower_set.intersection(clique_set)) for clique_set in list(clique_all_strong['set'])]\n",
    "# clique_all_strong['n_salt'] = [len(salt_set.intersection(clique_set)) for clique_set in list(clique_all_strong['set'])]\n",
    "# clique_all_strong['n_fat_oil'] = [len(fat_oil_set.intersection(clique_set)) for clique_set in list(clique_all_strong['set'])]\n",
    "# clique_all_strong['n_other_flavoring'] = [len(other_flavoring_set.intersection(clique_set)) for clique_set in list(clique_all_strong['set'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # FOREVERRRR\n",
    "# # Something is definitely very wrong, here. I'm not gonna worry abt it, tho, cause I'm not using this data atm.\n",
    "\n",
    "# def get_u_ld_superset_indices(clique_set):\n",
    "#     return [i for i, value in clique_u_ld['set'].iteritems() if clique_set.issubset(value)]\n",
    "\n",
    "# clique_upper['u_ld_supersets'] = clique_upper['set'].apply(get_u_ld_superset_indices)\n",
    "# clique_upper.to_pickle(os.path.join(root_path, 'DATA/clique_upper_with_supersets.pickle'))\n",
    "\n",
    "# clique_u_ld_strong['u_ld_supersets'] = clique_u_ld_strong['set'].apply(get_u_ld_superset_indices)\n",
    "# clique_u_ld_strong.to_pickle(os.path.join(root_path, 'DATA/clique_u_ld_strong_with_supersets.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 47, good enough (tried other things, ugh)\n",
    "# # This SHOULD be working, but (again) I don't need it so just not gonna bother.\n",
    "\n",
    "# def get_upper_subset_data(clique_set):    \n",
    "#     data_tuples = [(i, len(value)) for i, value in clique_upper['set'].iteritems() if value.issubset(clique_set)]\n",
    "#     subset_indices = [data_tuple[0] for data_tuple in data_tuples]\n",
    "#     subset_lengths = [data_tuple[1] for data_tuple in data_tuples]\n",
    "#     subset_lengths_sum = sum(subset_lengths)\n",
    "#     return (subset_indices, subset_lengths, subset_lengths_sum)\n",
    "\n",
    "# upper_subset_data = clique_u_ld['set'].apply(get_upper_subset_data)\n",
    "# clique_u_ld['upper_subset_indices'] = upper_subset_data.apply(lambda x: x[0])\n",
    "# clique_u_ld['upper_subset_lengths'] = upper_subset_data.apply(lambda x: x[1])\n",
    "# clique_u_ld['upper_subset_lengths_sum'] = upper_subset_data.apply(lambda x: x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 14s\n",
    "# # Once again, not gonna bother.\n",
    "\n",
    "# def get_strong_subset_data(clique_set):    \n",
    "#     data_tuples = [(i, len(value)) for i, value in clique_u_ld_strong['set'].iteritems() if value.issubset(clique_set)]\n",
    "#     subset_indices = [data_tuple[0] for data_tuple in data_tuples]\n",
    "#     subset_lengths = [data_tuple[1] for data_tuple in data_tuples]\n",
    "#     subset_lengths_sum = sum(subset_lengths)\n",
    "#     return (subset_indices, subset_lengths, subset_lengths_sum)\n",
    "\n",
    "# strong_subset_data = clique_u_ld['set'].apply(get_strong_subset_data)\n",
    "# clique_u_ld['strong_subset_indices'] = strong_subset_data.apply(lambda x: x[0])\n",
    "# clique_u_ld['strong_subset_lengths'] = strong_subset_data.apply(lambda x: x[1])\n",
    "# clique_u_ld['strong_subset_lengths_sum'] = strong_subset_data.apply(lambda x: x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 1m 17s\n",
    "# # Not rly using this I don't think, so not gonna bother\n",
    "\n",
    "# def get_connection_density(clique_list):\n",
    "#     connection_weighted_sum = 0\n",
    "#     for i_1, name_1 in enumerate(clique_list[:-1]):\n",
    "#         for i_2, name_2 in enumerate(clique_list[i_1+1:], i_1+1):\n",
    "#             connection = stir_fry_flavor_data[name_1][name_2]\n",
    "    \n",
    "#             if connection[0] == 'c':\n",
    "#                 pairs_with_score = .2\n",
    "#             elif connection[0] == 'd':\n",
    "#                 pairs_with_score = .4\n",
    "#             elif connection[0] == 'C':\n",
    "#                 pairs_with_score = .6\n",
    "#             elif connection[0] == 'D':\n",
    "#                 pairs_with_score = .8\n",
    "#             else:\n",
    "#                 print('OH NO! BAD PAIRING VALUE.')\n",
    "\n",
    "#             if connection[1] == '_':\n",
    "#                 strength_score = 0\n",
    "#             elif connection[1] == 's':\n",
    "#                 strength_score = .1\n",
    "#             elif connection[1] == 'S':\n",
    "#                 strength_score = .2\n",
    "#             else:\n",
    "#                 print('OH NO! BAD STRENGTH VALUE.')\n",
    "#             connection_weight = pairs_with_score + strength_score\n",
    "#             connection_weighted_sum += connection_weight\n",
    "#     connection_density = connection_weighted_sum / math.factorial(len(clique_list)-1)\n",
    "# #     print(connection_density)\n",
    "#     return connection_density\n",
    "\n",
    "\n",
    "\n",
    "# clique_u_ld['density'] = clique_u_ld['list'].apply(get_connection_density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THESE ARE THE ACTUAL CONTROLS FOR THE GENERATOR (I think? it's been a bit.)\n",
    "n_salts = 1\n",
    "n_fat_oils = 1\n",
    "n_other_flavorings_min = 1\n",
    "n_other_flavorings_max = 3\n",
    "n_foodstuffs_min = 3\n",
    "n_foodstuffs_max = 7\n",
    "# n_mushrooms_min = 0\n",
    "mushrooms_cap = 1 # possible to have more than this, if there are locked mushrooms (I don't update sets after locked/gen is established)\n",
    "n_beans_max = 1\n",
    "n_grains_max = 1\n",
    "# n_beans_min = 0\n",
    "# n_beans_max = 2\n",
    "# n_grains_min = 0\n",
    "# n_grains_max = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important, don't delete.\n",
    "\n",
    "reasonable_clique_upper = clique_upper.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # len10k: 500ms\n",
    "# # len1k: 50ms\n",
    "\n",
    "# # doing this before get_connections_data, cause filtering out nodes would screw w that\n",
    "\n",
    "# def get_reasonable_mushrooms_data(row):\n",
    "# #     reasonable_n_salts = min(row['n_salts'], n_salts)\n",
    "# #     reasonable_n_fat_oils = min(row['n_fat_oils'], n_fat_oils)\n",
    "# #     reasonable_n_other_flavorings = min(row['n_other_flavorings'], n_other_flavorings_max)\n",
    "# #     reasonable_n_foodstuffs = min(row['n_foodstuffs'], n_foodstuffs_max)\n",
    "#     reasonable_n_mushrooms = min(row['n_mushrooms'], mushrooms_cap)\n",
    "# #     reasonable_n_beans = min(row['n_beans'], n_beans_max)\n",
    "# #     reasonable_n_grains = min(row['n_grains'], n_grains_max)\n",
    "\n",
    "#     # what if I need to remove a mushroom, so I do that, but then I also need to remove a foodstuff so I double remove??\n",
    "#     # best not.\n",
    "# #     salts_to_remove_set = set(random.sample(row['salts'], row['n_salts'] - reasonable_n_salts))\n",
    "# #     fat_oils_to_remove_set = set(random.sample(row['fat_oils'], row['n_fat_oils'] - reasonable_n_fat_oils))\n",
    "# #     other_flavorings_to_remove_set = set(random.sample(row['other_flavorings'], row['n_other_flavorings'] - reasonable_n_other_flavorings))\n",
    "# #     foodstuffs_to_remove_set = set(random.sample(row['foodstuffs'], row['n_foodstuffs'] - reasonable_n_foodstuffs))\n",
    "#     mushrooms_to_remove_set = set(random.sample(row['mushrooms'], row['n_mushrooms'] - reasonable_n_mushrooms))\n",
    "# #     beans_to_remove_set = set(random.sample(row['beans'], row['n_beans'] - reasonable_n_beans))\n",
    "# #     grains_to_remove_set = set(random.sample(row['grains'], row['n_grains'] - reasonable_n_grains))\n",
    "    \n",
    "# #     print(\n",
    "# #         salts_to_remove_set,\n",
    "# #         fat_oils_to_remove_set,\n",
    "# #         other_flavorings_to_remove_set,\n",
    "# #         foodstuffs_to_remove_set,\n",
    "# #         mushrooms_to_remove_set,\n",
    "# #         beans_to_remove_set,\n",
    "# #         grains_to_remove_set\n",
    "# #     )\n",
    "    \n",
    "# #     reasonable_salts_set = row['salts_set'] - salts_to_remove_set\n",
    "# #     reasonable_fat_oils_set = row['fat_oils_set'] - fat_oils_to_remove_set\n",
    "# #     reasonable_other_flavorings_set = row['other_flavorings_set'] - other_flavorings_to_remove_set\n",
    "# #     reasonable_foodstuffs_set = row['foodstuffs_set'] - foodstuffs_to_remove_set\n",
    "#     reasonable_mushrooms_set = row['mushrooms_set'] - mushrooms_to_remove_set\n",
    "#     reasonable_foodstuffs_set = row['foodstuffs_set'] - mushrooms_to_remove_set\n",
    "# #     reasonable_beans_set = row['beans_set'] - beans_to_remove_set\n",
    "# #     reasonable_grains_set = row['grains_set'] - grains_to_remove_set\n",
    "    \n",
    "# #     reasonable_salts = list(reasonable_salts_set)\n",
    "# #     reasonable_fat_oils = list(reasonable_fat_oils_set)\n",
    "# #     reasonable_other_flavorings = list(reasonable_other_flavorings_set)\n",
    "#     reasonable_foodstuffs = list(reasonable_foodstuffs_set)\n",
    "#     reasonable_mushrooms = list(reasonable_mushrooms_set)\n",
    "# #     reasonable_beans = list(reasonable_beans_set)\n",
    "# #     reasonable_grains = list(reasonable_grains_set)\n",
    "    \n",
    "#     reasonable_set = row['set'].copy()\n",
    "# #     reasonable_set -= salts_to_remove_set\n",
    "# #     reasonable_set -= fat_oils_to_remove_set\n",
    "# #     reasonable_set -= other_flavorings_to_remove_set\n",
    "# #     reasonable_set -= foodstuffs_to_remove_set\n",
    "#     reasonable_set -= mushrooms_to_remove_set\n",
    "# #     reasonable_set -= beans_to_remove_set\n",
    "# #     reasonable_set -= grains_to_remove_set\n",
    "    \n",
    "#     reasonable_list = list(reasonable_set)\n",
    "#     reasonable_length = len(reasonable_list)\n",
    "\n",
    "#     return (\n",
    "# #         reasonable_n_salts,\n",
    "# #         reasonable_n_fat_oils,\n",
    "# #         reasonable_n_other_flavorings,\n",
    "# #         reasonable_n_foodstuffs,\n",
    "#         len(reasonable_foodstuffs),\n",
    "#         reasonable_n_mushrooms,\n",
    "# #         reasonable_n_beans,\n",
    "# #         reasonable_n_grains,\n",
    "# #         reasonable_salts_set,\n",
    "# #         reasonable_fat_oils_set,\n",
    "# #         reasonable_other_flavorings_set,\n",
    "#         reasonable_foodstuffs_set,\n",
    "#         reasonable_mushrooms_set,\n",
    "# #         reasonable_beans_set,\n",
    "# #         reasonable_grains_set,\n",
    "# #         reasonable_salts,\n",
    "# #         reasonable_fat_oils,\n",
    "# #         reasonable_other_flavorings,\n",
    "#         reasonable_foodstuffs,\n",
    "#         reasonable_mushrooms,\n",
    "# #         reasonable_beans,\n",
    "# #         reasonable_grains,\n",
    "#         reasonable_set,\n",
    "#         reasonable_list,\n",
    "#         reasonable_length\n",
    "#     )\n",
    "\n",
    "# reasonable_mushrooms_data = reasonable_clique_upper.apply(get_reasonable_mushrooms_data, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # gotta fiflter out less than 2's\n",
    "\n",
    "# # reasonable_clique_upper['reasonable_n_salts'] = reasonable_data.apply(lambda x: x[0])\n",
    "# # reasonable_clique_upper['reasonable_n_fat_oils'] = reasonable_data.apply(lambda x: x[1])\n",
    "# # reasonable_clique_upper['reasonable_n_other_flavorings'] = reasonable_data.apply(lambda x: x[2])\n",
    "# reasonable_clique_upper['reasonable_n_foodstuffs'] = reasonable_mushrooms_data.apply(lambda x: x[0])\n",
    "# reasonable_clique_upper['reasonable_n_mushrooms'] = reasonable_mushrooms_data.apply(lambda x: x[1])\n",
    "# # reasonable_clique_upper['reasonable_n_beans'] = reasonable_data.apply(lambda x: x[5])\n",
    "# # reasonable_clique_upper['reasonable_n_grains'] = reasonable_data.apply(lambda x: x[6])\n",
    "# # reasonable_clique_upper['reasonable_salts_set'] = reasonable_data.apply(lambda x: x[4])\n",
    "# # reasonable_clique_upper['reasonable_fat_oils_set'] = reasonable_data.apply(lambda x: x[5])\n",
    "# # reasonable_clique_upper['reasonable_other_flavorings_set'] = reasonable_data.apply(lambda x: x[6])\n",
    "# reasonable_clique_upper['reasonable_foodstuffs_set'] = reasonable_mushrooms_data.apply(lambda x: x[2])\n",
    "# reasonable_clique_upper['reasonable_mushrooms_set'] = reasonable_mushrooms_data.apply(lambda x: x[3])\n",
    "# # reasonable_clique_upper['reasonable_beans_set'] = reasonable_data.apply(lambda x: x[12])\n",
    "# # reasonable_clique_upper['reasonable_grains_set'] = reasonable_data.apply(lambda x: x[13])\n",
    "# # reasonable_clique_upper['reasonable_salts'] = reasonable_data.apply(lambda x: x[8])\n",
    "# # reasonable_clique_upper['reasonable_fat_oils'] = reasonable_data.apply(lambda x: x[9])\n",
    "# # reasonable_clique_upper['reasonable_other_flavorings'] = reasonable_data.apply(lambda x: x[10])\n",
    "# reasonable_clique_upper['reasonable_foodstuffs'] = reasonable_mushrooms_data.apply(lambda x: x[4])\n",
    "# reasonable_clique_upper['reasonable_mushrooms'] = reasonable_mushrooms_data.apply(lambda x: x[5])\n",
    "# # reasonable_clique_upper['reasonable_beans'] = reasonable_data.apply(lambda x: x[19])\n",
    "# # reasonable_clique_upper['reasonable_grains'] = reasonable_data.apply(lambda x: x[20])\n",
    "# reasonable_clique_upper['reasonable_set'] = reasonable_mushrooms_data.apply(lambda x: x[6])\n",
    "# reasonable_clique_upper['reasonable_list'] = reasonable_mushrooms_data.apply(lambda x: x[7])\n",
    "# reasonable_clique_upper['reasonable_length'] = reasonable_mushrooms_data.apply(lambda x: x[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.59 s, sys: 48 ms, total: 3.64 s\n",
      "Wall time: 3.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# len10k: 500ms\n",
    "# len1k: 50ms\n",
    "\n",
    "# doing this before get_connections_data, cause filtering out nodes would screw w that\n",
    "\n",
    "def get_reasonable_data(row):\n",
    "    # done first, since it messes w foodstuffs\n",
    "    reasonable_n_mushrooms = min(row['n_mushrooms'], mushrooms_cap)\n",
    "    mushrooms_to_remove_set = set(random.sample(row['mushrooms'], row['n_mushrooms'] - reasonable_n_mushrooms))\n",
    "    reasonable_mushrooms_set = row['mushrooms_set'] - mushrooms_to_remove_set\n",
    "    reasonable_mushrooms = list(reasonable_mushrooms_set)\n",
    "#     print(reasonable_n_mushrooms, mushrooms_to_remove_set, reasonable_mushrooms_set, row['mushrooms'])\n",
    "#     print(mushrooms_to_remove_set)\n",
    "    deshroomed_foodstuffs_set = row['foodstuffs_set'] - mushrooms_to_remove_set\n",
    "    deshroomed_foodstuffs = list(deshroomed_foodstuffs_set)\n",
    "    deshroomed_n_foodstuffs = len(deshroomed_foodstuffs)\n",
    "\n",
    "    deshroomed_set = row['set'].copy()\n",
    "    deshroomed_set -= mushrooms_to_remove_set\n",
    "    deshroomed_list = list(deshroomed_set)\n",
    "    deshroomed_length = len(deshroomed_list)\n",
    "    \n",
    "    reasonable_n_salts = min(row['n_salts'], n_salts)\n",
    "    reasonable_n_fat_oils = min(row['n_fat_oils'], n_fat_oils)\n",
    "    reasonable_n_other_flavorings = min(row['n_other_flavorings'], n_other_flavorings_max)\n",
    "    reasonable_n_foodstuffs = min(deshroomed_n_foodstuffs, n_foodstuffs_max)\n",
    "#     reasonable_n_mushrooms = min(row['n_mushrooms'], n_mushrooms_max)\n",
    "#     reasonable_n_beans = min(row['n_beans'], n_beans_max)\n",
    "#     reasonable_n_grains = min(row['n_grains'], n_grains_max)\n",
    "\n",
    "    # what if I need to remove a mushroom, so I do that, but then I also need to remove a foodstuff so I double remove??\n",
    "    # best not.\n",
    "    salts_to_remove_set = set(random.sample(row['salts'], row['n_salts'] - reasonable_n_salts))\n",
    "    fat_oils_to_remove_set = set(random.sample(row['fat_oils'], row['n_fat_oils'] - reasonable_n_fat_oils))\n",
    "    other_flavorings_to_remove_set = set(random.sample(row['other_flavorings'], row['n_other_flavorings'] - reasonable_n_other_flavorings))\n",
    "    foodstuffs_to_remove_set = set(random.sample(deshroomed_foodstuffs, deshroomed_n_foodstuffs - reasonable_n_foodstuffs))\n",
    "#     mushrooms_to_remove_set = set(random.sample(row['mushrooms'], row['n_mushrooms'] - reasonable_n_mushrooms))\n",
    "#     beans_to_remove_set = set(random.sample(row['beans'], row['n_beans'] - reasonable_n_beans))\n",
    "#     grains_to_remove_set = set(random.sample(row['grains'], row['n_grains'] - reasonable_n_grains))\n",
    "    \n",
    "#     print(\n",
    "#         salts_to_remove_set,\n",
    "#         fat_oils_to_remove_set,\n",
    "#         other_flavorings_to_remove_set,\n",
    "#         foodstuffs_to_remove_set,\n",
    "#         mushrooms_to_remove_set,\n",
    "#         beans_to_remove_set,\n",
    "#         grains_to_remove_set\n",
    "#     )\n",
    "    \n",
    "    reasonable_salts_set = row['salts_set'] - salts_to_remove_set\n",
    "    reasonable_fat_oils_set = row['fat_oils_set'] - fat_oils_to_remove_set\n",
    "    reasonable_other_flavorings_set = row['other_flavorings_set'] - other_flavorings_to_remove_set\n",
    "    reasonable_foodstuffs_set = deshroomed_foodstuffs_set - foodstuffs_to_remove_set\n",
    "#     reasonable_mushrooms_set = row['mushrooms_set'] - mushrooms_to_remove_set\n",
    "#     reasonable_beans_set = row['beans_set'] - beans_to_remove_set\n",
    "#     reasonable_grains_set = row['grains_set'] - grains_to_remove_set\n",
    "    \n",
    "    reasonable_salts = list(reasonable_salts_set)\n",
    "    reasonable_fat_oils = list(reasonable_fat_oils_set)\n",
    "    reasonable_other_flavorings = list(reasonable_other_flavorings_set)\n",
    "    reasonable_foodstuffs = list(reasonable_foodstuffs_set)\n",
    "#     reasonable_mushrooms = list(reasonable_mushrooms_set)\n",
    "#     reasonable_beans = list(reasonable_beans_set)\n",
    "#     reasonable_grains = list(reasonable_grains_set)\n",
    "    \n",
    "#     reasonable_set = row['set'].copy()\n",
    "    reasonable_set = deshroomed_set # omitting the copy here (for negligible speed)\n",
    "    reasonable_set -= salts_to_remove_set\n",
    "    reasonable_set -= fat_oils_to_remove_set\n",
    "    reasonable_set -= other_flavorings_to_remove_set\n",
    "    reasonable_set -= foodstuffs_to_remove_set\n",
    "#     reasonable_set -= mushrooms_to_remove_set\n",
    "#     reasonable_set -= beans_to_remove_set\n",
    "#     reasonable_set -= grains_to_remove_set\n",
    "    \n",
    "    reasonable_list = list(reasonable_set)\n",
    "    reasonable_length = len(reasonable_list)\n",
    "\n",
    "    return (\n",
    "        reasonable_n_salts,\n",
    "        reasonable_n_fat_oils,\n",
    "        reasonable_n_other_flavorings,\n",
    "        reasonable_n_foodstuffs,\n",
    "#         reasonable_n_mushrooms,\n",
    "#         reasonable_n_beans,\n",
    "#         reasonable_n_grains,\n",
    "        reasonable_salts_set,\n",
    "        reasonable_fat_oils_set,\n",
    "        reasonable_other_flavorings_set,\n",
    "        reasonable_foodstuffs_set,\n",
    "#         reasonable_mushrooms_set,\n",
    "#         reasonable_beans_set,\n",
    "#         reasonable_grains_set,\n",
    "        reasonable_salts,\n",
    "        reasonable_fat_oils,\n",
    "        reasonable_other_flavorings,\n",
    "        reasonable_foodstuffs,\n",
    "#         reasonable_mushrooms,\n",
    "#         reasonable_beans,\n",
    "#         reasonable_grains,\n",
    "        reasonable_set,\n",
    "        reasonable_list,\n",
    "        reasonable_length\n",
    "    )\n",
    "\n",
    "reasonable_data = reasonable_clique_upper.apply(get_reasonable_data, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 123 ms, sys: 0 ns, total: 123 ms\n",
      "Wall time: 123 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# gotta filter out less than 2's\n",
    "\n",
    "# reasonable_clique_upper = clique_upper.copy()\n",
    "\n",
    "reasonable_clique_upper['reasonable_n_salts'] = reasonable_data.apply(lambda x: x[0])\n",
    "reasonable_clique_upper['reasonable_n_fat_oils'] = reasonable_data.apply(lambda x: x[1])\n",
    "reasonable_clique_upper['reasonable_n_other_flavorings'] = reasonable_data.apply(lambda x: x[2])\n",
    "reasonable_clique_upper['reasonable_n_foodstuffs'] = reasonable_data.apply(lambda x: x[3])\n",
    "# reasonable_clique_upper['reasonable_n_mushrooms'] = reasonable_data.apply(lambda x: x[4])\n",
    "# reasonable_clique_upper['reasonable_n_beans'] = reasonable_data.apply(lambda x: x[5])\n",
    "# reasonable_clique_upper['reasonable_n_grains'] = reasonable_data.apply(lambda x: x[6])\n",
    "reasonable_clique_upper['reasonable_salts_set'] = reasonable_data.apply(lambda x: x[4])\n",
    "reasonable_clique_upper['reasonable_fat_oils_set'] = reasonable_data.apply(lambda x: x[5])\n",
    "reasonable_clique_upper['reasonable_other_flavorings_set'] = reasonable_data.apply(lambda x: x[6])\n",
    "reasonable_clique_upper['reasonable_foodstuffs_set'] = reasonable_data.apply(lambda x: x[7])\n",
    "# reasonable_clique_upper['reasonable_mushrooms_set'] = reasonable_data.apply(lambda x: x[11])\n",
    "# reasonable_clique_upper['reasonable_beans_set'] = reasonable_data.apply(lambda x: x[12])\n",
    "# reasonable_clique_upper['reasonable_grains_set'] = reasonable_data.apply(lambda x: x[13])\n",
    "reasonable_clique_upper['reasonable_salts'] = reasonable_data.apply(lambda x: x[8])\n",
    "reasonable_clique_upper['reasonable_fat_oils'] = reasonable_data.apply(lambda x: x[9])\n",
    "reasonable_clique_upper['reasonable_other_flavorings'] = reasonable_data.apply(lambda x: x[10])\n",
    "reasonable_clique_upper['reasonable_foodstuffs'] = reasonable_data.apply(lambda x: x[11])\n",
    "# reasonable_clique_upper['reasonable_mushrooms'] = reasonable_data.apply(lambda x: x[18])\n",
    "# reasonable_clique_upper['reasonable_beans'] = reasonable_data.apply(lambda x: x[19])\n",
    "# reasonable_clique_upper['reasonable_grains'] = reasonable_data.apply(lambda x: x[20])\n",
    "reasonable_clique_upper['reasonable_set'] = reasonable_data.apply(lambda x: x[12])\n",
    "reasonable_clique_upper['reasonable_list'] = reasonable_data.apply(lambda x: x[13])\n",
    "reasonable_clique_upper['reasonable_length'] = reasonable_data.apply(lambda x: x[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT don't remove:\n",
    "\n",
    "reasonable_clique_upper = reasonable_clique_upper[reasonable_clique_upper['reasonable_list'].apply(lambda x: len(x) >= 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.24 s, sys: 16 ms, total: 3.26 s\n",
      "Wall time: 3.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 2m 47s\n",
    "# fuuuuck not working (oh that's cause I blew connections up)\n",
    "# anyway, with upper it's only 8s\n",
    "  \n",
    "G = nx.Graph()\n",
    "def get_connections_data(clique_list):\n",
    "    G.clear()\n",
    "    G.add_nodes_from(clique_list)\n",
    "    weighted_edges = []\n",
    "    connections = []\n",
    "#     print(clique_list)\n",
    "    for i_1, name_1 in enumerate(clique_list[:-1]):\n",
    "        for i_2, name_2 in enumerate(clique_list[i_1+1:], i_1+1):\n",
    "            connection = stir_fry_flavor_data[name_1][name_2]\n",
    "    \n",
    "            # Really, since I'm using upper the only demerit values here will be .5333 and .4. but, I think I've adjusted variation to 0-1 anyway, so it should be ok (?)\n",
    "            if connection[0] == 'c':\n",
    "                pairs_with_demerit = .8\n",
    "            elif connection[0] == 'd':\n",
    "                pairs_with_demerit = .6666\n",
    "            elif connection[0] == 'C':\n",
    "                pairs_with_demerit = .5333\n",
    "            elif connection[0] == 'D':\n",
    "                pairs_with_demerit = .4\n",
    "            else:\n",
    "                print('OH NO! BAD PAIRING VALUE.')\n",
    "\n",
    "            if connection[1] == '_':\n",
    "                strength_demerit = .2\n",
    "            elif connection[1] == 's':\n",
    "                strength_demerit = .15\n",
    "            elif connection[1] == 'S':\n",
    "                strength_demerit = .1\n",
    "            else:\n",
    "                print('OH NO! BAD STRENGTH VALUE.')\n",
    "                \n",
    "            connection_length = pairs_with_demerit + strength_demerit\n",
    "            weighted_edges.append((name_1, name_2, connection_length))\n",
    "            connections.append((name_1, name_2, connection))\n",
    "            \n",
    "    G.add_weighted_edges_from(weighted_edges)\n",
    "    path_length = nx.average_shortest_path_length(G, weight='weight')\n",
    "    return (path_length, connections)\n",
    "\n",
    "connections_data = reasonable_clique_upper['reasonable_list'].apply(get_connections_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasonable_clique_upper['path_length'] = connections_data.apply(lambda x: x[0])\n",
    "reasonable_clique_upper['connections'] = connections_data.apply(lambda x: x[1])\n",
    "reasonable_clique_upper['connection_values'] = connections_data.apply(lambda x: [c[2] for c in x[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install networkx==2.4\n",
    "# !pip install pyvis\n",
    "# !pip install python-louvain\n",
    "# !pip install matplotlib\n",
    "import networkx as nx\n",
    "from pyvis import network as net\n",
    "import community\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.26 s, sys: 40 ms, total: 1.3 s\n",
      "Wall time: 1.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# THIS IS A CRUCIAL THING, PROLLY NEEDS FINE TUNING\n",
    "# for example, I'm still getting some high-scored len-3's. good 3's, but stilll..\n",
    "# ANOTHER OPTION\n",
    "    # add filler data for remaining nodes, then score like I would in generator\n",
    "\n",
    "fruit_set = set(stir_fry_flavor_data[stir_fry_flavor_data['stir_fry_fruit'] == 'y']['name'])\n",
    "protein_set = set(stir_fry_flavor_data[stir_fry_flavor_data['stir_fry_protein'] == 'y']['name'])\n",
    "bean_set = set(stir_fry_flavor_data[stir_fry_flavor_data['stir_fry_protein_bean'] == 'y']['name'])\n",
    "grain_set = set(stir_fry_flavor_data[stir_fry_flavor_data['stir_fry_grain'] == 'y']['name'])\n",
    "    \n",
    "max_length_upper = max(reasonable_clique_upper['reasonable_length'].tolist())\n",
    "\n",
    "def get_score(row):\n",
    "    \n",
    "    # NODE LENGTH BONUS\n",
    "    # scale lengths so that max is 10, then square to skew sampling toward long cliques, then scale to 1 max\n",
    "    nodes_length_score = (row['reasonable_length'] * 10/max_length_upper)**2 / 100\n",
    "\n",
    "    # now that everything is upper, this is just selecting for ud and strong. so, I'm going to prioritize length for now.\n",
    "    \n",
    "    # FOOD GROUPS BONUS\n",
    "    n_fruit = len(row['reasonable_set'].intersection(fruit_set))\n",
    "    n_protein = len(row['reasonable_set'].intersection(protein_set))\n",
    "\n",
    "#     0, .7 or 1\n",
    "    protein_score = (n_protein/2)**.5\n",
    "\n",
    "    # very few, but also 0, .7, 1\n",
    "    fruit_score = (n_fruit/2)**.5\n",
    "    \n",
    "    food_group_score = protein_score + fruit_score\n",
    "    \n",
    "    # PATH LENGTH BONUS\n",
    "    path_length_score = 2.3/row['path_length'] - 3.1\n",
    "    \n",
    "    # RICE BEANS DEMERIT\n",
    "    # I'm thinking giving a fat demerit to cliques with too many rice, beans will be faster than what I did with mushrooms\n",
    "    # Jeez tho, to have both is sooo kludgeeey\n",
    "    n_beans = len(row['reasonable_set'].intersection(bean_set))\n",
    "    n_grains = len(row['reasonable_set'].intersection(grain_set))\n",
    "\n",
    "    # trying to keep this low, to tack onto 0-1 score\n",
    "    rice_beans_demerit = 0\n",
    "    if n_beans == 2:\n",
    "        rice_beans_demerit += .1\n",
    "    elif n_beans > 2:\n",
    "        rice_beans_demerit += .25\n",
    "    if n_grains == 2:\n",
    "        rice_beans_demerit += .1\n",
    "    elif n_grains > 2:\n",
    "        rice_beans_demerit += .25\n",
    "        \n",
    "    score = (nodes_length_score*1.6 + path_length_score*1 + food_group_score*.4)/3 - rice_beans_demerit\n",
    "#     score = path_length_score\n",
    "#     print(score, row['path_length'], row['length'])\n",
    "#     print(round(score, 2), round(nodes_length_score, 2), round(food_group_score, 2), round(path_length_score, 2))\n",
    "    if score > 0:\n",
    "        return score\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "reasonable_clique_upper['score'] = reasonable_clique_upper.apply(get_score, axis=1)\n",
    "reasonable_clique_upper.sort_values('score', ascending=False, inplace=True) #IMPORTANT\n",
    "reasonable_clique_upper.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reasonable_clique_upper.to_pickle(os.path.join(root_path, 'DATA/stir_fry_reasonable_clique_upper.pickle'))\n",
    "# reasonable_clique_upper.to_pickle(os.path.join(root_path, '../data/stir_fry_reasonable_clique_upper.pickle'))\n",
    "# reasonable_clique_upper = pd.read_pickle(os.path.join(root_path, 'DATA/stir_fry_reasonable_clique_upper.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.57 ms, sys: 0 ns, total: 2.57 ms\n",
      "Wall time: 2.08 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "average_score_for_length = dict(reasonable_clique_upper.groupby('reasonable_length')['score'].mean())\n",
    "def get_average_score(length):\n",
    "    if length >= 2:\n",
    "        return average_score_for_length[length]\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10/15/2020: this is so I can select weak rando ingredients, that won't clash with clique.\n",
    "# Should probably be fine-tuned at some pt, this is just top of my head\n",
    "\n",
    "def get_weak_score(strong):\n",
    "    if strong == 'Y':\n",
    "        return 1\n",
    "    elif strong == 'y':\n",
    "        return 3\n",
    "    else:\n",
    "        return 9\n",
    "                        \n",
    "stir_fry_flavor_data['weak_score'] = stir_fry_flavor_data['strong'].apply(get_weak_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 374 ms, sys: 8 ms, total: 382 ms\n",
      "Wall time: 381 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 10/15/2020: this is creating a graph, weighted by how \"good\" pairings are?\n",
    "\n",
    "# ugggggh 4.54s\n",
    "# guess I can't rly do this for all..\n",
    "stir_fry_names = stir_fry_flavor_data['name']\n",
    "\n",
    "stir_fry_g = nx.Graph()\n",
    "stir_fry_g.add_nodes_from(stir_fry_names)\n",
    "weighted_edges = []\n",
    "for i_1, name_1 in enumerate(stir_fry_names[:-1]):\n",
    "    for i_2, name_2 in enumerate(stir_fry_names[i_1+1:], i_1+1):\n",
    "        connection = stir_fry_flavor_data[name_1][name_2]\n",
    "\n",
    "        # Really, since I'm using upper the only demerit values here will be .5333 and .4. but, I think I've adjusted variation to 0-1 anyway, so it should be ok (?)\n",
    "        if connection[0] == 'c':\n",
    "            pairs_with_demerit = .8\n",
    "        elif connection[0] == 'd':\n",
    "            pairs_with_demerit = .6666\n",
    "        elif connection[0] == 'C':\n",
    "            pairs_with_demerit = .5333\n",
    "        elif connection[0] == 'D':\n",
    "            pairs_with_demerit = .4\n",
    "\n",
    "        if connection[1] == '_':\n",
    "            strength_demerit = .2\n",
    "        elif connection[1] == 's':\n",
    "            strength_demerit = .15\n",
    "        elif connection[1] == 'S':\n",
    "            strength_demerit = .1\n",
    "\n",
    "        connection_length = pairs_with_demerit + strength_demerit\n",
    "        weighted_edges.append((name_1, name_2, connection_length))\n",
    "\n",
    "stir_fry_g.add_weighted_edges_from(weighted_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7 µs, sys: 0 ns, total: 7 µs\n",
      "Wall time: 9.78 µs\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# # 4.8s, yikes.. good thing I can do it beforehand\n",
    "\n",
    "# stir_fry_shortest_path_lengths = nx.shortest_path_length(stir_fry_g, weight='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.18 s, sys: 0 ns, total: 4.18 s\n",
      "Wall time: 4.18 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# # pretty fast, 6ms\n",
    "# present_shortest_path_lengths = {outer_key: {inner_key: inner_item for inner_key, inner_item in outer_item.items() if inner_key in present_set}  for outer_key, outer_item in stir_fry_shortest_path_lengths if outer_key in present_set}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.15 s, sys: 3.75 ms, total: 4.16 s\n",
      "Wall time: 4.16 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# # 4.15s\n",
    "# # no idea why above stopped working. did I change it by accident?\n",
    "\n",
    "# stir_fry_shortest_path_lengths = nx.shortest_path_length(stir_fry_g, weight='weight')\n",
    "# present_shortest_path_lengths = {outer_key: {inner_key: inner_item for inner_key, inner_item in outer_item.items() if inner_key in present_set}  for outer_key, outer_item in stir_fry_shortest_path_lengths if outer_key in present_set}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.85 s, sys: 12 ms, total: 3.86 s\n",
      "Wall time: 3.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 3.98s\n",
    "# I do not understand what is happening. It has been a while. Things have changed?\n",
    "\n",
    "stir_fry_shortest_path_lengths = {key: value for key, value in nx.shortest_path_length(stir_fry_g, weight='weight')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'to_pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-a2dd0a382c32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstir_fry_shortest_path_lengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DATA/stir_fry_shortest_path_lengths.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# stir_fry_shortest_path_lengths = pd.read_pickle(os.path.join(root_path, 'DATA/stir_fry_shortest_path_lengths.pickle'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'to_pickle'"
     ]
    }
   ],
   "source": [
    "# stir_fry_shortest_path_lengths.to_pickle(os.path.join(root_path, 'DATA/stir_fry_shortest_path_lengths.pickle'))\n",
    "# stir_fry_shortest_path_lengths = pd.read_pickle(os.path.join(root_path, 'DATA/stir_fry_shortest_path_lengths.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# import pickle\n",
    "# with open(os.path.join(root_path, 'DATA/stir_fry_shortest_path_lengths.pickle'), 'wb') as handle:\n",
    "#     pickle.dump(stir_fry_shortest_path_lengths, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open(os.path.join(root_path, '../data/stir_fry_shortest_path_lengths.pickle'), 'wb') as handle:\n",
    "#     pickle.dump(stir_fry_shortest_path_lengths, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open(os.path.join(root_path, 'DATA/stir_fry_shortest_path_lengths.pickle'), 'rb') as handle:\n",
    "#     stir_fry_shortest_path_lengths = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.2. Generator set-up\n",
    "(stuff to do once/call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26 ms, sys: 3.87 ms, total: 29.9 ms\n",
      "Wall time: 29.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 30ms, could be a lil faster maybe\n",
    "\n",
    "# TODO: go through these, comment out unused\n",
    "\n",
    "# gonna start off assuming all are present\n",
    "# also gonna start off w no locked\n",
    "# n_present = random.randrange(len(stir_fry_flavor_data)/2, len(stir_fry_flavor_data)+1)\n",
    "n_present = len(stir_fry_flavor_data) - 10\n",
    "present = stir_fry_flavor_data.sample(n_present)\n",
    "present_set = set(present['name'])\n",
    "present_strong_set = set(present[present['strong'].isin(['Y', 'y'])]['name'])\n",
    "# not_present = stir_fry_flavor_data[~stir_fry_flavor_data['name'].isin(present['name'])]\n",
    "not_present_set = set(stir_fry_flavor_data['name']) - present_set\n",
    "# not_present_set = set(not_present)\n",
    "# present_shortest_path_lengths = {outer_key: {inner_key: inner_item for inner_key, inner_item in outer_item.items() if inner_key in present_set}  for outer_key, outer_item in stir_fry_shortest_path_lengths.items() if outer_key in present_set}\n",
    "present_shortest_path_lengths = {key: stir_fry_shortest_path_lengths[key] for key in stir_fry_shortest_path_lengths if key in present_set}\n",
    "\n",
    "# n_locked = random.randrange(0, 6)\n",
    "n_locked = 3\n",
    "locked = present.sample(n_locked) # random for now, but will be handed down\n",
    "locked_set = set(locked['name'])\n",
    "# locked_set_strong = set(locked[locked['strong'].isin(['Y', 'y'])]['name'])\n",
    "locked_fat_oils = locked[locked['stir_fry_fat_oil'] == 'y']\n",
    "locked_salts = locked[locked['stir_fry_salt'] == 'y']\n",
    "locked_other_flavorings = locked[(locked['stir_fry_flavoring'] == 'y') & (locked['stir_fry_salt'] != 'y')]\n",
    "locked_foodstuffs = locked[(locked['stir_fry_fat_oil'] != 'y') & (locked['stir_fry_salt'] != 'y') & (locked['stir_fry_flavoring'] != 'y')] # why am I selecting not salt? already selecting not flavoring. don't wanna mess tho..\n",
    "locked_fat_oils_set = set(locked_fat_oils['name'])\n",
    "locked_salts_set = set(locked_salts['name'])\n",
    "locked_other_flavorings_set = set(locked_other_flavorings['name'])\n",
    "locked_foodstuffs_set = set(locked_foodstuffs['name'])\n",
    "\n",
    "# locked_mushrooms = locked[locked['stir_fry_mushroom'] == 'y']\n",
    "# locked_beans = locked[locked['stir_fry_grain'] == 'y']\n",
    "# locked_grains = locked[locked['stir_fry_protein_bean'] == 'y']\n",
    "\n",
    "the_rest = present[~present['name'].isin(locked['name'])]\n",
    "the_rest_set = set(the_rest['name'])\n",
    "the_rest_fat_oils = the_rest[the_rest['stir_fry_fat_oil'] == 'y']\n",
    "the_rest_salts = the_rest[the_rest['stir_fry_salt'] == 'y']\n",
    "the_rest_other_flavorings = the_rest[(the_rest['stir_fry_flavoring'] == 'y') & (the_rest['stir_fry_salt'] != 'y')]\n",
    "the_rest_foodstuffs = the_rest[(the_rest['stir_fry_fat_oil'] != 'y') & (the_rest['stir_fry_salt'] != 'y') & (the_rest['stir_fry_flavoring'] != 'y')]\n",
    "the_rest_fat_oils_set = set(the_rest_fat_oils['name'])\n",
    "the_rest_salts_set = set(the_rest_salts['name'])\n",
    "the_rest_other_flavorings_set = set(the_rest_other_flavorings['name'])\n",
    "the_rest_foodstuffs_set = set(the_rest_foodstuffs['name'])\n",
    "\n",
    "the_rest_weak = present[~present['strong'].isin(['y', 'Y'])]\n",
    "the_rest_weak_set = set(the_rest_weak['name'])\n",
    "the_rest_weak_fat_oils = the_rest_weak[the_rest_weak['stir_fry_fat_oil'] == 'y']\n",
    "the_rest_weak_salts = the_rest_weak[the_rest_weak['stir_fry_salt'] == 'y']\n",
    "the_rest_weak_other_flavorings = the_rest_weak[(the_rest_weak['stir_fry_flavoring'] == 'y') & (the_rest_weak['stir_fry_salt'] != 'y')]\n",
    "the_rest_weak_foodstuffs = the_rest_weak[(the_rest_weak['stir_fry_fat_oil'] != 'y') & (the_rest_weak['stir_fry_salt'] != 'y') & (the_rest_weak['stir_fry_flavoring'] != 'y')]\n",
    "\n",
    "n_gen_salts = max(n_salts - len(locked_salts), 0)\n",
    "n_gen_fat_oils = max(n_fat_oils - len(locked_fat_oils), 0)\n",
    "n_gen_other_flavorings_min = max(n_other_flavorings_min - len(locked_other_flavorings), 0)\n",
    "n_gen_other_flavorings_max = max(n_other_flavorings_max - len(locked_other_flavorings), 0) # yikes, I had this as min..\n",
    "n_gen_foodstuffs_min = max(n_foodstuffs_min - len(locked_foodstuffs), 0)\n",
    "n_gen_foodstuffs_max = max(n_foodstuffs_max - len(locked_foodstuffs), 0)\n",
    "\n",
    "n_salts_actual = max(n_salts, len(locked_salts))\n",
    "n_fat_oils_actual = max(n_fat_oils, len(locked_fat_oils))\n",
    "\n",
    "# not totally sure thesse are right (that both min and max are compared to locked)\n",
    "n_other_flavorings_min_actual = max(n_other_flavorings_min, len(locked_other_flavorings))\n",
    "n_other_flavorings_max_actual = max(n_other_flavorings_max, len(locked_other_flavorings))\n",
    "n_foodstuffs_min_actual = max(n_foodstuffs_min, len(locked_foodstuffs))\n",
    "n_foodstuffs_max_actual = max(n_foodstuffs_max, len(locked_foodstuffs))\n",
    "\n",
    "# n_gen_mushrooms_min = max(n_mushrooms_min - len(locked_mushrooms), 0)\n",
    "# n_gen_mushrooms_max = max(n_mushrooms_max - len(locked_foodstuffs), 0)\n",
    "# n_gen_beans_min = max(n_beans_min - len(locked_beans), 0)\n",
    "# n_gen_beans_max = max(n_beans_max - len(locked_foodstuffs), 0)\n",
    "# n_gen_grains_min = max(n_grains_min - len(locked_grains), 0)\n",
    "# n_gen_grains_max = max(n_grains_max - len(locked_foodstuffs), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.61 ms, sys: 40 µs, total: 3.65 ms\n",
      "Wall time: 3.12 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# 63ms, without slicing\n",
    "# 15.5ms 1000\n",
    "\n",
    "# IMPORTANT don't remove\n",
    "\n",
    "ok_cliques = reasonable_clique_upper[:1000].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 152 ms, sys: 0 ns, total: 152 ms\n",
      "Wall time: 152 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 1.5s, without slicing\n",
    "#150ms, with slicing\n",
    "#180ms, with the shit I am now pulling T.T\n",
    "\n",
    "# A note on mushrooms:\n",
    "    # as is it is possible for locked mushrooms to result in more mushrooms than mushroom_cap.\n",
    "    # easy enough to update the sets below with fewer mushrooms,\n",
    "    # but idk if its worth the trouble.\n",
    "    \n",
    "# FUCK, just did a lot of changes (accounting for locked in clique), but rly don't wanna go back check work rn..\n",
    "    # so tireddd\n",
    "    \n",
    "def get_ok_data(row):\n",
    "    updated_set = row['reasonable_set'] - not_present_set\n",
    "    updated_salts_set = row['reasonable_salts_set'] - not_present_set\n",
    "    updated_fat_oils_set = row['reasonable_fat_oils_set'] - not_present_set\n",
    "    updated_other_flavorings_set = row['reasonable_other_flavorings_set'] - not_present_set\n",
    "    updated_foodstuffs_set = row['reasonable_foodstuffs_set'] - not_present_set\n",
    "#     updated_mushrooms_set = row['reasonable_mushrooms_set'] - not_present_set\n",
    "#     updated_beans_set = row['reasonable_beans_set'] - not_present_set\n",
    "#     updated_grains_set = row['reasonable_grains_set'] - not_present_set\n",
    "    \n",
    "    updated_n_salts = len(updated_salts_set)\n",
    "    updated_n_fat_oils = len(updated_fat_oils_set)\n",
    "    updated_n_other_flavorings = len(updated_other_flavorings_set)\n",
    "    updated_n_foodstuffs = len(updated_foodstuffs_set)\n",
    "#     updated_n_mushrooms = len(updated_mushrooms_set)\n",
    "#     updated_n_beans = len(updated_beans_set)\n",
    "#     updated_n_grains = len(updated_grains_set)\n",
    "\n",
    "#     ok_n_salts = min(updated_n_salts, n_gen_salts)\n",
    "#     ok_n_fat_oils = min(updated_n_fat_oils, n_gen_fat_oils)\n",
    "#     ok_n_other_flavorings = min(updated_n_other_flavorings, n_gen_other_flavorings_max)\n",
    "#     ok_n_foodstuffs = min(updated_n_foodstuffs, n_gen_foodstuffs_max)\n",
    "#     ok_n_mushrooms = min(updated_n_mushrooms, n_gen_mushrooms_max)\n",
    "#     ok_n_beans = min(updated_n_beans, n_gen_beans_max)\n",
    "#     ok_n_grains = min(updated_n_grains, n_gen_grains_max)\n",
    "\n",
    "    # lessee, to do this right I have to be counting locked + this sor sosmething :-PPPPPP bleh\n",
    "    # the question is, how many salts do I need to take away from this clique\n",
    "        # let's say there is 1 locked salt\n",
    "            # if that locked salt is in clique, none, then\n",
    "            # if that locked salt ISN\"T in clique, another is, take that\n",
    "            # if that locked salt AND another is in, take the other\n",
    "    # so the union of set of salts in clique and locked salts cannot be more than n_salts_actual (which is at least as big as locked salts)\n",
    "\n",
    "    n_salts_so_far = len(updated_salts_set.union(locked_salts_set))\n",
    "    n_salts_to_remove = max(n_salts_so_far - n_salts_actual, 0)\n",
    "    salts_to_remove_from_set = updated_salts_set - locked_salts_set\n",
    "    more_salts_to_remove_set = set(random.sample(salts_to_remove_from_set, n_salts_to_remove))\n",
    "    \n",
    "    n_fat_oils_so_far = len(updated_fat_oils_set.union(locked_fat_oils_set))\n",
    "    n_fat_oils_to_remove = max(n_fat_oils_so_far - n_fat_oils_actual, 0)\n",
    "    fat_oils_to_remove_from_set = updated_fat_oils_set - locked_fat_oils_set\n",
    "    more_fat_oils_to_remove_set = set(random.sample(fat_oils_to_remove_from_set, n_fat_oils_to_remove))\n",
    "\n",
    "    n_other_flavorings_so_far = len(updated_other_flavorings_set.union(locked_other_flavorings_set))\n",
    "    n_other_flavorings_to_remove = max(n_other_flavorings_so_far - n_other_flavorings_max_actual, 0) # is this right?\n",
    "    other_flavorings_to_remove_from_set = updated_other_flavorings_set - locked_other_flavorings_set\n",
    "    more_other_flavorings_to_remove_set = set(random.sample(salts_to_remove_from_set, n_salts_to_remove))\n",
    "\n",
    "    n_foodstuffs_so_far = len(updated_foodstuffs_set.union(locked_foodstuffs_set))\n",
    "    n_foodstuffs_to_remove = max(n_foodstuffs_so_far - n_foodstuffs_max_actual, 0) # is this right? thinks so - it's keeping foodstuffs from above max\n",
    "    foodstuffs_to_remove_from_set = updated_foodstuffs_set - locked_foodstuffs_set\n",
    "    more_foodstuffs_to_remove_set = set(random.sample(foodstuffs_to_remove_from_set, n_foodstuffs_to_remove))\n",
    "\n",
    "    \n",
    "    \n",
    "#     more_salts_to_remove_set = set(random.sample(updated_salts_set, updated_n_salts - ok_n_salts))\n",
    "#     more_fat_oils_to_remove_set = set(random.sample(updated_fat_oils_set, updated_n_fat_oils - ok_n_fat_oils))\n",
    "#     more_other_flavorings_to_remove_set = set(random.sample(updated_other_flavorings_set, updated_n_other_flavorings - ok_n_other_flavorings))\n",
    "#     more_foodstuffs_to_remove_set = set(random.sample(updated_foodstuffs_set, updated_n_foodstuffs - ok_n_foodstuffs))\n",
    "#     more_mushrooms_to_remove_set = set(random.sample(updated_mushrooms_set, updated_n_mushrooms - ok_n_mushrooms))\n",
    "#     more_beans_to_remove_set = set(random.sample(updated_beans_set, updated_n_beans - ok_n_beans))\n",
    "#     more_grains_to_remove_set = set(random.sample(updated_grains_set, updated_n_grains - ok_n_grains))\n",
    "    \n",
    "#     ok_salts_set = updated_salts_set - more_salts_to_remove_set\n",
    "#     ok_fat_oils_set = updated_fat_oils_set - more_fat_oils_to_remove_set\n",
    "    ok_other_flavorings_set = updated_other_flavorings_set - more_other_flavorings_to_remove_set\n",
    "    ok_foodstuffs_set = updated_foodstuffs_set - more_foodstuffs_to_remove_set\n",
    "#     ok_mushrooms_set = updated_mushrooms_set - more_mushrooms_to_remove_set\n",
    "#     ok_beans_set = updated_beans_set - more_beans_to_remove_set\n",
    "#     ok_grains_set = updated_grains_set - more_grains_to_remove_set\n",
    "    \n",
    "    ok_set = updated_set # not bothering w copy, for (negligible) speed reasons\n",
    "    ok_set -= more_salts_to_remove_set\n",
    "    ok_set -= more_fat_oils_to_remove_set\n",
    "    ok_set -= more_other_flavorings_to_remove_set\n",
    "    ok_set -= more_foodstuffs_to_remove_set\n",
    "#     ok_set -= more_mushrooms_to_remove_set\n",
    "#     ok_set -= more_beans_to_remove_set\n",
    "#     ok_set -= more_grains_to_remove_set\n",
    "    \n",
    "    ok_list = list(ok_set)\n",
    "    ok_length = len(ok_list)\n",
    "    ok_n_locked = len(ok_set.intersection(locked_set))\n",
    "    ok_n_other_flavorings = len(ok_other_flavorings_set)\n",
    "    ok_n_foodstuffs = len(ok_foodstuffs_set)\n",
    "    \n",
    "    old_score_bonus_factor = row['score']/get_average_score(row['reasonable_length']) # how above/below avg score was previously\n",
    "    ok_score = get_average_score(ok_length)*old_score_bonus_factor\n",
    "    ok_score_xtreme = ok_score**2 # skew scores way upward, for use as weights\n",
    "\n",
    "    # damn but this is kludgily arbitrary\n",
    "    ok_score_lockified = ok_score + ok_n_locked\n",
    "    ok_score_lockified_xtreme = ok_score_lockified**2\n",
    "    \n",
    "    ok_strong_set = ok_set.intersection(present_strong_set)\n",
    "    \n",
    "    #\n",
    "    return (\n",
    "        ok_set,\n",
    "        ok_list,\n",
    "        ok_length,\n",
    "        ok_score,\n",
    "        ok_score_xtreme,\n",
    "#         ok_n_salts,\n",
    "#         ok_n_fat_oils,\n",
    "        ok_n_locked,\n",
    "        ok_score_lockified,\n",
    "        ok_score_lockified_xtreme,\n",
    "        ok_strong_set,\n",
    "#         ok_n_mushrooms,\n",
    "#         ok_n_beans,\n",
    "#         ok_n_grains\n",
    "        ok_n_other_flavorings,\n",
    "        ok_n_foodstuffs,\n",
    "    )\n",
    "    \n",
    "ok_data = ok_cliques.apply(get_ok_data, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.9 ms, sys: 0 ns, total: 15.9 ms\n",
      "Wall time: 15.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ok_cliques['ok_set'] = ok_data.apply(lambda x: x[0])\n",
    "ok_cliques['ok_list'] = ok_data.apply(lambda x: x[1])\n",
    "ok_cliques['ok_length'] = ok_data.apply(lambda x: x[2])\n",
    "ok_cliques['ok_score'] = ok_data.apply(lambda x: x[3])\n",
    "ok_cliques['ok_score_xtreme'] = ok_data.apply(lambda x: x[4])\n",
    "# ok_cliques['ok_n_salts'] = ok_data.apply(lambda x: x[5])\n",
    "# ok_cliques['ok_n_fat_oils'] = ok_data.apply(lambda x: x[6])\n",
    "ok_cliques['ok_n_locked'] = ok_data.apply(lambda x: x[5])\n",
    "ok_cliques['ok_score_lockified'] = ok_data.apply(lambda x: x[6])\n",
    "ok_cliques['ok_score_lockified_xtreme'] = ok_data.apply(lambda x: x[7])\n",
    "ok_cliques['ok_strong_set'] = ok_data.apply(lambda x: x[8])\n",
    "# ok_cliques['ok_n_mushrooms'] = ok_data.apply(lambda x: x[9])\n",
    "# ok_cliques['ok_n_beans'] = ok_data.apply(lambda x: x[10])\n",
    "# ok_cliques['ok_n_grains'] = ok_data.apply(lambda x: x[11])\n",
    "ok_cliques['ok_n_other_flavorings'] = ok_data.apply(lambda x: x[9])\n",
    "ok_cliques['ok_n_foodstuffs'] = ok_data.apply(lambda x: x[10])\n",
    "\n",
    "# UPDATE: don't think this is actually necessary, since I'll just be sampling going fwd? maybe not, idk\n",
    "# ok_cliques.sort_values('ok_score', ascending=False, inplace=True) # this is smart to do here, but faster to replace the \"copy\" step above with this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ok_cliques['ok_n_locked'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ok_cliques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.8 ms, sys: 59 µs, total: 14.9 ms\n",
      "Wall time: 13.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# IMPORTANT don't remove.\n",
    "# UPDATE sorting by ok score, not lockified\n",
    "\n",
    "ok_cliques = ok_cliques[ok_cliques['ok_list'].apply(lambda x: len(x) >= 2)]\n",
    "ok_cliques = ok_cliques.sort_values('ok_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "# goal: select from non_clique the_rest, using distance from cliques as weights AND fullfilling n_gen_xyz constraints (where possible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things to note around here:\n",
    "- I've decided I'm going to draw directly out of supersets, rather than core then supersets. few reasons:\n",
    "    - simpler, faster\n",
    "    - not much bigger necessarily (or at least increase in size is lost in size variation among cliques), and we'll be removing not_present stuff anyway to make them smaller so actually smaller is bad.\n",
    "\n",
    "- I'll also be doing a kinda tricksy thing where, instead of filtering out cliques that contain not_present ingredients, I'll just be cutting out the not_present ingredients from those cliques. why:\n",
    "    - the clique list doesn't include cliques that are subsets of larger cliques (that'd take enumerate_all_cliques, which gives too much\n",
    "    - it's better to have a slightly smaller big clique than cut that out and choose a small one that's biggest it can be. (there's no award for having clique that's big as it can be)\n",
    "- I guess also, I'll be scoring partially on core size\n",
    "- also also, I might include u_ld with core of cliques where nodes are strong, rather than just cliques whose core consists of strong connections\n",
    "\n",
    "Ok another pretty complicated thing i wanna remember for if doing a blog\n",
    "- removing present nodes from cliques would fuck w score so\n",
    "- if I did \"enumerate_all_cliques\", I could just remove all that didn't include locked and did include a not_present\n",
    "- and then eliminate all with superset in same dataframe, since the (valid) superset would obviously be better\n",
    "- so to make this\n",
    "    - go through same motions but with enumerate_all_cliques?\n",
    "    - we'll see as we go\n",
    "- oof tho\n",
    "    - I am trying this now, and enumerate all cliques for u_ld is taking loooong time\n",
    "    - might be unworkable\n",
    "    - I guess I could try recalculating scores, OR\n",
    "        - just adjust score by some predetermined factor depending on how many not_present get taken away\n",
    "    - OR\n",
    "        - leave as is, don't even bother, so what if they don't get sampled perfectly in proportion to actual score\n",
    "- what I might actually go with\n",
    "    - calculate average score demerit for removing various numbers of present ingredients, then apply that ( {n_removed: score_demerit} ) according to n removed from each clique\n",
    "- wait nvm this\n",
    "    - get average scores for each clique length\n",
    "    - if a clique has not_present nodes removed, set it's score equal to halfway between old score and average score for new n_nodes\n",
    "    \n",
    "I guess while I'm at it here's another funky thing:\n",
    "- I'll be folding in umbrella ingredients, I think\n",
    "- gonna do it by\n",
    "    - calling all of its matches category?\n",
    "    - then adding umbrella matches on to regular matches if the match is greater than regular match?\n",
    "    - think so\n",
    "    \n",
    "Another thing\n",
    "- had to do some bleh workarounds to remove extra stuff instead of filtering whole click. bleeehhh so much typing I'm not joing it was horrible\n",
    "\n",
    "Another thing\n",
    "- still cocming up w a soln for that beans, grains, mush are all in foodstuffs\n",
    "- I *really* hope that hasn't already fucked me over (my assuming those were exclusive)\n",
    "    - Update: was fucked over a lil\n",
    "- soln: forgedaboudit\n",
    "    - removing mush, bean, grain entirely. can always add it back in.\n",
    "    \n",
    "Some more weird stuff\n",
    "- makin sure ok_data takes account of mix of locked, unlocked when it say s there's too muchhhh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# selected_clique = ok_cliques.sample(1).iloc[0]\n",
    "# the_rest_sans_clique_set = the_rest_set - selected_clique['ok_set']\n",
    "# the_rest_sans_clique_names = list(the_rest_sans_clique_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# #supa dupa fast\n",
    "\n",
    "# center_name = random.sample(selected_clique['ok_strong_set'], k=1)[0]\n",
    "# items_from_center = {key: value for key, value in present_shortest_path_lengths[strong_center_name].items() if key != center_name}\n",
    "# names_from_center = [key for key, item in items_from_center.items()]\n",
    "# lengths_from_center = [item for key, item in items_from_center.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# #supa dupa fast\n",
    "# # NO IDEA why above wasn't working when I got back. Is this even the most recent nb???\n",
    "\n",
    "# strong_center_name = random.sample(selected_clique['ok_strong_set'], k=1)[0]\n",
    "# items_from_center = {key: value for key, value in present_shortest_path_lengths[strong_center_name].items() if key != center_name}\n",
    "# names_from_center = [key for key, item in items_from_center.items()]\n",
    "# lengths_from_center = [item for key, item in items_from_center.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# lengths_from_clique_strongs_tuples = []\n",
    "# for present_name in list(present_set):\n",
    "#     if not present_name in selected_clique['ok_set']:\n",
    "#         lengths_from_each_strong = [present_shortest_path_lengths[center_name][present_name] for center_name in list(selected_clique['ok_strong_set'])]\n",
    "#         lengths_from_clique_strongs_tuples.append((present_name, sum(lengths_from_each_strong) / len(lengths_from_each_strong)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # fast: 8ms\n",
    "# # do this w each addition? w each strong addition?\n",
    "\n",
    "# lengths_from_clique_tuples = []\n",
    "# for present_name in list(present_set):\n",
    "#     if not present_name in selected_clique['ok_set']:\n",
    "#         lengths_from_each_clique_node = [present_shortest_path_lengths[clique_node][present_name] for clique_node in selected_clique['ok_list']]\n",
    "#         lengths_from_clique_tuples.append((present_name, sum(lengths_from_each_clique_node) / len(lengths_from_each_clique_node)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# selected_names = selected_clique['ok_list']\n",
    "\n",
    "\n",
    "# def get_sort_key(length_tuple):\n",
    "#     return length_tuple[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # fast: 8ms\n",
    "# # just changed selected_clique['ok list'] out for selected_names - so fuckin fast why now?\n",
    "# # and doing that outside is fast too? wtf?\n",
    "# # huh, it's the clique thing in the comprehension - bizarre, but now I know!\n",
    "# # so for sure do this each time, now\n",
    "# # worth weighting by strong?\n",
    "# # oh! you know what? Each value is already weighted :-D\n",
    "\n",
    "\n",
    "# lengths_from_clique_tuples = []\n",
    "# for present_name in list(present_set):\n",
    "#     if not present_name in selected_names:\n",
    "#         lengths_from_each_clique_node = [present_shortest_path_lengths[clique_node][present_name] for clique_node in selected_names]\n",
    "#         lengths_from_clique_tuples.append((present_name, sum(lengths_from_each_clique_node) / len(lengths_from_each_clique_node)))\n",
    "        \n",
    "# lengths_from_clique_tuples.sort(key=get_sort_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # super duper fast\n",
    "\n",
    "# def get_sort_key(length_tuple):\n",
    "#     return length_tuple[1]\n",
    "\n",
    "# lengths_from_clique_strongs_tuples.sort(key=get_sort_key)\n",
    "# lengths_from_clique_tuples.sort(key=get_sort_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "# already sorted, so I guess I could just find the closest eg. salt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locked_salts_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.3. Generator\n",
    "(contains code that will execute many times / call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N GEN OTHER FLAVORINGS ACTUAL 0\n",
      "N GEN FOODSTUFFS ACTUAL 3\n",
      "foodstuffs min/max 3 7\n",
      "actual foodstuffs min/max 3 7\n",
      "locked foodstuffs 0\n",
      "actual foodstuffs 3\n",
      "foodstuffs_so_far_set_len 3\n",
      "n_additional_foodstuffs 0\n",
      "n gen foodstuffs actual 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"500px\"\n",
       "            height=\"500px\"\n",
       "            src=\"cliqued_net.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fb21e4f5470>"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # %%time\n",
    "# # 60ms/iteration with no score\n",
    "# # same for with pairing bonus, weirdly. I guess nx is crazy fast?\n",
    "# # same with flavor bonus?? bizarre.\n",
    "# # k, more like 45 without thing I forgot to cocmment\n",
    "\n",
    "# # is lockified stuff enough to include locked? should I be doing multiple stages, 2 locked then 1 then none? or something?\n",
    "\n",
    "# # WILL have to deal with disconnected graphs\n",
    "\n",
    "# def get_sort_key(length_tuple):\n",
    "#     return length_tuple[1]\n",
    "\n",
    "# # pretty weird that I'm prolly gonna use density here, but average_shortest_path_length in scoring cliques..\n",
    "# # I guess it's fair I'm using the fast one where it matters?\n",
    "# # except with nodes, density would make *more* sense since it wouldn't be giving disconnected gaphs a too-high score\n",
    "# def n_possible_edges(n_nodes):\n",
    "#     return int((n_nodes*(n_nodes-1)) / 2)\n",
    "\n",
    "# def get_first_name_in_set(sorted_tuples, food_set):\n",
    "#     for sorted_tuple in sorted_tuples:\n",
    "#         if sorted_tuple[0] in food_set:\n",
    "#             return sorted_tuple[0]\n",
    "   \n",
    "# # n_other_flavorings_atual = max(n_other_flavorings_min)\n",
    "\n",
    "# # n_gen_salts = max(n_salts - len(locked_salts), 0)\n",
    "# # n_gen_fat_oils = max(n_fat_oils - len(locked_fat_oils), 0)\n",
    "# # n_gen_other_flavorings_min = max(n_other_flavorings_min - len(locked_other_flavorings), 0)\n",
    "# # n_gen_other_flavorings_max = max(n_other_flavorings_min - len(locked_other_flavorings), 0)\n",
    "# # n_gen_foodstuffs_min = max(n_foodstuffs_min - len(locked_foodstuffs), 0)\n",
    "# # n_gen_foodstuffs_max = max(n_foodstuffs_max - len(locked_foodstuffs), 0)\n",
    "\n",
    "# # n_gen_other_flavorings = min(random.randrange(n_gen_other_flavorings_min, n_gen_other_flavorings_max+1), len(the_rest_other_flavorings))\n",
    "# # n_gen_foodstuffs = min(random.randrange(n_gen_foodstuffs_min, n_gen_foodstuffs_max+1), len(the_rest_foodstuffs))\n",
    "\n",
    "# n_other_flavorings_actual = random.randrange(n_other_flavorings_min_actual, n_other_flavorings_max_actual+1)\n",
    "# n_foodstuffs_actual = random.randrange(n_foodstuffs_min_actual, n_foodstuffs_max_actual+1)\n",
    "\n",
    "# n_gen_other_flavorings_actual = n_other_flavorings_actual - len(locked_other_flavorings)\n",
    "# n_gen_foodstuffs_actual = n_foodstuffs_actual - len(locked_foodstuffs)\n",
    "# print('N GEN OTHER FLAVORINGS ACTUAL', n_gen_other_flavorings_actual)\n",
    "# print('N GEN FOODSTUFFS ACTUAL', n_gen_foodstuffs_actual)\n",
    "\n",
    "# final_cliques = ok_cliques[(ok_cliques['ok_n_other_flavorings'] <= n_gen_other_flavorings_actual) & (ok_cliques['ok_n_foodstuffs'] <= n_gen_foodstuffs_actual)]\n",
    "\n",
    "# # edge case: no cliques left (meh)\n",
    "# # clique = ok_cliques[:100].sample(1, weights='ok_score_lockified_xtreme').iloc[0] # to skew sample toward top\n",
    "# clique = ok_cliques.sample(1, weights='ok_score_xtreme').iloc[0] # to skew sample toward top\n",
    "\n",
    "# try:\n",
    "#     clique_ingredients = present.loc[clique['ok_list']]\n",
    "# except:\n",
    "#     print('MAJOR PROBLEM! Likely cause: clique_data contains not-present ingredients.')\n",
    "#     clique_ingredients = stir_fry_flavor_data.loc[clique['ok_list']]\n",
    "\n",
    "# salts_so_far_set = clique['ok_set'].intersection(salt_set).union(locked_salts_set)\n",
    "# fat_oils_so_far_set = clique['ok_set'].intersection(fat_oil_set).union(locked_fat_oils_set)\n",
    "# other_flavorings_so_far_set = clique['ok_set'].intersection(other_flavoring_set).union(locked_other_flavorings_set)\n",
    "# foodstuffs_so_far_set = clique['ok_set'].intersection(foodstuff_set).union(locked_foodstuffs_set)\n",
    "# ingredients_so_far_set = clique['ok_set'].union(locked_set)\n",
    "\n",
    "# n_additional_salts = n_salts_actual - len(salts_so_far_set) # pretty sure there shouldn't be more salts so far than salts actual\n",
    "# n_additional_fat_oils = n_fat_oils_actual - len(fat_oils_so_far_set) # pretty sure there shouldn't be more salts so far than salts actual\n",
    "# n_additional_other_flavorings = max(n_other_flavorings_actual - len(other_flavorings_so_far_set), 0) # pretty sure there shouldn't be more salts so far than salts actual\n",
    "# n_additional_foodstuffs = max(n_foodstuffs_actual - len(foodstuffs_so_far_set), 0) # pretty sure there shouldn't be more salts so far than salts actual\n",
    "# # n_locked_clique_other_flavorings\n",
    "# # n_locked_clique_foodstuffs\n",
    "# # n_locked_clique_salts\n",
    "# # n_locked_clique_fat_oils\n",
    "\n",
    "# # or could I rethink things to just, deal with that clique is combined locked & not locked?\n",
    "# # also, don't think I'm adding locked, below, to selected ingredients\n",
    "# # how about, add clique foodstuffs to the locked foodstuffs NOT in clique, to get all foodstuffs except additional\n",
    "# # or, make a set combining clique and locked, then use that to determine what's still needed\n",
    "\n",
    "# # Kludgy hack to accomodate for possibility that clique has more of a thing than gen (though it should have less than gen_max)\n",
    "# # says 'don't select more' in that case. there are worse things, I guess...\n",
    "# # n_additional_other_flavorings = max(n_gen_other_flavorings - clique['ok_n_other_flavorings'], 0)\n",
    "# # n_additional_foodstuffs = max(n_gen_foodstuffs - clique['ok_n_foodstuffs'], 0)\n",
    "# print('foodstuffs min/max', n_foodstuffs_min, n_foodstuffs_max)\n",
    "# print('actual foodstuffs min/max', n_foodstuffs_min_actual, n_foodstuffs_max_actual)\n",
    "# print('locked foodstuffs', len(locked_foodstuffs))\n",
    "# print('actual foodstuffs', n_foodstuffs_actual)#, clique['ok_n_foodstuffs'])\n",
    "# print('foodstuffs_so_far_set_len', len(foodstuffs_so_far_set))\n",
    "# print('n_additional_foodstuffs', n_additional_foodstuffs)\n",
    "# print('n gen foodstuffs actual', n_gen_foodstuffs_actual)\n",
    "\n",
    "# # For most of these, tho, we should have already assured that n <= gen\n",
    "# # n_additional_salts = n_gen_salts - clique['ok_n_salts'] # take the number of salts we need and subtract the number of salts present in the clique we selected\n",
    "# # n_additional_fat_oils = n_gen_fat_oils - clique['ok_n_fat_oils']\n",
    "\n",
    "# # print('flav, food, salt, fat', n_additional_other_flavorings, n_additional_foodstuffs, n_additional_salts, n_additional_fat_oils)\n",
    "# # selected_names = clique_ingredients['name'].tolist()\n",
    "# # additional_foodstuff_names = []\n",
    "# # for i in range(n_additional_foodstuffs):\n",
    "# #     lengths_from_selected_tuples = []\n",
    "# #     for present_name in list(present_set):\n",
    "# #         if not present_name in selected_names:\n",
    "# #             lengths_from_each_clique_node = [present_shortest_path_lengths[clique_node][present_name] for clique_node in selected_names]\n",
    "# #             lengths_from_selected_tuples.append((present_name, sum(lengths_from_each_clique_node) / len(lengths_from_each_clique_node)))\n",
    "\n",
    "# #     lengths_from_selected_tuples.sort(key=get_sort_key)    \n",
    "# #     additional_foodstuff_names.append(get_first_name_in_set(lengths_from_selected_tuples, foodstuff_set))\n",
    "# #     print(selected_names)\n",
    "# #     print(additional_foodstuff_names)\n",
    "# #     print()        \n",
    "            \n",
    "# # lengths_from_clique_tuples.sort(key=get_sort_key)\n",
    "\n",
    "# # so even if I was gonna sample at random what I NEED to be sampling is unlocked - clique\n",
    "# additional_salts = the_rest_salts[~the_rest_salts['name'].isin(clique_ingredients['name'])].sample(n_additional_salts, weights='weak_score')\n",
    "# additional_fat_oils = the_rest_fat_oils[~the_rest_fat_oils['name'].isin(clique_ingredients['name'])].sample(n_additional_fat_oils, weights='weak_score')\n",
    "# additional_other_flavorings = the_rest_other_flavorings[~the_rest_other_flavorings['name'].isin(clique_ingredients['name'])].sample(n_additional_other_flavorings, weights='weak_score')\n",
    "# additional_foodstuffs = the_rest_foodstuffs[~the_rest_foodstuffs['name'].isin(clique_ingredients['name'])].sample(n_additional_foodstuffs, weights='weak_score')\n",
    "\n",
    "# ingredients_so_far = present[present['name'].isin(ingredients_so_far_set)]\n",
    "\n",
    "# # selected_salts = locked_salts.append(the_rest_salts[~the_rest_salts['name'].isin(clique_ingredients['name'])].sample(n_gen_salts, weights='weak_score'))\n",
    "# # selected_fat_oils = locked_fat_oils.append(the_rest_fat_oils.sample(n_gen_fat_oils, weights='weak_score'))\n",
    "# # selected_other_flavorings = locked_other_flavorings.append(the_rest_other_flavorings.sample(n_gen_other_flavorings, weights='weak_score'))\n",
    "# # selected_foodstuffs = locked_foodstuffs.append(the_rest_foodstuffs.sample(n_gen_foodstuffs, weights='weak_score'))\n",
    "# # selected_ingredients = clique_ingredients.append(additional_salts).append(additional_fat_oils).append(additional_other_flavorings).append(additional_foodstuffs)\n",
    "# selected_ingredients = ingredients_so_far.append(additional_salts).append(additional_fat_oils).append(additional_other_flavorings).append(additional_foodstuffs)\n",
    "# # selected_ingredients = clique_ingredients\n",
    "# selected_names = selected_ingredients['name'].values.tolist()\n",
    "\n",
    "# #     lower_category_pairs = []\n",
    "# #     lower_direct_pairs = []\n",
    "# #     upper_category_pairs = []\n",
    "# #     upper_direct_pairs = []\n",
    "# connections = []\n",
    "# weighted_edges = []\n",
    "\n",
    "# for i_1, name_1 in enumerate(selected_names[:-1]):\n",
    "#     for i_2, name_2 in enumerate(selected_names[i_1+1:], i_1+1):\n",
    "#         connection = selected_ingredients[name_1][name_2]\n",
    "#         if connection[0] != '_':\n",
    "# #                 print(connection)\n",
    "    \n",
    "#                 # Really, since I'm using upper the only demerit values here will be .5333 and .4. but, I think I've adjusted variation to 0-1 anyway, so it should be ok (?)\n",
    "#             if connection[0] == 'c':\n",
    "#                 pairs_with_demerit = .8\n",
    "#             elif connection[0] == 'd':\n",
    "#                 pairs_with_demerit = .6666\n",
    "#             elif connection[0] == 'C':\n",
    "#                 pairs_with_demerit = .5333\n",
    "#             elif connection[0] == 'D':\n",
    "#                 pairs_with_demerit = .4\n",
    "#             else:\n",
    "#                 print('OH NO! BAD PAIRING VALUE.')\n",
    "\n",
    "#             if connection[1] == '_':\n",
    "#                 strength_demerit = .2\n",
    "#             elif connection[1] == 's':\n",
    "#                 strength_demerit = .15\n",
    "#             elif connection[1] == 'S':\n",
    "#                 strength_demerit = .1\n",
    "#             else:\n",
    "#                 print('OH NO! BAD STRENGTH VALUE.')\n",
    "                \n",
    "                \n",
    "#             connection_weight = pairs_with_demerit + strength_demerit\n",
    "#             weighted_edges.append((name_1, name_2, connection_weight))\n",
    "#             connections.append((name_1, name_2, connection))\n",
    "\n",
    "# selected_g = nx.Graph()\n",
    "# selected_g.add_nodes_from(selected_names)\n",
    "# selected_g.add_weighted_edges_from(weighted_edges)\n",
    "# score = 0\n",
    "\n",
    "# # PAIRING BONUS ============================================================================================\n",
    "# # ranges from roughly (0 to 1) * 3, tho could be a lil over or under that range\n",
    "# average_shortest_path_length = nx.average_shortest_path_length(selected_g, weight='weight')\n",
    "# average_shortest_path_score = 1 / average_shortest_path_length * 4 - 1\n",
    "# score += average_shortest_path_score * 3\n",
    "\n",
    "# # # FLAVOR BALANCE BONUS =============================================================================================\n",
    "# # # ranges from roughly (0 to 1) * 1 (could be a lil over/under)\n",
    "# # n_sweet_lower = (selected_ingredients['sweet'] == 'y').sum()\n",
    "# # n_sweet_upper = (selected_ingredients['sweet'] == 'Y').sum()\n",
    "# # n_salty_lower = (selected_ingredients['salty'] == 'y').sum()\n",
    "# # n_salty_upper = (selected_ingredients['salty'] == 'Y').sum()\n",
    "# # n_sour_lower = (selected_ingredients['sour'] == 'y').sum()\n",
    "# # n_sour_upper = (selected_ingredients['sour'] == 'Y').sum()\n",
    "# # n_savory_lower = (selected_ingredients['savory'] == 'y').sum()\n",
    "# # n_savory_upper = (selected_ingredients['savory'] == 'Y').sum()\n",
    "# # n_bitter_lower = (selected_ingredients['bitter'] == 'y').sum()\n",
    "# # n_bitter_upper = (selected_ingredients['bitter'] == 'Y').sum()\n",
    "# # n_spicy_lower = (selected_ingredients['spicy'] == 'y').sum()\n",
    "# # n_spicy_upper = (selected_ingredients['spicy'] == 'Y').sum()\n",
    "\n",
    "# # # each varies from roughly .5 to 1 (normalized to the average flavor score)\n",
    "# # sweet_score = (n_sweet_lower/2 + n_sweet_upper)/6\n",
    "# # salty_score = (n_salty_lower/2 + n_salty_upper)/4\n",
    "# # sour_score = (n_sour_lower/2 + n_sour_upper)/2.5\n",
    "# # savory_score = (n_savory_lower/2 + n_savory_upper)/3\n",
    "# # bitter_score = (n_bitter_lower/2 + n_bitter_upper)/3\n",
    "# # spicy_score = (n_spicy_lower/2 + n_spicy_upper)/3\n",
    "# # print(sweet_score, salty_score, sour_score, savory_score, bitter_score, spicy_score)\n",
    "\n",
    "# # # damn but this seems arbitrary 7 kludgy\n",
    "# # flavor_balance_score = 1 / (1 + abs(1-sweet_score) + abs(1-salty_score) + abs(1-sour_score) + abs(1-savory_score) + abs(1-spicy_score))\n",
    "# # # print(flavor_balance_score)\n",
    "# # score += flavor_balance_score\n",
    "# #     print()\n",
    "\n",
    "    \n",
    "# # FOOD GROUPS BONUS ==========================================================================================\n",
    "# n_fruit = (selected_ingredients['stir_fry_fruit'] == 'y').sum()\n",
    "# n_protein = (selected_ingredients['stir_fry_protein'] == 'y').sum()\n",
    "\n",
    "# # very very few, so realistically max .7\n",
    "# fruit_score = (n_fruit/2)**.5\n",
    "\n",
    "# #     0, .7, occaaaasionally 1\n",
    "# protein_score = (n_protein/2)**.5\n",
    "\n",
    "# # print(fruit_score, protein_score)\n",
    "# food_group_score = protein_score + fruit_score\n",
    "# score += food_group_score\n",
    "\n",
    "# # # will bias toward larger stir_frys, slightly\n",
    "# # # PROTEIN BONUS =========================================================================================\n",
    "# # # ranges from roughly (0 to 1) * .5 (mostly balanced on its own)\n",
    "# #     n_protein = (selected_ingredients['stir_fry_protein'] == 'y').sum()\n",
    "    \n",
    "# #     # /2 for steep diminishing returns (?)\n",
    "# #     protein_score = (n_protein/2)**.5 * .75\n",
    "# # #     print(protein_score)\n",
    "# #     score += protein_score * .5\n",
    "    \n",
    "# #     if score > top_score:\n",
    "# #         top_average_shortest_path_score = average_shortest_path_score\n",
    "# #         top_flavor_balance_score = flavor_balance_score\n",
    "# #         top_protein_score = protein_score\n",
    "# #         top_score = score\n",
    "# #         top_lc_pairs = lower_category_pairs\n",
    "# #         top_ld_pairs = lower_direct_pairs\n",
    "# #         top_uc_pairs = upper_category_pairs\n",
    "# #         top_ud_pairs = upper_direct_pairs\n",
    "# #         top_selected_ingredients = selected_ingredients\n",
    "# #         top_G = G\n",
    "# # print('RAW PAIRING BONUS', top_average_shortest_path_score)\n",
    "# # print('RAW FLAVOR BALANCE BONUS', top_flavor_balance_score)\n",
    "# # print('RAW PROTEIN BONUS', top_protein_score)\n",
    "# # print('SCORE', top_score)\n",
    "\n",
    "# cliqued_net = net.Network(notebook=True)\n",
    "\n",
    "# nodes = selected_ingredients['name'].tolist()\n",
    "\n",
    "# def get_color(row):\n",
    "# #     return 'grey'\n",
    "#     if row['stir_fry_veg'] == 'y':\n",
    "#         return 'green'\n",
    "#     elif row['stir_fry_fruit'] == 'y':\n",
    "#         return 'orange'\n",
    "#     elif row['stir_fry_protein'] == 'y':\n",
    "#         return 'brown'\n",
    "#     elif row['stir_fry_grain'] == 'y':\n",
    "#         return 'tan'\n",
    "#     else:\n",
    "#         return 'lightgrey'\n",
    "# nodes_color = selected_ingredients.apply(get_color, axis=1).tolist()\n",
    "\n",
    "# cliqued_net.add_nodes(\n",
    "#     nodes=nodes,\n",
    "#     color=nodes_color\n",
    "# )\n",
    "\n",
    "# for connection in connections:\n",
    "#     if connection[2][0] == 'c':\n",
    "#         cliqued_net.add_edge(connection[0], connection[1], physics=False, color='lightgrey')\n",
    "#     if connection[2][0] == 'd':\n",
    "#         cliqued_net.add_edge(connection[0], connection[1], physics=False, color='grey')\n",
    "#     if connection[2][0] == 'C':\n",
    "#         cliqued_net.add_edge(connection[0], connection[1], color='darkgrey')\n",
    "#     if connection[2][0] == 'D':\n",
    "#         cliqued_net.add_edge(connection[0], connection[1], color='black')\n",
    " \n",
    "# # print(score, average_shortest_path_length)\n",
    "# cliqued_net.show('cliqued_net.html')\n",
    "\n",
    "# # ISSUES\n",
    "#     # now that clique can include locked, is that messing me up?\n",
    "#         # cause I was assuming clique to be gen, but really it's a combo of gen and locked..\n",
    "#     # clique seems to be reliably more than gen_foodstuffs. like, by big margin.\n",
    "#         # could I take notes on how many foodstuffs each clique has, then draw from ones where it's smaller???\n",
    "#         # pity it'd be filtering (not smallening), buuut hey fuck it\n",
    "#         # but then, if person locks a bunch, wouldn't they want more foodstuffs?\n",
    "#         # MOAR IDEAS\n",
    "#             # it's really just foodstuffs and gen flavorings. would be no problem if there wasn't a range bn min/max, so that's an option.\n",
    "#             # could also generate sm, md, lg stir fry sizes (pair flavorings & regulars?)\n",
    "#             # or even make versions for each possible n foodstuffs\n",
    "#             # split into multiple rows?\n",
    "#             # idk man. suuure seems nice to just narrow the min-max\n",
    "#             # OR discard more than is needed?\n",
    "#                 # let's say it's 3-7, with 3 locked already\n",
    "#                 # wait, tho. that means there's more than usual shorties, but it's still pulled at random from high scored\n",
    "#             # OR discard random num?\n",
    "#             # OR sample more from small, if num is small? tho that still omits goodies inside of big ones.\n",
    "#             # radical - what if aim was more like, find clique that encompasses locked, then draw from that?????\n",
    "#             # THIS^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "#                 # & in absence of locked?\n",
    "#                     # I guess it's, like, do I pick big then whittle it down? or filter to small then pick one? which would lead to higher score?\n",
    "#                     # I guess I'd want to pick one that has at least enough of everything, then discard excess\n",
    "#                     # should think more about what alg leads to big pic big score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ALMONDS (and UNSWEETENED ALMOND BUTTER; see also MILK, ALMOND)</th>\n",
       "      <th>ANISE SEEDS</th>\n",
       "      <th>APPLES (and APPLE CIDER, APPLE JUICE and/or APPLESAUCE)</th>\n",
       "      <th>ARTICHOKE HEARTS (see also ARTICHOKES)</th>\n",
       "      <th>ARTICHOKES, JERUSALEM (aka SUNCHOKES)</th>\n",
       "      <th>ARUGULA (aka ROCKET)</th>\n",
       "      <th>ASPARAGUS</th>\n",
       "      <th>BASIL</th>\n",
       "      <th>BAY LEAF</th>\n",
       "      <th>BEANS, BLACK (aka TURTLE BEANS)</th>\n",
       "      <th>...</th>\n",
       "      <th>ok_list</th>\n",
       "      <th>ok_length</th>\n",
       "      <th>ok_score</th>\n",
       "      <th>ok_score_xtreme</th>\n",
       "      <th>ok_n_locked</th>\n",
       "      <th>ok_score_lockified</th>\n",
       "      <th>ok_score_lockified_xtreme</th>\n",
       "      <th>ok_strong_set</th>\n",
       "      <th>ok_n_other_flavorings</th>\n",
       "      <th>ok_n_foodstuffs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 259 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ALMONDS (and UNSWEETENED ALMOND BUTTER; see also MILK, ALMOND), ANISE SEEDS, APPLES (and APPLE CIDER, APPLE JUICE and/or APPLESAUCE), ARTICHOKE HEARTS (see also ARTICHOKES), ARTICHOKES, JERUSALEM (aka SUNCHOKES), ARUGULA (aka ROCKET), ASPARAGUS, BASIL, BAY LEAF, BEANS, BLACK (aka TURTLE BEANS), BEANS, PINTO, BEETS, BELL PEPPERS—IN GENERAL, or MIXED, BOK CHOY (aka CHINESE CABBAGE or PAK CHOI), BRAGG LIQUID AMINOS, BROCCOLI, BROCCOLINI, BRUSSELS SPROUTS, BUTTER, CABBAGE, GREEN, CABBAGE, RED, CARAWAY SEEDS, CARROTS, CASHEWS and CASHEW NUT BUTTER, CAULIFLOWER, CELERY, CELERY ROOT (aka CELERIAC), CHARD, e.g., RAINBOW, RED/RUBY, SWISS, or MIXED, CHEESE, CHEDDAR, CHEESE, PARMESAN, CHESTNUTS, CHIA SEEDS, CHICKPEAS (aka GARBANZO BEANS), CHILES, CHIPOTLE, CHILES, HABANERO, CHILES, JALAPEÑO, CHILES, SERRANO, CHILI PEPPER FLAKES, CHILI POWDER, CHIVES, CHIVES, GARLIC (aka CHINESE CHIVES), CILANTRO (aka CHINESE PARSLEY or FRESH CORIANDER LEAF), COCONUT BUTTER, COCONUT WATER, CORN, CORNMEAL and POLENTA (see also GRITS), CUMIN, DILL SEEDS (see also DILL and DILL WEED), DILL WEED (see also DILL and DILL SEEDS), EGGPLANT (aka AUBERGINE), EGGS (e.g., FRESH), ENDIVE (aka BELGIAN ENDIVE), FENNEL, FENNEL FRONDS (or LEAVES), FENNEL SEEDS, FENUGREEK, FIDDLEHEAD FERNS, FLAXSEEDS, FLOWERS, EDIBLE, GARLIC, GARLIC, GREEN (aka BABY GARLIC or SPRING GARLIC), GINGER, POWDERED (i.e., dried, ground), GINGER—IN GENERAL, GREENS, BEET, GREENS, COLLARD, GREENS, DANDELION, HAZELNUTS (aka FILBERTS), HONEY—IN GENERAL, KALE, KELP, KELP GRANULES, and KELP POWDER (see also ARAME, KOMBU, SEA VEGETABLES, and WAKAME), LAVENDER, LEEKS, LEMON VERBENA, LEMONGRASS, LEMONS, LENTILS, BROWN, LENTILS, GREEN, LENTILS, YELLOW, LIMES (e.g., JUICE, ZEST), LIQUID SMOKE, MANGOES, MILK, ALMOND, MILK, COCONUT, MILK, GOAT, MILK, HEMP, MILK, SOY, MILK, e.g., WHOLE or NONFAT—IN GENERAL, MILLET, MISO, DARK, MUSHROOMS, BLACK TRUMPET, MUSHROOMS, BUTTON (aka WHITE MUSHROOMS), MUSHROOMS, CHANTERELLE, MUSHROOMS, CHICKEN OF THE WOODS, MUSHROOMS, CREMINI (aka CRIMINI or ITALIAN BROWN MUSHROOMS), MUSHROOMS, MOREL, MUSHROOMS, OYSTER, MUSHROOMS, PORCINI (aka BOLETES or CÈPES; see also MUSHROOMS, WILD), MUSHROOMS, PORTOBELLO, MUSHROOMS, SHIITAKE—DRIED and FRESH, MUSTARD POWDER (aka DRY MUSTARD), ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 259 columns]"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ok_cliques.sample(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foodstuffs min/max 3 7\n",
      "actual foodstuffs min/max 3 7\n",
      "locked foodstuffs 0\n",
      "actual foodstuffs 3\n",
      "foodstuffs_so_far_set_len 3\n",
      "n_additional_foodstuffs 0\n",
      "n gen foodstuffs actual 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"500px\"\n",
       "            height=\"500px\"\n",
       "            src=\"cliqued_net.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fb22596f7b8>"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # %%time\n",
    "\n",
    "# # FILTERING FOR FINAL CLIQUES IN THIS ONE (cliques that don't have more other_flavorings or foodsteffs than the generated actual)\n",
    "\n",
    "# # 60ms/iteration with no score\n",
    "# # same for with pairing bonus, weirdly. I guess nx is crazy fast?\n",
    "# # same with flavor bonus?? bizarre.\n",
    "# # k, more like 45 without thing I forgot to cocmment\n",
    "\n",
    "# # is lockified stuff enough to include locked? should I be doing multiple stages, 2 locked then 1 then none? or something?\n",
    "\n",
    "# # WILL have to deal with disconnected graphs\n",
    "\n",
    "# def get_sort_key(length_tuple):\n",
    "#     return length_tuple[1]\n",
    "\n",
    "# # pretty weird that I'm prolly gonna use density here, but average_shortest_path_length in scoring cliques..\n",
    "# # I guess it's fair I'm using the fast one where it matters?\n",
    "# # except with nodes, density would make *more* sense since it wouldn't be giving disconnected gaphs a too-high score\n",
    "# def n_possible_edges(n_nodes):\n",
    "#     return int((n_nodes*(n_nodes-1)) / 2)\n",
    "\n",
    "# def get_first_name_in_set(sorted_tuples, food_set):\n",
    "#     for sorted_tuple in sorted_tuples:\n",
    "#         if sorted_tuple[0] in food_set:\n",
    "#             return sorted_tuple[0]\n",
    "   \n",
    "# # n_other_flavorings_atual = max(n_other_flavorings_min)\n",
    "\n",
    "# # n_gen_salts = max(n_salts - len(locked_salts), 0)\n",
    "# # n_gen_fat_oils = max(n_fat_oils - len(locked_fat_oils), 0)\n",
    "# # n_gen_other_flavorings_min = max(n_other_flavorings_min - len(locked_other_flavorings), 0)\n",
    "# # n_gen_other_flavorings_max = max(n_other_flavorings_min - len(locked_other_flavorings), 0)\n",
    "# # n_gen_foodstuffs_min = max(n_foodstuffs_min - len(locked_foodstuffs), 0)\n",
    "# # n_gen_foodstuffs_max = max(n_foodstuffs_max - len(locked_foodstuffs), 0)\n",
    "\n",
    "# # n_gen_other_flavorings = min(random.randrange(n_gen_other_flavorings_min, n_gen_other_flavorings_max+1), len(the_rest_other_flavorings))\n",
    "# # n_gen_foodstuffs = min(random.randrange(n_gen_foodstuffs_min, n_gen_foodstuffs_max+1), len(the_rest_foodstuffs))\n",
    "\n",
    "# n_other_flavorings_actual = random.randrange(n_other_flavorings_min_actual, n_other_flavorings_max_actual+1)\n",
    "# n_foodstuffs_actual = random.randrange(n_foodstuffs_min_actual, n_foodstuffs_max_actual+1)\n",
    "\n",
    "# n_gen_other_flavorings_actual = n_other_flavorings_actual - len(locked_other_flavorings)\n",
    "# n_gen_foodstuffs_actual = n_foodstuffs_actual - len(locked_foodstuffs)\n",
    "\n",
    "# final_cliques = ok_cliques[(ok_cliques['ok_n_other_flavorings'] <= n_gen_other_flavorings_actual) & (ok_cliques['ok_n_foodstuffs'] <= n_gen_foodstuffs_actual)]\n",
    "\n",
    "# # edge case: no cliques left (meh)\n",
    "# # clique = ok_cliques[:100].sample(1, weights='ok_score_lockified_xtreme').iloc[0] # to skew sample toward top\n",
    "# if len(final_cliques) > 0:\n",
    "#     clique = final_cliques.sample(1, weights='ok_score_xtreme').iloc[0] # to skew sample toward top\n",
    "# else:\n",
    "#     clique = final_cliques.sample(0)\n",
    "    \n",
    "# try:\n",
    "#     clique_ingredients = present.loc[clique['ok_list']]\n",
    "# except:\n",
    "#     print('MAJOR PROBLEM! Likely cause: clique_data contains not-present ingredients.')\n",
    "#     clique_ingredients = stir_fry_flavor_data.loc[clique['ok_list']]\n",
    "\n",
    "# salts_so_far_set = clique['ok_set'].intersection(salt_set).union(locked_salts_set)\n",
    "# fat_oils_so_far_set = clique['ok_set'].intersection(fat_oil_set).union(locked_fat_oils_set)\n",
    "# other_flavorings_so_far_set = clique['ok_set'].intersection(other_flavoring_set).union(locked_other_flavorings_set)\n",
    "# foodstuffs_so_far_set = clique['ok_set'].intersection(foodstuff_set).union(locked_foodstuffs_set)\n",
    "# ingredients_so_far_set = clique['ok_set'].union(locked_set)\n",
    "\n",
    "# n_additional_salts = n_salts_actual - len(salts_so_far_set) # pretty sure there shouldn't be more salts so far than salts actual\n",
    "# n_additional_fat_oils = n_fat_oils_actual - len(fat_oils_so_far_set) # pretty sure there shouldn't be more salts so far than salts actual\n",
    "# n_additional_other_flavorings = max(n_other_flavorings_actual - len(other_flavorings_so_far_set), 0) # pretty sure there shouldn't be more salts so far than salts actual\n",
    "# n_additional_foodstuffs = max(n_foodstuffs_actual - len(foodstuffs_so_far_set), 0) # pretty sure there shouldn't be more salts so far than salts actual\n",
    "# # n_locked_clique_other_flavorings\n",
    "# # n_locked_clique_foodstuffs\n",
    "# # n_locked_clique_salts\n",
    "# # n_locked_clique_fat_oils\n",
    "\n",
    "# # or could I rethink things to just, deal with that clique is combined locked & not locked?\n",
    "# # also, don't think I'm adding locked, below, to selected ingredients\n",
    "# # how about, add clique foodstuffs to the locked foodstuffs NOT in clique, to get all foodstuffs except additional\n",
    "# # or, make a set combining clique and locked, then use that to determine what's still needed\n",
    "\n",
    "# # Kludgy hack to accomodate for possibility that clique has more of a thing than gen (though it should have less than gen_max)\n",
    "# # says 'don't select more' in that case. there are worse things, I guess...\n",
    "# # n_additional_other_flavorings = max(n_gen_other_flavorings - clique['ok_n_other_flavorings'], 0)\n",
    "# # n_additional_foodstuffs = max(n_gen_foodstuffs - clique['ok_n_foodstuffs'], 0)\n",
    "# print('foodstuffs min/max', n_foodstuffs_min, n_foodstuffs_max)\n",
    "# print('actual foodstuffs min/max', n_foodstuffs_min_actual, n_foodstuffs_max_actual)\n",
    "# print('locked foodstuffs', len(locked_foodstuffs))\n",
    "# print('actual foodstuffs', n_foodstuffs_actual)#, clique['ok_n_foodstuffs'])\n",
    "# print('foodstuffs_so_far_set_len', len(foodstuffs_so_far_set))\n",
    "# print('n_additional_foodstuffs', n_additional_foodstuffs)\n",
    "# print('n gen foodstuffs actual', n_gen_foodstuffs_actual)\n",
    "\n",
    "# # For most of these, tho, we should have already assured that n <= gen\n",
    "# # n_additional_salts = n_gen_salts - clique['ok_n_salts'] # take the number of salts we need and subtract the number of salts present in the clique we selected\n",
    "# # n_additional_fat_oils = n_gen_fat_oils - clique['ok_n_fat_oils']\n",
    "\n",
    "# # print('flav, food, salt, fat', n_additional_other_flavorings, n_additional_foodstuffs, n_additional_salts, n_additional_fat_oils)\n",
    "# # selected_names = clique_ingredients['name'].tolist()\n",
    "# # additional_foodstuff_names = []\n",
    "# # for i in range(n_additional_foodstuffs):\n",
    "# #     lengths_from_selected_tuples = []\n",
    "# #     for present_name in list(present_set):\n",
    "# #         if not present_name in selected_names:\n",
    "# #             lengths_from_each_clique_node = [present_shortest_path_lengths[clique_node][present_name] for clique_node in selected_names]\n",
    "# #             lengths_from_selected_tuples.append((present_name, sum(lengths_from_each_clique_node) / len(lengths_from_each_clique_node)))\n",
    "\n",
    "# #     lengths_from_selected_tuples.sort(key=get_sort_key)    \n",
    "# #     additional_foodstuff_names.append(get_first_name_in_set(lengths_from_selected_tuples, foodstuff_set))\n",
    "# #     print(selected_names)\n",
    "# #     print(additional_foodstuff_names)\n",
    "# #     print()        \n",
    "            \n",
    "# # lengths_from_clique_tuples.sort(key=get_sort_key)\n",
    "\n",
    "# # so even if I was gonna sample at random what I NEED to be sampling is unlocked - clique\n",
    "# additional_salts = the_rest_salts[~the_rest_salts['name'].isin(clique_ingredients['name'])].sample(n_additional_salts, weights='weak_score')\n",
    "# additional_fat_oils = the_rest_fat_oils[~the_rest_fat_oils['name'].isin(clique_ingredients['name'])].sample(n_additional_fat_oils, weights='weak_score')\n",
    "# additional_other_flavorings = the_rest_other_flavorings[~the_rest_other_flavorings['name'].isin(clique_ingredients['name'])].sample(n_additional_other_flavorings, weights='weak_score')\n",
    "# additional_foodstuffs = the_rest_foodstuffs[~the_rest_foodstuffs['name'].isin(clique_ingredients['name'])].sample(n_additional_foodstuffs, weights='weak_score')\n",
    "\n",
    "# ingredients_so_far = present[present['name'].isin(ingredients_so_far_set)]\n",
    "\n",
    "# # selected_salts = locked_salts.append(the_rest_salts[~the_rest_salts['name'].isin(clique_ingredients['name'])].sample(n_gen_salts, weights='weak_score'))\n",
    "# # selected_fat_oils = locked_fat_oils.append(the_rest_fat_oils.sample(n_gen_fat_oils, weights='weak_score'))\n",
    "# # selected_other_flavorings = locked_other_flavorings.append(the_rest_other_flavorings.sample(n_gen_other_flavorings, weights='weak_score'))\n",
    "# # selected_foodstuffs = locked_foodstuffs.append(the_rest_foodstuffs.sample(n_gen_foodstuffs, weights='weak_score'))\n",
    "# # selected_ingredients = clique_ingredients.append(additional_salts).append(additional_fat_oils).append(additional_other_flavorings).append(additional_foodstuffs)\n",
    "# selected_ingredients = ingredients_so_far.append(additional_salts).append(additional_fat_oils).append(additional_other_flavorings).append(additional_foodstuffs)\n",
    "# # selected_ingredients = clique_ingredients\n",
    "# selected_names = selected_ingredients['name'].values.tolist()\n",
    "\n",
    "# #     lower_category_pairs = []\n",
    "# #     lower_direct_pairs = []\n",
    "# #     upper_category_pairs = []\n",
    "# #     upper_direct_pairs = []\n",
    "# connections = []\n",
    "# weighted_edges = []\n",
    "\n",
    "# for i_1, name_1 in enumerate(selected_names[:-1]):\n",
    "#     for i_2, name_2 in enumerate(selected_names[i_1+1:], i_1+1):\n",
    "#         connection = selected_ingredients[name_1][name_2]\n",
    "#         if connection[0] != '_':\n",
    "# #                 print(connection)\n",
    "    \n",
    "#                 # Really, since I'm using upper the only demerit values here will be .5333 and .4. but, I think I've adjusted variation to 0-1 anyway, so it should be ok (?)\n",
    "#             if connection[0] == 'c':\n",
    "#                 pairs_with_demerit = .8\n",
    "#             elif connection[0] == 'd':\n",
    "#                 pairs_with_demerit = .6666\n",
    "#             elif connection[0] == 'C':\n",
    "#                 pairs_with_demerit = .5333\n",
    "#             elif connection[0] == 'D':\n",
    "#                 pairs_with_demerit = .4\n",
    "#             else:\n",
    "#                 print('OH NO! BAD PAIRING VALUE.')\n",
    "\n",
    "#             if connection[1] == '_':\n",
    "#                 strength_demerit = .2\n",
    "#             elif connection[1] == 's':\n",
    "#                 strength_demerit = .15\n",
    "#             elif connection[1] == 'S':\n",
    "#                 strength_demerit = .1\n",
    "#             else:\n",
    "#                 print('OH NO! BAD STRENGTH VALUE.')\n",
    "                \n",
    "                \n",
    "#             connection_weight = pairs_with_demerit + strength_demerit\n",
    "#             weighted_edges.append((name_1, name_2, connection_weight))\n",
    "#             connections.append((name_1, name_2, connection))\n",
    "\n",
    "# selected_g = nx.Graph()\n",
    "# selected_g.add_nodes_from(selected_names)\n",
    "# selected_g.add_weighted_edges_from(weighted_edges)\n",
    "# score = 0\n",
    "\n",
    "# # PAIRING BONUS ============================================================================================\n",
    "# # ranges from roughly (0 to 1) * 3, tho could be a lil over or under that range\n",
    "# average_shortest_path_length = nx.average_shortest_path_length(selected_g, weight='weight')\n",
    "# average_shortest_path_score = 1 / average_shortest_path_length * 4 - 1\n",
    "# score += average_shortest_path_score * 3\n",
    "\n",
    "# # # FLAVOR BALANCE BONUS =============================================================================================\n",
    "# # # ranges from roughly (0 to 1) * 1 (could be a lil over/under)\n",
    "# # n_sweet_lower = (selected_ingredients['sweet'] == 'y').sum()\n",
    "# # n_sweet_upper = (selected_ingredients['sweet'] == 'Y').sum()\n",
    "# # n_salty_lower = (selected_ingredients['salty'] == 'y').sum()\n",
    "# # n_salty_upper = (selected_ingredients['salty'] == 'Y').sum()\n",
    "# # n_sour_lower = (selected_ingredients['sour'] == 'y').sum()\n",
    "# # n_sour_upper = (selected_ingredients['sour'] == 'Y').sum()\n",
    "# # n_savory_lower = (selected_ingredients['savory'] == 'y').sum()\n",
    "# # n_savory_upper = (selected_ingredients['savory'] == 'Y').sum()\n",
    "# # n_bitter_lower = (selected_ingredients['bitter'] == 'y').sum()\n",
    "# # n_bitter_upper = (selected_ingredients['bitter'] == 'Y').sum()\n",
    "# # n_spicy_lower = (selected_ingredients['spicy'] == 'y').sum()\n",
    "# # n_spicy_upper = (selected_ingredients['spicy'] == 'Y').sum()\n",
    "\n",
    "# # # each varies from roughly .5 to 1 (normalized to the average flavor score)\n",
    "# # sweet_score = (n_sweet_lower/2 + n_sweet_upper)/6\n",
    "# # salty_score = (n_salty_lower/2 + n_salty_upper)/4\n",
    "# # sour_score = (n_sour_lower/2 + n_sour_upper)/2.5\n",
    "# # savory_score = (n_savory_lower/2 + n_savory_upper)/3\n",
    "# # bitter_score = (n_bitter_lower/2 + n_bitter_upper)/3\n",
    "# # spicy_score = (n_spicy_lower/2 + n_spicy_upper)/3\n",
    "# # print(sweet_score, salty_score, sour_score, savory_score, bitter_score, spicy_score)\n",
    "\n",
    "# # # damn but this seems arbitrary 7 kludgy\n",
    "# # flavor_balance_score = 1 / (1 + abs(1-sweet_score) + abs(1-salty_score) + abs(1-sour_score) + abs(1-savory_score) + abs(1-spicy_score))\n",
    "# # # print(flavor_balance_score)\n",
    "# # score += flavor_balance_score\n",
    "# #     print()\n",
    "\n",
    "    \n",
    "# # FOOD GROUPS BONUS ==========================================================================================\n",
    "# n_fruit = (selected_ingredients['stir_fry_fruit'] == 'y').sum()\n",
    "# n_protein = (selected_ingredients['stir_fry_protein'] == 'y').sum()\n",
    "\n",
    "# # very very few, so realistically max .7\n",
    "# fruit_score = (n_fruit/2)**.5\n",
    "\n",
    "# #     0, .7, occaaaasionally 1\n",
    "# protein_score = (n_protein/2)**.5\n",
    "\n",
    "# # print(fruit_score, protein_score)\n",
    "# food_group_score = protein_score + fruit_score\n",
    "# score += food_group_score\n",
    "\n",
    "# # # will bias toward larger stir_frys, slightly\n",
    "# # # PROTEIN BONUS =========================================================================================\n",
    "# # # ranges from roughly (0 to 1) * .5 (mostly balanced on its own)\n",
    "# #     n_protein = (selected_ingredients['stir_fry_protein'] == 'y').sum()\n",
    "    \n",
    "# #     # /2 for steep diminishing returns (?)\n",
    "# #     protein_score = (n_protein/2)**.5 * .75\n",
    "# # #     print(protein_score)\n",
    "# #     score += protein_score * .5\n",
    "    \n",
    "# #     if score > top_score:\n",
    "# #         top_average_shortest_path_score = average_shortest_path_score\n",
    "# #         top_flavor_balance_score = flavor_balance_score\n",
    "# #         top_protein_score = protein_score\n",
    "# #         top_score = score\n",
    "# #         top_lc_pairs = lower_category_pairs\n",
    "# #         top_ld_pairs = lower_direct_pairs\n",
    "# #         top_uc_pairs = upper_category_pairs\n",
    "# #         top_ud_pairs = upper_direct_pairs\n",
    "# #         top_selected_ingredients = selected_ingredients\n",
    "# #         top_G = G\n",
    "# # print('RAW PAIRING BONUS', top_average_shortest_path_score)\n",
    "# # print('RAW FLAVOR BALANCE BONUS', top_flavor_balance_score)\n",
    "# # print('RAW PROTEIN BONUS', top_protein_score)\n",
    "# # print('SCORE', top_score)\n",
    "\n",
    "# cliqued_net = net.Network(notebook=True)\n",
    "\n",
    "# nodes = selected_ingredients['name'].tolist()\n",
    "\n",
    "# def get_color(row):\n",
    "# #     return 'grey'\n",
    "#     if row['stir_fry_veg'] == 'y':\n",
    "#         return 'green'\n",
    "#     elif row['stir_fry_fruit'] == 'y':\n",
    "#         return 'orange'\n",
    "#     elif row['stir_fry_protein'] == 'y':\n",
    "#         return 'brown'\n",
    "#     elif row['stir_fry_grain'] == 'y':\n",
    "#         return 'tan'\n",
    "#     else:\n",
    "#         return 'lightgrey'\n",
    "# nodes_color = selected_ingredients.apply(get_color, axis=1).tolist()\n",
    "\n",
    "# cliqued_net.add_nodes(\n",
    "#     nodes=nodes,\n",
    "#     color=nodes_color\n",
    "# )\n",
    "\n",
    "# for connection in connections:\n",
    "#     if connection[2][0] == 'c':\n",
    "#         cliqued_net.add_edge(connection[0], connection[1], physics=False, color='lightgrey')\n",
    "#     if connection[2][0] == 'd':\n",
    "#         cliqued_net.add_edge(connection[0], connection[1], physics=False, color='grey')\n",
    "#     if connection[2][0] == 'C':\n",
    "#         cliqued_net.add_edge(connection[0], connection[1], color='darkgrey')\n",
    "#     if connection[2][0] == 'D':\n",
    "#         cliqued_net.add_edge(connection[0], connection[1], color='black')\n",
    " \n",
    "# # print(score, average_shortest_path_length)\n",
    "# cliqued_net.show('cliqued_net.html')\n",
    "\n",
    "# # ISSUES\n",
    "#     # now that clique can include locked, is that messing me up?\n",
    "#         # cause I was assuming clique to be gen, but really it's a combo of gen and locked..\n",
    "#     # clique seems to be reliably more than gen_foodstuffs. like, by big margin.\n",
    "#         # could I take notes on how many foodstuffs each clique has, then draw from ones where it's smaller???\n",
    "#         # pity it'd be filtering (not smallening), buuut hey fuck it\n",
    "#         # but then, if person locks a bunch, wouldn't they want more foodstuffs?\n",
    "#         # MOAR IDEAS\n",
    "#             # it's really just foodstuffs and gen flavorings. would be no problem if there wasn't a range bn min/max, so that's an option.\n",
    "#             # could also generate sm, md, lg stir fry sizes (pair flavorings & regulars?)\n",
    "#             # or even make versions for each possible n foodstuffs\n",
    "#             # split into multiple rows?\n",
    "#             # idk man. suuure seems nice to just narrow the min-max\n",
    "#             # OR discard more than is needed?\n",
    "#                 # let's say it's 3-7, with 3 locked already\n",
    "#                 # wait, tho. that means there's more than usual shorties, but it's still pulled at random from high scored\n",
    "#             # OR discard random num?\n",
    "#             # OR sample more from small, if num is small? tho that still omits goodies inside of big ones.\n",
    "#             # radical - what if aim was more like, find clique that encompasses locked, then draw from that?????\n",
    "#             # THIS^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "#                 # & in absence of locked?\n",
    "#                     # I guess it's, like, do I pick big then whittle it down? or filter to small then pick one? which would lead to higher score?\n",
    "#                     # I guess I'd want to pick one that has at least enough of everything, then discard excess\n",
    "#                     # should think more about what alg leads to big pic big score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_other_flavorings_min_actual' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_other_flavorings_min_actual' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def get_sort_key(length_tuple):\n",
    "    return length_tuple[1]\n",
    "\n",
    "def n_possible_edges(n_nodes):\n",
    "    return int((n_nodes*(n_nodes-1)) / 2)\n",
    "\n",
    "def get_first_name_in_set(sorted_tuples, food_set):\n",
    "    for sorted_tuple in sorted_tuples:\n",
    "        if sorted_tuple[0] in food_set:\n",
    "            return sorted_tuple[0]\n",
    "\n",
    "n_iterations = 100\n",
    "keep_iterating = True\n",
    "\n",
    "n_attempts_before_deciding = 10\n",
    "n_attempts_before_giving_up = 5\n",
    "\n",
    "scoring_method = 'tbd'\n",
    "top_score = 0\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    connection_attempt = 1\n",
    "    while True:\n",
    "        n_other_flavorings_actual = random.randrange(n_other_flavorings_min_actual, n_other_flavorings_max_actual+1)\n",
    "        n_foodstuffs_actual = random.randrange(n_foodstuffs_min_actual, n_foodstuffs_max_actual+1)\n",
    "\n",
    "        n_gen_other_flavorings_actual = n_other_flavorings_actual - len(locked_other_flavorings)\n",
    "        n_gen_foodstuffs_actual = n_foodstuffs_actual - len(locked_foodstuffs)\n",
    "\n",
    "        final_cliques = ok_cliques[(ok_cliques['ok_n_other_flavorings'] <= n_gen_other_flavorings_actual) & (ok_cliques['ok_n_foodstuffs'] <= n_gen_foodstuffs_actual)]\n",
    "\n",
    "        if len(final_cliques) > 0:\n",
    "            clique = final_cliques.sample(1, weights='ok_score_xtreme').iloc[0] # to skew sample toward top\n",
    "        else:\n",
    "            clique = final_cliques.sample(0)\n",
    "\n",
    "        try:\n",
    "            clique_ingredients = present.loc[clique['ok_list']]\n",
    "        except:\n",
    "            print('MAJOR PROBLEM! Likely cause: clique_data contains not-present ingredients.')\n",
    "            clique_ingredients = stir_fry_flavor_data.loc[clique['ok_list']]\n",
    "\n",
    "        salts_so_far_set = clique['ok_set'].intersection(salt_set).union(locked_salts_set)\n",
    "        fat_oils_so_far_set = clique['ok_set'].intersection(fat_oil_set).union(locked_fat_oils_set)\n",
    "        other_flavorings_so_far_set = clique['ok_set'].intersection(other_flavoring_set).union(locked_other_flavorings_set)\n",
    "        foodstuffs_so_far_set = clique['ok_set'].intersection(foodstuff_set).union(locked_foodstuffs_set)\n",
    "        ingredients_so_far_set = clique['ok_set'].union(locked_set)\n",
    "\n",
    "        n_additional_salts = n_salts_actual - len(salts_so_far_set) # pretty sure there shouldn't be more salts so far than salts actual\n",
    "        n_additional_fat_oils = n_fat_oils_actual - len(fat_oils_so_far_set) # pretty sure there shouldn't be more salts so far than salts actual\n",
    "        n_additional_other_flavorings = max(n_other_flavorings_actual - len(other_flavorings_so_far_set), 0) # pretty sure there shouldn't be more salts so far than salts actual\n",
    "        n_additional_foodstuffs = max(n_foodstuffs_actual - len(foodstuffs_so_far_set), 0) # pretty sure there shouldn't be more salts so far than salts actual\n",
    "\n",
    "        additional_salts = the_rest_salts[~the_rest_salts['name'].isin(clique_ingredients['name'])].sample(n_additional_salts, weights='weak_score')\n",
    "        additional_fat_oils = the_rest_fat_oils[~the_rest_fat_oils['name'].isin(clique_ingredients['name'])].sample(n_additional_fat_oils, weights='weak_score')\n",
    "        additional_other_flavorings = the_rest_other_flavorings[~the_rest_other_flavorings['name'].isin(clique_ingredients['name'])].sample(n_additional_other_flavorings, weights='weak_score')\n",
    "        additional_foodstuffs = the_rest_foodstuffs[~the_rest_foodstuffs['name'].isin(clique_ingredients['name'])].sample(n_additional_foodstuffs, weights='weak_score')\n",
    "\n",
    "        ingredients_so_far = present[present['name'].isin(ingredients_so_far_set)]\n",
    "\n",
    "        selected_ingredients = ingredients_so_far.append(additional_salts).append(additional_fat_oils).append(additional_other_flavorings).append(additional_foodstuffs)\n",
    "        selected_names = selected_ingredients['name'].values.tolist()\n",
    "\n",
    "        connections = []\n",
    "        weighted_edges = []\n",
    "\n",
    "        for i_1, name_1 in enumerate(selected_names[:-1]):\n",
    "            for i_2, name_2 in enumerate(selected_names[i_1+1:], i_1+1):\n",
    "                connection = selected_ingredients[name_1][name_2]\n",
    "                if connection[0] != '_':\n",
    "                    if connection[0] == 'c':\n",
    "                        pairs_with_demerit = .8\n",
    "                    elif connection[0] == 'd':\n",
    "                        pairs_with_demerit = .6666\n",
    "                    elif connection[0] == 'C':\n",
    "                        pairs_with_demerit = .5333\n",
    "                    elif connection[0] == 'D':\n",
    "                        pairs_with_demerit = .4\n",
    "                    else:\n",
    "                        print('OH NO! BAD PAIRING VALUE.')\n",
    "\n",
    "                    if connection[1] == '_':\n",
    "                        strength_demerit = .2\n",
    "                    elif connection[1] == 's':\n",
    "                        strength_demerit = .15\n",
    "                    elif connection[1] == 'S':\n",
    "                        strength_demerit = .1\n",
    "                    else:\n",
    "                        print('OH NO! BAD STRENGTH VALUE.')\n",
    "\n",
    "\n",
    "                    connection_weight = pairs_with_demerit + strength_demerit\n",
    "                    weighted_edges.append((name_1, name_2, connection_weight))\n",
    "                    connections.append((name_1, name_2, connection))\n",
    "\n",
    "        selected_g = nx.Graph()\n",
    "        selected_g.add_nodes_from(selected_names)\n",
    "        selected_g.add_weighted_edges_from(weighted_edges)\n",
    "        connected_components = list(nx.connected_components(selected_g))\n",
    "        \n",
    "        if scoring_method == 'tbd':\n",
    "            if len(connected_components) == 1:\n",
    "                print('Scoring method set to \"connected\"')\n",
    "                scoring_method = 'connected'\n",
    "                break\n",
    "            elif connection_attempt >= n_attempts_before_deciding:\n",
    "                print('Scoring method set to \"disconnected\"')\n",
    "                scoring_method = 'disconnected'\n",
    "                break\n",
    "        elif scoring_method == 'connected':\n",
    "            if len(connected_components) == 1:\n",
    "                break\n",
    "            elif connection_attempt >= n_attempts_before_giving_up:\n",
    "                print('Giving up')\n",
    "                keep_iterating = False\n",
    "                break\n",
    "        elif scoring_method == 'disconnected':\n",
    "            break\n",
    "            \n",
    "        connection_attempt += 1\n",
    "        \n",
    "    if not keep_iterating:\n",
    "        break # Just go with the best iteration so far (rather than slogging through disconnected graphs)\n",
    "    \n",
    "    score = 0\n",
    "\n",
    "    if scoring_method == 'connected':\n",
    "        # CONNECTED PAIRING BONUS ============================================================================================\n",
    "        # ranges from roughly (0 to 1) * 3, tho could be a lil over or under that range\n",
    "        average_shortest_path_length = nx.average_shortest_path_length(selected_g, weight='weight')\n",
    "        average_shortest_path_score = 1 / average_shortest_path_length * 2 - 1.75\n",
    "#         print('AVERAGE SHORTEST PATH SCORE', average_shortest_path_score)\n",
    "        score += average_shortest_path_score * 3\n",
    "    else:\n",
    "        # DISCONNECTED PAIRING BONUS ============================================================================================\n",
    "        # not really sure how this sranges. hopefully (0 - 1) * 3? Hard to test.\n",
    "        largest_cc = max(connected_components, key=len)\n",
    "        largest_subgraph = selected_g.subgraph(largest_cc) # .copy()?\n",
    "        largest_subgraph_g = nx.Graph(largest_subgraph)\n",
    "        \n",
    "        average_shortest_path_length = nx.average_shortest_path_length(largest_subgraph_g, weight='weight')\n",
    "        average_shortest_path_score = 1 / average_shortest_path_length * 2 - 1.75\n",
    "#         print('DISCONNECTED AVERAGE SHORTEST PATH SCORE', average_shortest_path_score)\n",
    "        score += average_shortest_path_score * 3\n",
    "        \n",
    "    # FLAVOR BALANCE BONUS =============================================================================================\n",
    "    # ranges from roughly (0 to 1) * 1 (could be a lil over/under)\n",
    "    # keeping this the same from ye olde stir fry gen (app version)\n",
    "    n_sweet_lower = (selected_ingredients['sweet'] == 'y').sum()\n",
    "    n_sweet_upper = (selected_ingredients['sweet'] == 'Y').sum()\n",
    "    n_salty_lower = (selected_ingredients['salty'] == 'y').sum()\n",
    "    n_salty_upper = (selected_ingredients['salty'] == 'Y').sum()\n",
    "    n_sour_lower = (selected_ingredients['sour'] == 'y').sum()\n",
    "    n_sour_upper = (selected_ingredients['sour'] == 'Y').sum()\n",
    "    n_savory_lower = (selected_ingredients['savory'] == 'y').sum()\n",
    "    n_savory_upper = (selected_ingredients['savory'] == 'Y').sum()\n",
    "    n_bitter_lower = (selected_ingredients['bitter'] == 'y').sum()\n",
    "    n_bitter_upper = (selected_ingredients['bitter'] == 'Y').sum()\n",
    "    n_spicy_lower = (selected_ingredients['spicy'] == 'y').sum()\n",
    "    n_spicy_upper = (selected_ingredients['spicy'] == 'Y').sum()\n",
    "\n",
    "#     # each varies from roughly .5 to 1 (normalized to the average flavor score)\n",
    "#     sweet_score = (n_sweet_lower/2 + n_sweet_upper)/6\n",
    "#     salty_score = (n_salty_lower/2 + n_salty_upper)/4\n",
    "#     sour_score = (n_sour_lower/2 + n_sour_upper)/2.5\n",
    "#     savory_score = (n_savory_lower/2 + n_savory_upper)/3\n",
    "#     bitter_score = (n_bitter_lower/2 + n_bitter_upper)/3\n",
    "#     spicy_score = (n_spicy_lower/2 + n_spicy_upper)/3\n",
    "# #     print(sweet_score, salty_score, sour_score, savory_score, bitter_score, spicy_score)\n",
    "\n",
    "# #     damn but this seems arbitrary & kludgy\n",
    "# #     flavor_balance_score = 1 / (1 + abs(1-sweet_score) + abs(1-salty_score) + abs(1-sour_score) + abs(1-savory_score) + abs(1-spicy_score))\n",
    "    \n",
    "#     flavor_balance_score = 1 / (1 + abs(1-sweet_score) + abs(1-salty_score) + abs(1-sour_score) + abs(1-savory_score) + abs(1-spicy_score)) * 8 - 1.9\n",
    "#     print('FLAVOR BALANCE SCORE', flavor_balance_score)\n",
    "#     score += flavor_balance_score\n",
    "\n",
    "# SIMPLIFYING THIS, cause at end of the day it's up to user to balance shit    \n",
    "\n",
    "    # TWO OBJECTIVES: reward having a little of every flavor, maaaybe punish for unbalanced flavors\n",
    "    sweet_score = min(n_sweet_lower/2 + n_sweet_upper, 1)\n",
    "    salty_score = min(n_salty_lower/2 + n_salty_upper, 1)\n",
    "    sour_score = min(n_sour_lower/2 + n_sour_upper, 1)\n",
    "    savory_score = min(n_savory_lower/2 + n_savory_upper, 1)\n",
    "    bitter_score = min(n_bitter_lower/2 + n_bitter_upper, 1)\n",
    "    spicy_score = min(n_spicy_lower/2 + n_spicy_upper, 1)\n",
    "\n",
    "    flavor_balance_score = 0\n",
    "    flavor_balance_score += sweet_score*3 # rly want something sweet in there\n",
    "    flavor_balance_score += salty_score*.5 # can always use salt\n",
    "    flavor_balance_score += sour_score*2 # like me some sour\n",
    "    flavor_balance_score += savory_score*3 # LOVE me some savory\n",
    "    flavor_balance_score += bitter_score # idk\n",
    "    flavor_balance_score += spicy_score*2 # can be nice\n",
    "    flavor_balance_score = flavor_balance_score * .2 - 1.3\n",
    "    \n",
    "#     print('FLAVOR BALANCE SCORE', flavor_balance_score)\n",
    "    score += flavor_balance_score\n",
    "\n",
    "    # FOOD GROUPS BONUS ==========================================================================================\n",
    "#     n_fruit = (selected_ingredients['stir_fry_fruit'] == 'y').sum()\n",
    "#     n_protein = (selected_ingredients['stir_fry_protein'] == 'y').sum()\n",
    "\n",
    "#     # very very few, so realistically max .7\n",
    "#     fruit_score = (n_fruit/2)**.5\n",
    "#     print('FRUIT SCORE', fruit_score)\n",
    "\n",
    "#     #     0, .7, occaaaasionally 1\n",
    "#     protein_score = (n_protein/2)**.5\n",
    "#     print('PROTEIN SCORE', protein_score)\n",
    "\n",
    "    if 'y' in selected_ingredients['stir_fry_protein'].values:\n",
    "        protein_score = .5\n",
    "    else:\n",
    "        protein_score = 0\n",
    "    \n",
    "    if 'y' in selected_ingredients['stir_fry_fruit'].values:\n",
    "        fruit_score = .5\n",
    "    else:\n",
    "        fruit_score = 0\n",
    "\n",
    "    # print(fruit_score, protein_score)\n",
    "    food_group_score = protein_score + fruit_score\n",
    "#     print('PROTEIN FRUIT', protein_score, fruit_score)\n",
    "    score += food_group_score\n",
    "\n",
    "    # # will bias toward larger stir_frys, slightly\n",
    "    # # PROTEIN BONUS =========================================================================================\n",
    "    # # ranges from roughly (0 to 1) * .5 (mostly balanced on its own)\n",
    "    #     n_protein = (selected_ingredients['stir_fry_protein'] == 'y').sum()\n",
    "\n",
    "    #     # /2 for steep diminishing returns (?)\n",
    "    #     protein_score = (n_protein/2)**.5 * .75\n",
    "    # #     print(protein_score)\n",
    "    #     score += protein_score * .5\n",
    "\n",
    "    if score > top_score:\n",
    "        top_average_shortest_path_score = average_shortest_path_score\n",
    "        top_flavor_balance_score = flavor_balance_score\n",
    "        top_food_group_score = food_group_score\n",
    "#         top_protein_score = protein_score\n",
    "        top_score = score\n",
    "#         top_lc_pairs = lower_category_pairs\n",
    "#         top_ld_pairs = lower_direct_pairs\n",
    "#         top_uc_pairs = upper_category_pairs\n",
    "        top_ud_pairs = upper_direct_pairs\n",
    "        top_connections = connections\n",
    "        top_selected_ingredients = selected_ingredients\n",
    "#         top_selected_g = selected_g\n",
    "#     print('RAW PAIRING BONUS', top_average_shortest_path_score)\n",
    "#     print('RAW FLAVOR BALANCE BONUS', top_flavor_balance_score)\n",
    "#     print('RAW PROTEIN BONUS', top_protein_score)\n",
    "#     print('SCORE', top_score)\n",
    "#     print()\n",
    "print()\n",
    "print('RAW PAIRING BONUS', top_average_shortest_path_score)\n",
    "print('RAW FLAVOR BALANCE BONUS', top_flavor_balance_score)\n",
    "print('RAW FOOD GROUP BONUS', top_food_group_score)\n",
    "print('SCORE', top_score)\n",
    "\n",
    "cliqued_net = net.Network(notebook=True)\n",
    "\n",
    "nodes = top_selected_ingredients['name'].tolist()\n",
    "\n",
    "def get_color(row):\n",
    "#     return 'grey'\n",
    "    if row['stir_fry_veg'] == 'y':\n",
    "        return 'green'\n",
    "    elif row['stir_fry_fruit'] == 'y':\n",
    "        return 'orange'\n",
    "    elif row['stir_fry_protein'] == 'y':\n",
    "        return 'brown'\n",
    "    elif row['stir_fry_grain'] == 'y':\n",
    "        return 'tan'\n",
    "    else:\n",
    "        return 'lightgrey'\n",
    "nodes_color = top_selected_ingredients.apply(get_color, axis=1).tolist()\n",
    "\n",
    "cliqued_net.add_nodes(\n",
    "    nodes=nodes,\n",
    "    color=nodes_color\n",
    ")\n",
    "\n",
    "for connection in top_connections:\n",
    "    if connection[2][0] == 'c':\n",
    "        cliqued_net.add_edge(connection[0], connection[1], physics=False, color='lightgrey')\n",
    "    if connection[2][0] == 'd':\n",
    "        cliqued_net.add_edge(connection[0], connection[1], physics=False, color='grey')\n",
    "    if connection[2][0] == 'C':\n",
    "        cliqued_net.add_edge(connection[0], connection[1], color='darkgrey')\n",
    "    if connection[2][0] == 'D':\n",
    "        cliqued_net.add_edge(connection[0], connection[1], color='black')\n",
    " \n",
    "cliqued_net.show('cliqued_net.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GARLIC',\n",
       " 'MUSHROOMS, SHIITAKE—DRIED and FRESH',\n",
       " 'ONIONS, PEARL',\n",
       " 'PARSNIPS',\n",
       " 'POTATOES, BLUE (or PURPLE)'}"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foodstuffs_so_far_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "842"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ok_cliques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_cliques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['CORN',\n",
       "  'TOMATOES, TOMATO JUICE, TOMATO PASTE, and TOMATO SAUCE',\n",
       "  'CILANTRO (aka CHINESE PARSLEY or FRESH CORIANDER LEAF)',\n",
       "  'CHILES, SERRANO',\n",
       "  'BEANS, BLACK (aka TURTLE BEANS)',\n",
       "  'GARLIC, GREEN (aka BABY GARLIC or SPRING GARLIC)']]"
      ]
     },
     "execution_count": 624,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(final_cliques.sample(1)['ok_list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CABBAGE, RED',\n",
       " 'CHEESE, CHEDDAR',\n",
       " 'LENTILS, YELLOW',\n",
       " 'MUSHROOMS, BUTTON (aka WHITE MUSHROOMS)',\n",
       " 'MUSHROOMS, CREMINI (aka CRIMINI or ITALIAN BROWN MUSHROOMS)',\n",
       " 'MUSHROOMS, OYSTER',\n",
       " 'MUSHROOMS, SHIITAKE—DRIED and FRESH',\n",
       " 'ONIONS, RED (see also ONIONS)',\n",
       " 'PARSNIPS',\n",
       " 'POTATOES, THICK-SKINNED (e.g., IDAHO, RUSSET)',\n",
       " 'SQUASH, BUTTERNUT (see also SQUASH, WINTER)',\n",
       " 'THYME'}"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingredients_so_far_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BASIL', 'THYME', 'VINEGAR, APPLE CIDER (aka VINEGAR, CIDER)'}"
      ]
     },
     "execution_count": 625,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locked_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BASIL', 'THYME', 'VINEGAR, APPLE CIDER (aka VINEGAR, CIDER)'}"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locked_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1066,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.11 s, sys: 0 ns, total: 2.11 s\n",
      "Wall time: 2.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Am I going to have to adjust n_foodstuffs in cliques each iteration, if it's below n_foodstuffs of clique?\n",
    "    # cause cliques are only filtered to include fewer foodstuffs than max\n",
    "    # alternative: adjust n_gen_foodstuffs based on what's present in clique (easier)\n",
    "# Ugh, gotta sample from the_rest except clique\n",
    "\n",
    "top_score = 0\n",
    "for try_i in range(100):   \n",
    "    n_gen_other_flavorings = min(random.randrange(n_gen_other_flavorings_min, n_gen_other_flavorings_max+1), len(the_rest_other_flavorings))\n",
    "    n_gen_foodstuffs = min(random.randrange(n_gen_foodstuffs_min, n_gen_foodstuffs_max+1), len(the_rest_foodstuffs))\n",
    "\n",
    "    # edge case: no cliques left (meh)\n",
    "    clique = ok_cliques[:100].sample(1, weights='ok_score_xtreme').iloc[0] # to skew sample toward top\n",
    "#     cliques_xtreme = ok_cliques[:100].sample(10, weights='adjusted_score_xtreme')\n",
    "#     cliques = ok_cliques[:100].sample(10, weights='adjusted_score')\n",
    "#     print(cliques_xtreme['score'].mean())\n",
    "#     print(clique['adjusted_score'].iloc[0])\n",
    "#     print(clique['list'].iloc[0])\n",
    "#     print(type(clique.iloc[0]))\n",
    "\n",
    "    # hacky hack to account for cliques with more ingredients than actual gen (even though there's fewer than max gen)\n",
    "    # might replace it with something better later\n",
    "#     if clique['ok_n_other_flavorings'] > n_gen_other_flavorings:\n",
    "#         n_gen_other_flavorings = clique['ok_n_other_flavorings']\n",
    "#     if clique['']\n",
    "    \n",
    "    try:\n",
    "        clique_ingredients = present.loc[clique['ok_list']]\n",
    "    except:\n",
    "        print('MAJOR PROBLEM! Likely cause: clique_data contains not-present ingredients.')\n",
    "        clique_ingredients = stir_fry_flavor_data.loc[clique['ok_list']]\n",
    "    \n",
    "    # Kludgy hack to accomodate for possibility that clique has more of a thing than gen (though it should have less than gen_max)\n",
    "    # says 'don't select more' in that case. there are worse things, I guess...\n",
    "    n_additional_other_flavorings = max(n_gen_other_flavorings - clique['ok_n_other_flavorings'], 0)\n",
    "    n_additional_foodstuffs = max(n_gen_foodstuffs - clique['ok_n_foodstuffs'], 0)\n",
    "        \n",
    "    # For most of these, tho, we should have already assured that n <= gen\n",
    "    n_additional_salts = n_gen_salts - clique['ok_n_salts'] # take the number of salts we need and subtract the number of salts present in the clique we selected\n",
    "    n_additional_fat_oils = n_gen_fat_oils - clique['ok_n_fat_oils']\n",
    "    \n",
    "    additional_salts = the_rest_salts[~the_rest_salts['name'].isin(clique_ingredients['name'])].sample(n_additional_salts, weights='weak_score')\n",
    "    additional_fat_oils = the_rest_fat_oils[~the_rest_fat_oils['name'].isin(clique_ingredients['name'])].sample(n_additional_fat_oils, weights='weak_score')\n",
    "    additional_other_flavorings = the_rest_other_flavorings[~the_rest_other_flavorings['name'].isin(clique_ingredients['name'])].sample(n_additional_other_flavorings, weights='weak_score')\n",
    "    additional_foodstuffs = the_rest_foodstuffs[~the_rest_foodstuffs['name'].isin(clique_ingredients['name'])].sample(n_additional_foodstuffs, weights='weak_score')\n",
    "    \n",
    "#     selected_salts = locked_salts.append(the_rest_salts[~the_rest_salts['name'].isin(clique_ingredients['name'])].sample(n_gen_salts, weights='weak_score'))\n",
    "#     selected_fat_oils = locked_fat_oils.append(the_rest_fat_oils.sample(n_gen_fat_oils, weights='weak_score'))\n",
    "#     selected_other_flavorings = locked_other_flavorings.append(the_rest_other_flavorings.sample(n_gen_other_flavorings, weights='weak_score'))\n",
    "#     selected_foodstuffs = locked_foodstuffs.append(the_rest_foodstuffs.sample(n_gen_foodstuffs, weights='weak_score'))\n",
    "    selected_ingredients = clique_ingredients.append(additional_salts).append(additional_fat_oils).append(additional_other_flavorings).append(additional_foodstuffs)\n",
    "    selected_names = selected_ingredients['name'].values.tolist()\n",
    "\n",
    "# #     lower_category_pairs = []\n",
    "# #     lower_direct_pairs = []\n",
    "# #     upper_category_pairs = []\n",
    "# #     upper_direct_pairs = []\n",
    "#     connections = []\n",
    "\n",
    "#     for i_1, name_1 in enumerate(selected_names[:-1]):\n",
    "#         for i_2, name_2 in enumerate(selected_names[i_1+1:], i_1+1):\n",
    "#             connection = selected_ingredients[name_1][name_2]\n",
    "#             if connection[0] != '_':\n",
    "# #                 print(connection)\n",
    "#                 connections.append((name_1, name_2, connection))\n",
    "# #             if connection[0] == 'c':\n",
    "# #                 lower_category_pairs.append((name_1, name_2,))\n",
    "# #             elif connection[0] == 'd':\n",
    "# #                 lower_direct_pairs.append((name_1, name_2,))\n",
    "# #             elif connection[0] == 'C':\n",
    "# #                 upper_category_pairs.append((name_1, name_2,))\n",
    "# #             elif connection[0] == 'D':\n",
    "# #                 upper_direct_pairs.append((name_1, name_2,))\n",
    "\n",
    "# #     lower_pairs = lower_category_pairs + lower_direct_pairs\n",
    "# #     upper_pairs = upper_category_pairs + upper_direct_pairs\n",
    "# #     all_pairs = lower_pairs + upper_pairs\n",
    "  \n",
    "# #     G = nx.Graph()\n",
    "# #     G.add_nodes_from(selected_names)\n",
    "# #     G.add_edges_from(lower_category_pairs, length=2)\n",
    "# #     G.add_edges_from(lower_direct_pairs, length=1.5)\n",
    "# #     G.add_edges_from(upper_category_pairs, length=1.2)\n",
    "# #     G.add_edges_from(upper_direct_pairs, length=1)\n",
    "#     score = 0\n",
    "\n",
    "# # # PAIRING BONUS ============================================================================================\n",
    "# # # ranges from roughly (0 to 1) * 3, tho could be a lil over or under that range\n",
    "# #     average_shortest_path_length = nx.average_shortest_path_length(G, weight='length')\n",
    "# #     average_shortest_path_score = 1 / average_shortest_path_length * 4 - 1\n",
    "# # #     print(average_shortest_path_score)\n",
    "# #     score += average_shortest_path_score * 3\n",
    "  \n",
    "# # PAIRING BONUS ===============================================================================================\n",
    "#     connection_weighted_sum = 0\n",
    "#     for connection in connections:\n",
    "#         if connection[2][0] == 'c':\n",
    "#             pairs_with_score = .2\n",
    "#         elif connection[2][0] == 'd':\n",
    "#             pairs_with_score = .4\n",
    "#         elif connection[2][0] == 'C':\n",
    "#             pairs_with_score = .6\n",
    "#         elif connection[2][0] == 'D':\n",
    "#             pairs_with_score = .8\n",
    "#         else:\n",
    "#             print('OH NO! BAD PAIRING VALUE.')\n",
    "            \n",
    "#         if connection[2][1] == '_':\n",
    "#             strength_score = 0\n",
    "#         elif connection[2][1] == 's':\n",
    "#             strength_score = .1\n",
    "#         elif connection[2][1] == 'S':\n",
    "#             strength_score = .2\n",
    "#         else:\n",
    "#             print('OH NO! BAD STRENGTH VALUE.')\n",
    "#         connection_weight = pairs_with_score + strength_score\n",
    "#         connection_weighted_sum += connection_weight\n",
    "#     connection_density = connection_weighted_sum / math.factorial(len(selected_names)-1)\n",
    "#     print(connection_density)\n",
    "            \n",
    "# # COMMUNITY BONUS \n",
    "\n",
    "\n",
    "\n",
    "# # FLAVOR BALANCE BONUS =============================================================================================\n",
    "# # ranges from roughly (0 to 1) * 1 (could be a lil over/under)\n",
    "#     n_sweet_lower = (selected_ingredients['sweet'] == 'y').sum()\n",
    "#     n_sweet_upper = (selected_ingredients['sweet'] == 'Y').sum()\n",
    "#     n_salty_lower = (selected_ingredients['salty'] == 'y').sum()\n",
    "#     n_salty_upper = (selected_ingredients['salty'] == 'Y').sum()\n",
    "#     n_sour_lower = (selected_ingredients['sour'] == 'y').sum()\n",
    "#     n_sour_upper = (selected_ingredients['sour'] == 'Y').sum()\n",
    "#     n_savory_lower = (selected_ingredients['savory'] == 'y').sum()\n",
    "#     n_savory_upper = (selected_ingredients['savory'] == 'Y').sum()\n",
    "#     n_bitter_lower = (selected_ingredients['bitter'] == 'y').sum()\n",
    "#     n_bitter_upper = (selected_ingredients['bitter'] == 'Y').sum()\n",
    "#     n_spicy_lower = (selected_ingredients['spicy'] == 'y').sum()\n",
    "#     n_spicy_upper = (selected_ingredients['spicy'] == 'Y').sum()\n",
    "\n",
    "#     # each varies from roughly .5 to 1 (normalized to the average flavor score)\n",
    "#     sweet_score = (n_sweet_lower/2 + n_sweet_upper)/6\n",
    "#     salty_score = (n_salty_lower/2 + n_salty_upper)/4\n",
    "#     sour_score = (n_sour_lower/2 + n_sour_upper)/2.5\n",
    "#     savory_score = (n_savory_lower/2 + n_savory_upper)/3\n",
    "#     bitter_score = (n_bitter_lower/2 + n_bitter_upper)/3\n",
    "#     spicy_score = (n_spicy_lower/2 + n_spicy_upper)/3\n",
    "\n",
    "#     flavor_balance_score = 5 / (1 + abs(1-sweet_score) + abs(1-salty_score) + abs(1-sour_score) + abs(1-savory_score) + abs(1-spicy_score)) - .9\n",
    "# #     print(flavor_balance_score)\n",
    "#     score += flavor_balance_score\n",
    "# #     print()\n",
    "\n",
    "# # will bias toward larger stir_frys, slightly\n",
    "# # PROTEIN BONUS =========================================================================================\n",
    "# # ranges from roughly (0 to 1) * .5 (mostly balanced on its own)\n",
    "#     n_protein = (selected_ingredients['stir_fry_protein'] == 'y').sum()\n",
    "    \n",
    "#     # /2 for steep diminishing returns (?)\n",
    "#     protein_score = (n_protein/2)**.5 * .75\n",
    "# #     print(protein_score)\n",
    "#     score += protein_score * .5\n",
    "    \n",
    "#     if score > top_score:\n",
    "#         top_average_shortest_path_score = average_shortest_path_score\n",
    "#         top_flavor_balance_score = flavor_balance_score\n",
    "#         top_protein_score = protein_score\n",
    "#         top_score = score\n",
    "#         top_lc_pairs = lower_category_pairs\n",
    "#         top_ld_pairs = lower_direct_pairs\n",
    "#         top_uc_pairs = upper_category_pairs\n",
    "#         top_ud_pairs = upper_direct_pairs\n",
    "#         top_selected_ingredients = selected_ingredients\n",
    "#         top_G = G\n",
    "# print('RAW PAIRING BONUS', top_average_shortest_path_score)\n",
    "# print('RAW FLAVOR BALANCE BONUS', top_flavor_balance_score)\n",
    "# print('RAW PROTEIN BONUS', top_protein_score)\n",
    "# print('SCORE', top_score)\n",
    "\n",
    "# top_net = net.Network(notebook=True)\n",
    "\n",
    "# nodes = top_selected_ingredients['name'].tolist()\n",
    "\n",
    "# def get_color(row):\n",
    "# #     return 'grey'\n",
    "#     if row['stir_fry_veg'] == 'y':\n",
    "#         return 'green'\n",
    "#     elif row['stir_fry_fruit'] == 'y':\n",
    "#         return 'orange'\n",
    "#     elif row['stir_fry_protein'] == 'y':\n",
    "#         return 'brown'\n",
    "#     elif row['stir_fry_grain'] == 'y':\n",
    "#         return 'tan'\n",
    "#     else:\n",
    "#         return 'lightgrey'\n",
    "# nodes_color = top_selected_ingredients.apply(get_color, axis=1).tolist()\n",
    "\n",
    "# top_net.add_nodes(\n",
    "#     nodes=nodes,\n",
    "#     color=nodes_color\n",
    "# )\n",
    "\n",
    "# for pair in top_lc_pairs:\n",
    "#     top_net.add_edge(pair[0], pair[1], physics=False, color='lightgrey')\n",
    "# for pair in top_ld_pairs:\n",
    "#     top_net.add_edge(pair[0], pair[1], physics=False, color='grey')\n",
    "# for pair in top_uc_pairs:\n",
    "#     top_net.add_edge(pair[0], pair[1], color='darkgrey')\n",
    "# for pair in top_ud_pairs:\n",
    "#     top_net.add_edge(pair[0], pair[1], color='black')\n",
    "\n",
    "# top_net.show('top_net.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'found_cliques' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-572-23bcdbf69eec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0medges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclique\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_cliques\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclique\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mnodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_node'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'found_cliques' is not defined"
     ]
    }
   ],
   "source": [
    "thing_net = net.Network(notebook=True)\n",
    "        \n",
    "nodes = []\n",
    "edges = []\n",
    "\n",
    "for i, clique in enumerate(found_cliques):\n",
    "    if len(clique) > 6:\n",
    "        nodes.append(str(i) + '_node')\n",
    "        for node in clique:\n",
    "            node_name = str(i) + '_' + node\n",
    "            nodes.append(node_name)\n",
    "            edges.append((str(i) + '_node', node_name))\n",
    "        \n",
    "# print(nodes)\n",
    "thing_net.add_nodes(nodes=nodes)\n",
    "\n",
    "for edge in edges:\n",
    "    thing_net.add_edge(edge[0], edge[1])\n",
    "    \n",
    "# def get_color(row):\n",
    "# #     return 'grey'\n",
    "#     if row['stir_fry_veg'] == 'y':\n",
    "#         return 'green'\n",
    "#     elif row['stir_fry_fruit'] == 'y':\n",
    "#         return 'orange'\n",
    "#     elif row['stir_fry_protein'] == 'y':\n",
    "#         return 'brown'\n",
    "#     elif row['stir_fry_grain'] == 'y':\n",
    "#         return 'tan'\n",
    "#     else:\n",
    "#         return 'lightgrey'\n",
    "# nodes_color = top_selected_ingredients.apply(get_color, axis=1).tolist()\n",
    "\n",
    "# thing_net.add_nodes(\n",
    "#     nodes=nodes + list(range(100))\n",
    "# #     color=nodes_color\n",
    "# )\n",
    "\n",
    "    \n",
    "\n",
    "# for k, v in part.items():\n",
    "#     thing_net.add_edge(k, v)\n",
    "\n",
    "# for pair in top_lc_pairs:\n",
    "#     top_net.add_edge(pair[0], pair[1], physics=False, color='lightgrey')\n",
    "# for pair in top_ld_pairs:\n",
    "#     top_net.add_edge(pair[0], pair[1], physics=False, color='grey')\n",
    "# for pair in top_uc_pairs:\n",
    "#     top_net.add_edge(pair[0], pair[1], color='darkgrey')\n",
    "# for pair in top_ud_pairs:\n",
    "#     top_net.add_edge(pair[0], pair[1], color='black')\n",
    "\n",
    "thing_net.show('thing_net.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.4 ms, sys: 0 ns, total: 14.4 ms\n",
      "Wall time: 14.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "containing_cliques = cliques_containing_node(G, nodes=['WALNUTS', 'YAMS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 441 ms, sys: 3.88 ms, total: 444 ms\n",
      "Wall time: 444 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lengths = nx.all_pairs_dijkstra_path_length(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. Random control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"500px\"\n",
       "            height=\"500px\"\n",
       "            src=\"random_net.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f396b4e84a8>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_subgraphs = 2\n",
    "while n_subgraphs > 1: # keep shuffling until you get a well connected graph\n",
    "\n",
    "    n_gen_greens = random.randrange(n_gen_greens_min, n_gen_greens_max+1)\n",
    "\n",
    "    n_gen_extras = random.randrange(n_gen_extras_min, n_gen_extras_max+1)\n",
    "\n",
    "    n_gen_dressing_oils = max(1-len(locked_dressing_oils), 0)\n",
    "    n_gen_dressing_vinegars = max(1-len(locked_dressing_vinegars), 0)\n",
    "    n_gen_dressing_salts = max(1-len(locked_dressing_salts), 0)\n",
    "    n_gen_dressing_peppers = max(1-len(locked_dressing_oils), 0)\n",
    "\n",
    "    selected_greens = locked_greens.append(the_rest_greens.sample(n_gen_greens))\n",
    "    selected_extras = locked_extras.append(the_rest_extras.sample(n_gen_extras))\n",
    "    selected_dressing_oils = locked_dressing_oils.append(the_rest_dressing_oils.sample(n_gen_dressing_oils))\n",
    "    selected_dressing_vinegars = locked_dressing_vinegars.append(the_rest_dressing_vinegars.sample(n_gen_dressing_vinegars))\n",
    "    selected_dressing_salts = locked_dressing_salts.append(the_rest_dressing_salts.sample(n_gen_dressing_salts))\n",
    "    selected_dressing_peppers = locked_dressing_peppers.append(the_rest_dressing_peppers.sample(n_gen_dressing_peppers))\n",
    "    selected_ingredients = selected_greens.append(selected_extras).append(selected_dressing_oils).append(selected_dressing_vinegars).append(selected_dressing_salts).append(selected_dressing_peppers)\n",
    "    selected_names = selected_ingredients['name'].values.tolist()\n",
    "\n",
    "    lower_category_pairs = []\n",
    "    lower_direct_pairs = []\n",
    "    upper_category_pairs = []\n",
    "    upper_direct_pairs = []\n",
    "    lower_clashing_pairs = []\n",
    "    upper_clashing_pairs = []\n",
    "\n",
    "    # finicky but pretty fast\n",
    "    for i, col_name in enumerate(selected_names):\n",
    "        for j, row_name in enumerate(selected_names[i+1:]):\n",
    "            connection = selected_ingredients[col_name].tolist()[i+1+j] # this is what is finicky\n",
    "            if connection == 'c':\n",
    "                lower_category_pairs.append((col_name, row_name,))\n",
    "            elif connection == 'd':\n",
    "                lower_direct_pairs.append((col_name, row_name,))\n",
    "            elif connection == 'C':\n",
    "                upper_category_pairs.append((col_name, row_name,))\n",
    "            elif connection == 'D':\n",
    "                upper_direct_pairs.append((col_name, row_name,))\n",
    "            elif connection == 'n':\n",
    "                lower_clashing_pairs.append((col_name, row_name,))\n",
    "            elif connection == 'N':\n",
    "                upper_clashing_pairs.append((col_name, row_name,))\n",
    "    lower_pairs = lower_category_pairs + lower_direct_pairs\n",
    "    upper_pairs = upper_category_pairs + upper_direct_pairs\n",
    "    all_pairs = lower_pairs + upper_pairs\n",
    "    all_clashing_pairs = lower_clashing_pairs + upper_clashing_pairs\n",
    "\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(selected_names)\n",
    "    G.add_edges_from(lower_category_pairs, length=2)\n",
    "    G.add_edges_from(lower_direct_pairs, length=1.5)\n",
    "    G.add_edges_from(upper_category_pairs, length=1.2)\n",
    "    G.add_edges_from(upper_direct_pairs, length=1)\n",
    "    n_subgraphs = len(list(nx.connected_component_subgraphs(G)))\n",
    "\n",
    "random_net = net.Network(notebook=True)\n",
    "\n",
    "nodes = selected_ingredients['name'].tolist()\n",
    "\n",
    "def get_color(row):\n",
    "    if row['stir_fry_green'] == 'y':\n",
    "        return 'lightgreen'\n",
    "    elif row['stir_fry_extra'] == 'y':\n",
    "        if row['veg'] == 'y':\n",
    "            return 'green'\n",
    "        elif row['fruit'] == 'y':\n",
    "            return 'orange'\n",
    "        elif row['protein_nut_seed'] == 'y':\n",
    "            return 'brown'\n",
    "        else:\n",
    "            return 'lightblue'\n",
    "    elif row['stir_fry_dressing'] == 'y':\n",
    "        return 'lightgrey'\n",
    "nodes_color = selected_ingredients.apply(get_color, axis=1).tolist()\n",
    "\n",
    "random_net.add_nodes(\n",
    "    nodes=nodes,\n",
    "    color=nodes_color\n",
    ")\n",
    "\n",
    "for pair in lower_category_pairs:\n",
    "    random_net.add_edge(pair[0], pair[1], physics=False, color='lightgrey')\n",
    "for pair in lower_direct_pairs:\n",
    "    random_net.add_edge(pair[0], pair[1], physics=False, color='grey')\n",
    "for pair in upper_category_pairs:\n",
    "    random_net.add_edge(pair[0], pair[1], color='darkgrey')\n",
    "for pair in upper_direct_pairs:\n",
    "    random_net.add_edge(pair[0], pair[1], color='black')\n",
    "\n",
    "vegan = selected_ingredients['not_vegan'].sum() == ''\n",
    "gluten_free = selected_ingredients['gluten'].sum() == ''\n",
    "\n",
    "random_net.show('random_net.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Recording recipes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. Recording with flavor tool generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     stir_fry_recipe_data = pd.read_pickle(os.path.join(root_path, 'DATA/stir_fry_recipe_data_latest.pickle'))\n",
    "#     print('stir_fry RECIPE DATA IMPORT SUCCESSFUL')\n",
    "# except:\n",
    "#     stir_fry_recipe_data = pd.DataFrame({\n",
    "#         'vegan': [],\n",
    "#         'gluten_free': [],\n",
    "#         'basic': [],\n",
    "#         'best_of': [],\n",
    "#         'score': [],\n",
    "#         'pairing_density_bonus': [],\n",
    "#         'pair_strength_bonus': [],\n",
    "#         'clash_penalty': [],\n",
    "#         'flavor_balance_bonus': [],\n",
    "#         'texture_balance_bonus': [],\n",
    "#         'food_group_balance_bonus': [],\n",
    "#         'lc_pairs': [],\n",
    "#         'ld_pairs': [],\n",
    "#         'uc_pairs': [],\n",
    "#         'ud_pairs':[],\n",
    "#         'clashing_pairs': [],\n",
    "#         'ingredient_names': [],\n",
    "#         'leafy_green_names': [],\n",
    "#         'extra_names': [],\n",
    "#         'dressing_names': [],\n",
    "#     })\n",
    "#     print('IMPORT FAILED, CREATING NEW stir_fry RECIPE DATAFRAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stir_fry_greens = stir_fry_data[stir_fry_data['stir_fry_green'] == 'y']\n",
    "\n",
    "# stir_fry_extras = stir_fry_data[stir_fry_data['stir_fry_extra'] == 'y']\n",
    "# stir_fry_extra_veg = stir_fry_data[(stir_fry_data['veg'] == 'y') & (stir_fry_data['stir_fry_extra'] == 'y')]\n",
    "# stir_fry_extra_fruits = stir_fry_data[(stir_fry_data['fruit'] == 'y') & (stir_fry_data['stir_fry_extra'] == 'y')]\n",
    "# stir_fry_extra_nuts = stir_fry_data[(stir_fry_data['protein_seed'] == 'y') & (stir_fry_data['stir_fry_extra'] == 'y')]\n",
    "# stir_fry_extra_seeds = stir_fry_data[(stir_fry_data['protein_nut'] == 'y') & (stir_fry_data['stir_fry_extra'] == 'y')]\n",
    "# stir_fry_extra_tomatoes = stir_fry_data[stir_fry_data['stir_fry_extra_tomato'] == 'y']\n",
    "# stir_fry_extra_olives = stir_fry_data[stir_fry_data['stir_fry_extra_olive'] == 'y']\n",
    "# stir_fry_extra_cheeses = stir_fry_data[stir_fry_data['stir_fry_extra_cheese'] == 'y']\n",
    "# stir_fry_extra_eggs = stir_fry_data[stir_fry_data['stir_fry_extra_egg'] == 'y']\n",
    "# stir_fry_extra_croutons = stir_fry_data[stir_fry_data['stir_fry_extra_crouton'] == 'y']\n",
    "\n",
    "# stir_fry_dressing_oils = stir_fry_data[stir_fry_data['stir_fry_dressing_oil'] == 'y']\n",
    "# stir_fry_dressing_vinegars = stir_fry_data[stir_fry_data['stir_fry_dressing_vinegar'] == 'y']\n",
    "# stir_fry_dressing_salts = stir_fry_data[stir_fry_data['stir_fry_dressing_salt'] == 'y']\n",
    "# stir_fry_dressing_peppers = stir_fry_data[stir_fry_data['stir_fry_dressing_pepper'] == 'y']\n",
    "# stir_fry_dressing_garlics = stir_fry_data[stir_fry_data['stir_fry_dressing_garlic'] == 'y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !pip install networkx\n",
    "\n",
    "# import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# best_of = 300\n",
    "# for recipe_count in range(500):\n",
    "#     top_score = 0\n",
    "#     for i in range(best_of):\n",
    "#         n_subgraphs = 2\n",
    "#         while n_subgraphs > 1: # keep shuffling until you get a well connected graph\n",
    "#             n_greens = random.randrange(2, 4)\n",
    "#             n_extras = random.randrange(2, 6)\n",
    "#             n_dressing_oils = 1\n",
    "#             n_dressing_vinegars = 1\n",
    "#             n_dressing_salts = 1\n",
    "#             n_dressing_peppers = 1\n",
    "#             # n_dressing_garlics = random.randrange(0, 2) # maybe make presence dependent on the rest. or, just leave out for now.\n",
    "\n",
    "#             selected_greens = stir_fry_greens.sample(n_greens)\n",
    "#             selected_extras = stir_fry_extras.sample(n_extras)\n",
    "#             selected_dressing_oils = stir_fry_dressing_oils.sample(n_dressing_oils)\n",
    "#             selected_dressing_vinegars = stir_fry_dressing_vinegars.sample(n_dressing_vinegars)\n",
    "#             selected_dressing_salts = stir_fry_dressing_salts.sample(n_dressing_salts)\n",
    "#             selected_dressing_peppers = stir_fry_dressing_peppers.sample(n_dressing_peppers)\n",
    "#             selected_ingredients = selected_greens.append(selected_extras).append(selected_dressing_oils).append(selected_dressing_vinegars).append(selected_dressing_salts).append(selected_dressing_peppers)\n",
    "\n",
    "#             lower_category_pairs = []\n",
    "#             lower_direct_pairs = []\n",
    "#             upper_category_pairs = []\n",
    "#             upper_direct_pairs = []\n",
    "#             ingredients_list = selected_ingredients['name'].values.tolist()\n",
    "#             already_checked = []\n",
    "#             for ingredient_name in ingredients_list:\n",
    "#                 for lc_name in pairing_data['lower_category_names'][pairing_data['name'] == ingredient_name].iloc[0]:\n",
    "#                     if lc_name in ingredients_list and not lc_name in already_checked:\n",
    "#                         lower_category_pairs.append([ingredient_name, lc_name])\n",
    "#                 for ld_name in pairing_data['lower_direct_names'][pairing_data['name'] == ingredient_name].iloc[0]:\n",
    "#                     if ld_name in ingredients_list and not ld_name in already_checked:\n",
    "#                         lower_direct_pairs.append([ingredient_name, ld_name])\n",
    "#                 for uc_name in pairing_data['upper_category_names'][pairing_data['name'] == ingredient_name].iloc[0]:\n",
    "#                     if uc_name in ingredients_list and not uc_name in already_checked:\n",
    "#                         upper_category_pairs.append([ingredient_name, uc_name])\n",
    "#                 for ud_name in pairing_data['upper_direct_names'][pairing_data['name'] == ingredient_name].iloc[0]:\n",
    "#                     if ud_name in ingredients_list and not ud_name in already_checked:\n",
    "#                         upper_direct_pairs.append([ingredient_name, ud_name])\n",
    "#                 already_checked.append(ingredient_name)\n",
    "\n",
    "#             lower_pairs = lower_category_pairs + lower_direct_pairs\n",
    "#             upper_pairs = upper_category_pairs + upper_direct_pairs\n",
    "#             all_pairs = lower_pairs + upper_pairs\n",
    "\n",
    "#     #         all_pairs_sp = [tuple(sorted(pair)) for pair in all_pairs]\n",
    "#     #         print(len(all_pairs_sp), len(list(set(all_pairs_sp))))\n",
    "\n",
    "#     #         print('INGREDIENTS', ingredients_list)\n",
    "#     #         print()\n",
    "#     #         print('LC PAIRS', lower_category_pairs)\n",
    "#     #         print()\n",
    "#     #         print()\n",
    "\n",
    "#             G = nx.Graph()\n",
    "#             G.add_nodes_from(selected_ingredients['name'].values.tolist())\n",
    "#             G.add_edges_from(all_pairs)\n",
    "#             n_subgraphs = len(list(nx.connected_component_subgraphs(G)))\n",
    "\n",
    "#         score = 0\n",
    "\n",
    "#     # SIMPLER ALTERNATIVE: bonus for proportion of actual pairs over possible pairs? could then combine with pair strength bonus?\n",
    "#     # PAIRING DENSITY BONUS ============================================================================================\n",
    "#         # ranges from roughly (.1 to 1) * 4\n",
    "#         average_shortest_path_length = nx.average_shortest_path_length(G)\n",
    "#         average_shortest_path_score = 2.5 / average_shortest_path_length - 1.1\n",
    "#     #     print(average_shortest_path_score)\n",
    "#         score += average_shortest_path_score * 4    \n",
    "\n",
    "#     # # UPPER PAIRING BONUS ==============================================================================================\n",
    "#     #     # ranges from roughly (.25 to 1) * 3\n",
    "#     #     upper_proportion_score = len(upper_pairs) / len(all_pairs) * 2.5 # messed with this, not sure if it still works\n",
    "#     # #     print(upper_proportion_score)\n",
    "#     # #     print(len(upper_pairs), len(lower_pairs), len(all_pairs))\n",
    "#     # #     print()\n",
    "#     #     score += upper_proportion_score * 3\n",
    "\n",
    "#     # PAIR STRENGTH BONUS ==============================================================================================\n",
    "#         # ranges from roughly (0 to 1) * 3\n",
    "\n",
    "#         # I'm thinking of 'lower category' as default, and awarding points for steps up from that\n",
    "#         ld_bonus = 1*len(lower_direct_pairs)\n",
    "#         uc_bonus = 2*len(upper_category_pairs)\n",
    "#         ud_bonus = 5*len(upper_direct_pairs)\n",
    "#         pair_strength_score = .6*(ld_bonus + uc_bonus + ud_bonus)/len(all_pairs) # otherwise would tend toward large stir_frys\n",
    "#     #     print('LC', len(lower_category_pairs))\n",
    "#     #     print('LD', len(lower_direct_pairs))\n",
    "#     #     print('UC', len(upper_category_pairs))\n",
    "#     #     print('UD', len(upper_direct_pairs))\n",
    "#     #     print('SCORE', pair_strength_score)\n",
    "#     #     print()\n",
    "#         score += pair_strength_score * 3\n",
    "\n",
    "#     # important but easy to avoid, so not weighted too heavily\n",
    "#     # CLASH PENALTY ====================================================================================================\n",
    "#         # ranges from roughly (0 to 1) * -1.5\n",
    "#         all_clashing_pairs = []\n",
    "#         selected_ingredients_list = selected_ingredients['name'].values.tolist()\n",
    "#         for name in selected_ingredients_list:\n",
    "#             names_that_clash_with_name = clashes_with_data['all_clashes_with_names'][clashes_with_data['name'] == name].iloc[0]\n",
    "#             all_clashing_names = set(selected_ingredients_list).intersection(set(names_that_clash_with_name)) # selected names that clash with this selected name\n",
    "#             all_clashing_pairs += [tuple(sorted([name, all_clashing_name])) for all_clashing_name in all_clashing_names]\n",
    "\n",
    "#         all_clashing_pairs = list(set(all_clashing_pairs))\n",
    "#         all_clashing_pairs_score = len(list(all_clashing_pairs)) / 4\n",
    "#         score += len(all_clashing_pairs) * -1.5\n",
    "\n",
    "#     # # FRUIT BONUS ======================================================================================================\n",
    "#     #     # ranges from roughly (0 to 3) * .1\n",
    "#     #     n_fruit = len(selected_ingredients[selected_ingredients['fruit'] == 'y'])\n",
    "#     #     score += n_fruit * .1\n",
    "\n",
    "#     # # NUT SEED BONUS ===================================================================================================    \n",
    "#     #     # ranges from roughly (0 to 2) * .15\n",
    "#     #     n_nut_seed = len(selected_ingredients[selected_ingredients['protein_nut_seed'] == 'y'])\n",
    "#     #     score += n_nut_seed * .15\n",
    "\n",
    "#     # FLAVOR BALANCE BONUS =============================================================================================\n",
    "#         # ranges from roughly (0 to 1) * 1\n",
    "#         n_sweet_lower = len(selected_ingredients[selected_ingredients['sweet'] == 'y'])\n",
    "#         n_sweet_upper = len(selected_ingredients[selected_ingredients['sweet'] == 'Y'])\n",
    "#         n_salty_lower = len(selected_ingredients[selected_ingredients['salty'] == 'y'])\n",
    "#         n_salty_upper = len(selected_ingredients[selected_ingredients['salty'] == 'Y'])\n",
    "#         n_sour_lower = len(selected_ingredients[selected_ingredients['sour'] == 'y'])\n",
    "#         n_sour_upper = len(selected_ingredients[selected_ingredients['sour'] == 'Y'])\n",
    "#         n_savory_lower = len(selected_ingredients[selected_ingredients['savory'] == 'y'])\n",
    "#         n_savory_upper = len(selected_ingredients[selected_ingredients['savory'] == 'Y'])\n",
    "#         n_bitter_lower = len(selected_ingredients[selected_ingredients['bitter'] == 'y'])\n",
    "#         n_bitter_upper = len(selected_ingredients[selected_ingredients['bitter'] == 'Y'])\n",
    "#         n_spicy_lower = len(selected_ingredients[selected_ingredients['spicy'] == 'y'])\n",
    "#         n_spicy_upper = len(selected_ingredients[selected_ingredients['spicy'] == 'Y'])\n",
    "\n",
    "#         # each varies from roughly .5 to 1\n",
    "#         sweet_score = (n_sweet_lower/2 + n_sweet_upper)/5\n",
    "#         salty_score = (n_salty_lower/2 + n_salty_upper)*2/5\n",
    "#         sour_score = (n_sour_lower/2 + n_sour_upper)*2/5\n",
    "#         savory_score = (n_savory_lower/2 + n_savory_upper)*3/5\n",
    "#         bitter_score = (n_bitter_lower/2 + n_bitter_upper)*3/5\n",
    "#         spicy_score = (n_spicy_lower/2 + n_spicy_upper)*2/5\n",
    "\n",
    "#         flavor_balance_score = 5 / (1 + abs(1-sweet_score) + abs(1-salty_score) + abs(1-sour_score) + abs(1-savory_score) + abs(1-spicy_score)) - 1.25\n",
    "#     #     print(flavor_balance_score)\n",
    "\n",
    "#         score += flavor_balance_score\n",
    "#     #     print(sweet_score, salty_score, sour_score, savory_score, bitter_score, spicy_score)\n",
    "#     #     print()\n",
    "\n",
    "#     #     print(n_sweet_lower, n_sweet_upper)\n",
    "#     #     print(n_salty_lower, n_salty_upper)\n",
    "#     #     print(n_sour_lower, n_sour_upper)\n",
    "#     #     print(n_savory_lower, n_savory_upper)\n",
    "#     #     print(n_bitter_lower, n_bitter_upper)\n",
    "#     #     print(n_spicy_lower, n_spicy_upper)\n",
    "#     #     print()\n",
    "\n",
    "#     # TEXTURE BALANCE BONUS ============================================================================================\n",
    "#         # ranges from roughly (0 to 1) * .75\n",
    "#         n_crunchy_lower = len(selected_ingredients[selected_ingredients['stir_fry_crunchy'] == 'y'])\n",
    "#         n_crunchy_upper = len(selected_ingredients[selected_ingredients['stir_fry_crunchy'] == 'Y'])\n",
    "#         n_chewy_lower = len(selected_ingredients[selected_ingredients['stir_fry_chewy'] == 'y'])\n",
    "#         n_chewy_upper = len(selected_ingredients[selected_ingredients['stir_fry_chewy'] == 'Y'])\n",
    "#         n_juicy_lower = len(selected_ingredients[selected_ingredients['stir_fry_juicy'] == 'y'])\n",
    "#         n_juicy_upper = len(selected_ingredients[selected_ingredients['stir_fry_juicy'] == 'Y'])\n",
    "\n",
    "#         # each ranges from roughly 0 to 1\n",
    "#         crunchy_score = (n_crunchy_lower/2 + n_crunchy_upper)/3\n",
    "#         chewy_score = (n_chewy_lower/2 + n_chewy_upper)\n",
    "#         juicy_score = (n_juicy_lower/2 + n_juicy_upper)/3\n",
    "#     #     print(crunchy_score, chewy_score, juicy_score)\n",
    "\n",
    "#         texture_balance_score = 4 / (1 + abs(1-crunchy_score) + abs(1-chewy_score) + abs(1-juicy_score)) - 1\n",
    "#     #     print(texture_balance_score)\n",
    "#     #     print()\n",
    "\n",
    "#         score += texture_balance_score * .75\n",
    "\n",
    "#     # seems like it's hard to balance food groups on top of everything else. pity the scores aren't more independent\n",
    "#     # FOOD GROUP BALANCE BONUS =========================================================================================\n",
    "#         # ranges from roughly (25 to 1) * 2\n",
    "#         n_fruit = len(selected_ingredients[selected_ingredients['fruit'] == 'y'])\n",
    "#         n_veg = len(selected_ingredients[selected_ingredients['veg'] == 'y'])\n",
    "#         n_protein = len(selected_ingredients[selected_ingredients['protein'] == 'y'])\n",
    "\n",
    "#         # each varies from roughly 0 to 1 (sometimes a little over)\n",
    "#         fruit_score = n_fruit / 3\n",
    "#         veg_score = n_veg / 5\n",
    "#         protein_score = n_protein / 3\n",
    "#     #     print(fruit_score, veg_score, protein_score)\n",
    "\n",
    "#         food_group_balance_score = 3 / (1 + abs(1-fruit_score) + abs(1-veg_score) + abs(1-protein_score)) - .75\n",
    "#     #     print(food_group_balance_score)\n",
    "#     #     print()\n",
    "\n",
    "#         score += food_group_balance_score * 2\n",
    "\n",
    "#         if score > top_score:\n",
    "#             top_score = score\n",
    "#             top_food_group_balance_score = food_group_balance_score\n",
    "#             top_average_shortest_path_score = average_shortest_path_score\n",
    "#             top_flavor_balance_score = flavor_balance_score\n",
    "#             top_texture_balance_score = texture_balance_score\n",
    "#             top_all_clashing_pairs_score = all_clashing_pairs_score\n",
    "#             top_pair_strength_score = pair_strength_score\n",
    "#     #         top_upper_pairs = upper_pairs\n",
    "#     #         top_lower_pairs = lower_pairs\n",
    "#             top_lc_pairs = lower_category_pairs\n",
    "#             top_ld_pairs = lower_direct_pairs\n",
    "#             top_uc_pairs = upper_category_pairs\n",
    "#             top_ud_pairs = upper_direct_pairs\n",
    "#             top_selected_ingredients = selected_ingredients\n",
    "#             top_average_shortest_path_length = average_shortest_path_length\n",
    "#             top_all_clashing_pairs = all_clashing_pairs\n",
    "#     #         top_upper_proportion = len(upper_pairs) / (len(upper_pairs) + len(lower_pairs))\n",
    "#     # print('TOP AVG SHORTEST PATH LENGTH', top_average_shortest_path_length)\n",
    "#     # print('TOP UPPER PROPORTION', top_upper_proportion)\n",
    "# #     print('TOP AVERAGE SHORTEST PATH SCORE', top_average_shortest_path_score * 4)\n",
    "# #     print('TOP PAIR STRENGTH SCORE', top_pair_strength_score * 3)\n",
    "# #     print('TOP ALL CLASHING PAIRS SCORE', top_all_clashing_pairs_score * -1.5)\n",
    "# #     print('TOP FLAVOR BALANCE SCORE', top_flavor_balance_score)\n",
    "# #     print('TOP TEXTURE BALANCE SCORE', top_texture_balance_score * .75)\n",
    "# #     print('TOP FOOD GROUP BALANCE SCORE', top_food_group_balance_score * 2)\n",
    "# #     print('TOP_SCORE', top_score)  \n",
    "    \n",
    "#     vegan = top_selected_ingredients['not_vegan'].sum() == ''\n",
    "#     gluten_free = top_selected_ingredients['gluten'].sum() == ''\n",
    "    \n",
    "#     recipe_greens = top_selected_ingredients[top_selected_ingredients['stir_fry_green'] == 'y']\n",
    "#     recipe_extras = top_selected_ingredients[top_selected_ingredients['stir_fry_extra'] == 'y']\n",
    "#     recipe_dressing_oils = top_selected_ingredients[top_selected_ingredients['stir_fry_dressing_oil'] == 'y']\n",
    "#     recipe_dressing_vinegars = top_selected_ingredients[top_selected_ingredients['stir_fry_dressing_vinegar'] == 'y']\n",
    "#     recipe_dressing_salts = top_selected_ingredients[top_selected_ingredients['stir_fry_dressing_salt'] == 'y']\n",
    "#     recipe_dressing_peppers = top_selected_ingredients[top_selected_ingredients['stir_fry_dressing_pepper'] == 'y']\n",
    "#     recipe_dressing_garlics = top_selected_ingredients[top_selected_ingredients['stir_fry_dressing_garlic'] == 'y']\n",
    "#     # could just select 'stir_fry_dressing', but this includes garlics\n",
    "#     recipe_dressing = recipe_dressing_oils.append(recipe_dressing_vinegars).append(recipe_dressing_salts).append(recipe_dressing_peppers)\n",
    "\n",
    "#     new_recipe = pd.DataFrame({\n",
    "#         'vegan': [vegan],\n",
    "#         'gluten_free': [gluten_free],\n",
    "#         'basic': [stir_fry_data_is_basic],\n",
    "#         'best_of': [best_of],\n",
    "#         'score': [top_score],\n",
    "#         'pairing_density_bonus': [top_average_shortest_path_score * 4],\n",
    "#         'pair_strength_bonus': [top_pair_strength_score * 3],\n",
    "#         'clash_penalty': [top_all_clashing_pairs_score * 4],\n",
    "#         'flavor_balance_bonus': [top_flavor_balance_score],\n",
    "#         'texture_balance_bonus': [top_texture_balance_score * 75],\n",
    "#         'food_group_balance_bonus': [top_food_group_balance_score * 2],\n",
    "#         'lc_pairs': [top_lc_pairs],\n",
    "#         'ld_pairs': [top_ld_pairs],\n",
    "#         'uc_pairs': [top_uc_pairs],\n",
    "#         'ud_pairs':[top_ud_pairs],\n",
    "#         'clashing_pairs': [top_all_clashing_pairs],\n",
    "#         'ingredient_names': [top_selected_ingredients['name'].values.tolist()],\n",
    "#         'leafy_green_names': [recipe_greens['name'].values.tolist()],\n",
    "#         'extra_names': [recipe_extras['name'].values.tolist()],\n",
    "#         'dressing_names': [recipe_dressing['name'].values.tolist()],\n",
    "#     })\n",
    "#     stir_fry_recipe_data = stir_fry_recipe_data.append(new_recipe, sort=False)\n",
    "#     print('stir_fry RECIPE RECORDED. SCORE:', top_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import date\n",
    "# today = date.today()\n",
    "# date_string = f'{str(today.year)}_{str(today.month)}_{str(today.day)}'\n",
    "\n",
    "# stir_fry_recipe_data.to_pickle(os.path.join(root_path, 'DATA/stir_fry_recipe_data_latest.pickle'))\n",
    "# stir_fry_recipe_data.to_pickle(os.path.join(root_path, f'DATA/stir_fry_recipe_data_{date_string}.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2. Displaying flavor tool generator records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !pip install pyvis\n",
    "\n",
    "# from pyvis import network as net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recipe = stir_fry_recipe_data[(stir_fry_recipe_data['basic'] == True) & (stir_fry_recipe_data['vegan'] == True) & stir_fry_recipe_data['gluten_free'] == True].sample(1).iloc[0]\n",
    "# recipe_net = net.Network(notebook=True)\n",
    "\n",
    "# nodes = recipe['ingredient_names']\n",
    "\n",
    "# nodes_color = []\n",
    "# for name in recipe['ingredient_names']:\n",
    "#     ingredient = stir_fry_data[stir_fry_data['name'] == name].iloc[0]\n",
    "#     if ingredient['stir_fry_green'] == 'y':\n",
    "#         nodes_color.append('lightgreen')\n",
    "#     elif ingredient['stir_fry_extra'] == 'y':\n",
    "#         if ingredient['veg'] == 'y':\n",
    "#             nodes_color.append('green')\n",
    "#         elif ingredient['fruit'] == 'y':\n",
    "#             nodes_color.append('orange')\n",
    "#         elif ingredient['protein_nut_seed'] == 'y':\n",
    "#             nodes_color.append('brown')\n",
    "#         else:\n",
    "#             nodes_color.append('lightblue')\n",
    "#     elif ingredient['stir_fry_dressing'] == 'y':\n",
    "#         nodes_color.append('lightgrey')  \n",
    "        \n",
    "# recipe_net.add_nodes(\n",
    "#     nodes=nodes,\n",
    "#     color=nodes_color\n",
    "# )\n",
    "\n",
    "# for pair in recipe['lc_pairs']:\n",
    "#     recipe_net.add_edge(pair[0], pair[1], physics=False, color='lightgrey')\n",
    "\n",
    "# for pair in recipe['ld_pairs']:\n",
    "#     recipe_net.add_edge(pair[0], pair[1], physics=False, color='grey')\n",
    "    \n",
    "# for pair in recipe['uc_pairs']:\n",
    "#     recipe_net.add_edge(pair[0], pair[1], color='darkgrey')\n",
    "    \n",
    "# for pair in recipe['ud_pairs']:\n",
    "#     recipe_net.add_edge(pair[0], pair[1], color='black')\n",
    "\n",
    "# if not recipe['vegan']:\n",
    "#     print('NOT VEGAN')\n",
    "# if not recipe['gluten_free']:\n",
    "#     print('CONTAINS GLUTEN')\n",
    "# print('SCORE:', recipe['score'])\n",
    "# recipe_net.show('recipe_net.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('SEITAN (see also tips for NAMA-FU)', 2), ('ALMONDS (and UNSWEETENED ALMOND BUTTER; see also MILK, ALMOND)', 3), ('ORANGES, ORANGE JUICE, and ORANGE ZEST', 4), ('LAVENDER', 4), ('VINEGAR, BALSAMIC', 5), ('POTATOES, BLUE (or PURPLE)', 2)]\n",
      "Scoring method set to \"connected\"\n",
      "AVERAGE DEGREE 3.3333333333333335\n",
      "1.4999999999999998\n",
      "\n",
      "RAW PAIRING BONUS 0.11539322489180726\n",
      "RAW FLAVOR BALANCE BONUS -0.050000000000000044\n",
      "RAW FOOD GROUP BONUS 1.0\n",
      "SCORE 1.2961796746754217\n",
      "CPU times: user 16.8 ms, sys: 0 ns, total: 16.8 ms\n",
      "Wall time: 16.9 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"500px\"\n",
       "            height=\"500px\"\n",
       "            src=\"cliqued_net.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f37e1bc1908>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "n_iterations = 100\n",
    "keep_iterating = True\n",
    "\n",
    "n_attempts_before_deciding = 10\n",
    "n_attempts_before_giving_up = 5\n",
    "\n",
    "scoring_method = 'tbd'\n",
    "top_score = 0\n",
    "\n",
    "while True:\n",
    "    selected_ingredients = stir_fry_flavor_data.sample(6)\n",
    "    selected_names = selected_ingredients['name'].values.tolist()\n",
    "\n",
    "    connections = []\n",
    "    weighted_edges = []\n",
    "\n",
    "    for i_1, name_1 in enumerate(selected_names[:-1]):\n",
    "        for i_2, name_2 in enumerate(selected_names[i_1+1:], i_1+1):\n",
    "            connection = selected_ingredients[name_1][name_2]\n",
    "            if connection[0] != '_':\n",
    "                if connection[0] == 'c':\n",
    "                    pairs_with_demerit = .8\n",
    "                elif connection[0] == 'd':\n",
    "                    pairs_with_demerit = .6666\n",
    "                elif connection[0] == 'C':\n",
    "                    pairs_with_demerit = .5333\n",
    "                elif connection[0] == 'D':\n",
    "                    pairs_with_demerit = .4\n",
    "                else:\n",
    "                    print('OH NO! BAD PAIRING VALUE.')\n",
    "\n",
    "                if connection[1] == '_':\n",
    "                    strength_demerit = .2\n",
    "                elif connection[1] == 's':\n",
    "                    strength_demerit = .15\n",
    "                elif connection[1] == 'S':\n",
    "                    strength_demerit = .1\n",
    "                else:\n",
    "                    print('OH NO! BAD STRENGTH VALUE.')\n",
    "\n",
    "\n",
    "                connection_weight = pairs_with_demerit + strength_demerit\n",
    "                weighted_edges.append((name_1, name_2, connection_weight))\n",
    "                connections.append((name_1, name_2, connection))\n",
    "\n",
    "    selected_g = nx.Graph()\n",
    "    selected_g.add_nodes_from(selected_names)\n",
    "    selected_g.add_weighted_edges_from(weighted_edges)\n",
    "    connected_components = list(nx.connected_components(selected_g))\n",
    "    print(selected_g.degree())\n",
    "\n",
    "    if scoring_method == 'tbd':\n",
    "        if len(connected_components) == 1:\n",
    "            print('Scoring method set to \"connected\"')\n",
    "            scoring_method = 'connected'\n",
    "            break\n",
    "        elif connection_attempt >= n_attempts_before_deciding:\n",
    "            print('Scoring method set to \"disconnected\"')\n",
    "            scoring_method = 'disconnected'\n",
    "            break\n",
    "    elif scoring_method == 'connected':\n",
    "        if len(connected_components) == 1:\n",
    "            break\n",
    "        elif connection_attempt >= n_attempts_before_giving_up:\n",
    "            print('Giving up')\n",
    "            keep_iterating = False\n",
    "            break\n",
    "    elif scoring_method == 'disconnected':\n",
    "        break\n",
    "\n",
    "    connection_attempt += 1\n",
    "\n",
    "score = 0\n",
    "\n",
    "if scoring_method == 'connected':\n",
    "    # CONNECTED PAIRING BONUS ============================================================================================\n",
    "    # ranges from roughly (0 to 1) * 3, tho could be a lil over or under that range\n",
    "    average_shortest_path_length = nx.average_shortest_path_length(selected_g, weight='weight')\n",
    "    average_shortest_path_score = 1 / average_shortest_path_length * 2 - 1.75\n",
    "#         print('AVERAGE SHORTEST PATH SCORE', average_shortest_path_score)\n",
    "    score += average_shortest_path_score * 3\n",
    "else:\n",
    "    # DISCONNECTED PAIRING BONUS ============================================================================================\n",
    "    # not really sure how this sranges. hopefully (0 - 1) * 3? Hard to test.\n",
    "    largest_cc = max(connected_components, key=len)\n",
    "    largest_subgraph = selected_g.subgraph(largest_cc) # .copy()?\n",
    "    largest_subgraph_g = nx.Graph(largest_subgraph)\n",
    "\n",
    "    average_shortest_path_length = nx.average_shortest_path_length(largest_subgraph_g, weight='weight')\n",
    "    average_shortest_path_score = 1 / average_shortest_path_length * 2 - 1.75\n",
    "#         print('DISCONNECTED AVERAGE SHORTEST PATH SCORE', average_shortest_path_score)\n",
    "    score += average_shortest_path_score * 3\n",
    "    \n",
    "# Used for both strength and locked bonus:\n",
    "node_degrees = list(selected_g.degree())\n",
    "average_degree = sum([node_degree[1] for node_degree in node_degrees]) / len(node_degrees)\n",
    "print('AVERAGE DEGREE', average_degree)\n",
    "\n",
    "# STRENGTH BONUS =======================================================\n",
    "above_average = 0\n",
    "for node_degree in node_degrees:\n",
    "    if selected_ingredients['strong'][node_degree[0]] == 'Y':\n",
    "        above_average += (node_degree[1] - average_degree)\n",
    "    elif selected_ingredients['strong'][node_degree[0]] == 'y':\n",
    "        above_average += (node_degree[1] - average_degree) * .5\n",
    "print(above_average)\n",
    "\n",
    "\n",
    "# FLAVOR BALANCE BONUS =============================================================================================\n",
    "# ranges from roughly (0 to 1) * 1 (could be a lil over/under)\n",
    "# keeping this the same from ye olde stir fry gen (app version)\n",
    "n_sweet_lower = (selected_ingredients['sweet'] == 'y').sum()\n",
    "n_sweet_upper = (selected_ingredients['sweet'] == 'Y').sum()\n",
    "n_salty_lower = (selected_ingredients['salty'] == 'y').sum()\n",
    "n_salty_upper = (selected_ingredients['salty'] == 'Y').sum()\n",
    "n_sour_lower = (selected_ingredients['sour'] == 'y').sum()\n",
    "n_sour_upper = (selected_ingredients['sour'] == 'Y').sum()\n",
    "n_savory_lower = (selected_ingredients['savory'] == 'y').sum()\n",
    "n_savory_upper = (selected_ingredients['savory'] == 'Y').sum()\n",
    "n_bitter_lower = (selected_ingredients['bitter'] == 'y').sum()\n",
    "n_bitter_upper = (selected_ingredients['bitter'] == 'Y').sum()\n",
    "n_spicy_lower = (selected_ingredients['spicy'] == 'y').sum()\n",
    "n_spicy_upper = (selected_ingredients['spicy'] == 'Y').sum()\n",
    "\n",
    "#     # each varies from roughly .5 to 1 (normalized to the average flavor score)\n",
    "#     sweet_score = (n_sweet_lower/2 + n_sweet_upper)/6\n",
    "#     salty_score = (n_salty_lower/2 + n_salty_upper)/4\n",
    "#     sour_score = (n_sour_lower/2 + n_sour_upper)/2.5\n",
    "#     savory_score = (n_savory_lower/2 + n_savory_upper)/3\n",
    "#     bitter_score = (n_bitter_lower/2 + n_bitter_upper)/3\n",
    "#     spicy_score = (n_spicy_lower/2 + n_spicy_upper)/3\n",
    "# #     print(sweet_score, salty_score, sour_score, savory_score, bitter_score, spicy_score)\n",
    "\n",
    "# #     damn but this seems arbitrary & kludgy\n",
    "# #     flavor_balance_score = 1 / (1 + abs(1-sweet_score) + abs(1-salty_score) + abs(1-sour_score) + abs(1-savory_score) + abs(1-spicy_score))\n",
    "\n",
    "#     flavor_balance_score = 1 / (1 + abs(1-sweet_score) + abs(1-salty_score) + abs(1-sour_score) + abs(1-savory_score) + abs(1-spicy_score)) * 8 - 1.9\n",
    "#     print('FLAVOR BALANCE SCORE', flavor_balance_score)\n",
    "#     score += flavor_balance_score\n",
    "\n",
    "# SIMPLIFYING THIS, cause at end of the day it's up to user to balance shit    \n",
    "\n",
    "# TWO OBJECTIVES: reward having a little of every flavor, maaaybe punish for unbalanced flavors\n",
    "sweet_score = min(n_sweet_lower/2 + n_sweet_upper, 1)\n",
    "salty_score = min(n_salty_lower/2 + n_salty_upper, 1)\n",
    "sour_score = min(n_sour_lower/2 + n_sour_upper, 1)\n",
    "savory_score = min(n_savory_lower/2 + n_savory_upper, 1)\n",
    "bitter_score = min(n_bitter_lower/2 + n_bitter_upper, 1)\n",
    "spicy_score = min(n_spicy_lower/2 + n_spicy_upper, 1)\n",
    "\n",
    "flavor_balance_score = 0\n",
    "flavor_balance_score += sweet_score*3 # rly want something sweet in there\n",
    "flavor_balance_score += salty_score*.5 # can always use salt\n",
    "flavor_balance_score += sour_score*2 # like me some sour\n",
    "flavor_balance_score += savory_score*3 # LOVE me some savory\n",
    "flavor_balance_score += bitter_score # idk\n",
    "flavor_balance_score += spicy_score*2 # can be nice\n",
    "flavor_balance_score = flavor_balance_score * .2 - 1.3\n",
    "\n",
    "#     print('FLAVOR BALANCE SCORE', flavor_balance_score)\n",
    "score += flavor_balance_score\n",
    "\n",
    "# FOOD GROUPS BONUS ==========================================================================================\n",
    "#     n_fruit = (selected_ingredients['stir_fry_fruit'] == 'y').sum()\n",
    "#     n_protein = (selected_ingredients['stir_fry_protein'] == 'y').sum()\n",
    "\n",
    "#     # very very few, so realistically max .7\n",
    "#     fruit_score = (n_fruit/2)**.5\n",
    "#     print('FRUIT SCORE', fruit_score)\n",
    "\n",
    "#     #     0, .7, occaaaasionally 1\n",
    "#     protein_score = (n_protein/2)**.5\n",
    "#     print('PROTEIN SCORE', protein_score)\n",
    "\n",
    "if 'y' in selected_ingredients['stir_fry_protein'].values:\n",
    "    protein_score = .5\n",
    "else:\n",
    "    protein_score = 0\n",
    "\n",
    "if 'y' in selected_ingredients['stir_fry_fruit'].values:\n",
    "    fruit_score = .5\n",
    "else:\n",
    "    fruit_score = 0\n",
    "\n",
    "# print(fruit_score, protein_score)\n",
    "food_group_score = protein_score + fruit_score\n",
    "#     print('PROTEIN FRUIT', protein_score, fruit_score)\n",
    "score += food_group_score\n",
    "\n",
    "# # will bias toward larger stir_frys, slightly\n",
    "# # PROTEIN BONUS =========================================================================================\n",
    "# # ranges from roughly (0 to 1) * .5 (mostly balanced on its own)\n",
    "#     n_protein = (selected_ingredients['stir_fry_protein'] == 'y').sum()\n",
    "\n",
    "#     # /2 for steep diminishing returns (?)\n",
    "#     protein_score = (n_protein/2)**.5 * .75\n",
    "# #     print(protein_score)\n",
    "#     score += protein_score * .5\n",
    "\n",
    "top_average_shortest_path_score = average_shortest_path_score\n",
    "top_flavor_balance_score = flavor_balance_score\n",
    "top_food_group_score = food_group_score\n",
    "#         top_protein_score = protein_score\n",
    "top_score = score\n",
    "#         top_lc_pairs = lower_category_pairs\n",
    "#         top_ld_pairs = lower_direct_pairs\n",
    "#         top_uc_pairs = upper_category_pairs\n",
    "top_connections = connections\n",
    "top_selected_ingredients = selected_ingredients\n",
    "#         top_selected_g = selected_g\n",
    "#     print('RAW PAIRING BONUS', top_average_shortest_path_score)\n",
    "#     print('RAW FLAVOR BALANCE BONUS', top_flavor_balance_score)\n",
    "#     print('RAW PROTEIN BONUS', top_protein_score)\n",
    "#     print('SCORE', top_score)\n",
    "#     print()\n",
    "\n",
    "print()\n",
    "print('RAW PAIRING BONUS', top_average_shortest_path_score)\n",
    "print('RAW FLAVOR BALANCE BONUS', top_flavor_balance_score)\n",
    "print('RAW FOOD GROUP BONUS', top_food_group_score)\n",
    "print('SCORE', top_score)\n",
    "\n",
    "cliqued_net = net.Network(notebook=True)\n",
    "\n",
    "nodes = top_selected_ingredients['name'].tolist()\n",
    "\n",
    "def get_color(row):\n",
    "#     return 'grey'\n",
    "    if row['stir_fry_veg'] == 'y':\n",
    "        return 'green'\n",
    "    elif row['stir_fry_fruit'] == 'y':\n",
    "        return 'orange'\n",
    "    elif row['stir_fry_protein'] == 'y':\n",
    "        return 'brown'\n",
    "    elif row['stir_fry_grain'] == 'y':\n",
    "        return 'tan'\n",
    "    else:\n",
    "        return 'lightgrey'\n",
    "nodes_color = top_selected_ingredients.apply(get_color, axis=1).tolist()\n",
    "\n",
    "cliqued_net.add_nodes(\n",
    "    nodes=nodes,\n",
    "    color=nodes_color\n",
    ")\n",
    "\n",
    "for connection in top_connections:\n",
    "    if connection[2][0] == 'c':\n",
    "        cliqued_net.add_edge(connection[0], connection[1], physics=False, color='lightgrey')\n",
    "    if connection[2][0] == 'd':\n",
    "        cliqued_net.add_edge(connection[0], connection[1], physics=False, color='grey')\n",
    "    if connection[2][0] == 'C':\n",
    "        cliqued_net.add_edge(connection[0], connection[1], color='darkgrey')\n",
    "    if connection[2][0] == 'D':\n",
    "        cliqued_net.add_edge(connection[0], connection[1], color='black')\n",
    " \n",
    "cliqued_net.show('cliqued_net.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name_index\n",
       "SEITAN (see also tips for NAMA-FU)                                 \n",
       "ALMONDS (and UNSWEETENED ALMOND BUTTER; see also MILK, ALMOND)     \n",
       "ORANGES, ORANGE JUICE, and ORANGE ZEST                            y\n",
       "LAVENDER                                                          y\n",
       "VINEGAR, BALSAMIC                                                 y\n",
       "POTATOES, BLUE (or PURPLE)                                         \n",
       "Name: strong, dtype: object"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_ingredients['strong']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
